<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>现代循环神经网络 | (๑&gt;ᴗ&lt;๑)</title><meta name="author" content="画船听雨眠y"><meta name="copyright" content="画船听雨眠y"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="写在前面  参考书籍  Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. Dive into Deep Learning. 2020. 简介 - Dive-into-DL-PyTorch (tangshusen.me) 现代循环神经网络 source code: NJU-ymhui&#x2F;DeepLearning: Deep Learn">
<meta property="og:type" content="article">
<meta property="og:title" content="现代循环神经网络">
<meta property="og:url" content="https://nju-ymhui.github.io/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="(๑&gt;ᴗ&lt;๑)">
<meta property="og:description" content="写在前面  参考书籍  Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. Dive into Deep Learning. 2020. 简介 - Dive-into-DL-PyTorch (tangshusen.me) 现代循环神经网络 source code: NJU-ymhui&#x2F;DeepLearning: Deep Learn">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://nju-ymhui.github.io/ymhui.github.io/img/myicon.ico">
<meta property="article:published_time" content="2024-09-23T09:25:08.000Z">
<meta property="article:modified_time" content="2024-09-26T02:14:03.254Z">
<meta property="article:author" content="画船听雨眠y">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://nju-ymhui.github.io/ymhui.github.io/img/myicon.ico"><link rel="shortcut icon" href="/ymhui.github.io/img/myicon.ico"><link rel="canonical" href="https://nju-ymhui.github.io/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/ymhui.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{
      const saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
      
      window.btf = {
        saveToLocal: saveToLocal,
        getScript: (url, attr = {}) => new Promise((resolve, reject) => {
          const script = document.createElement('script')
          script.src = url
          script.async = true
          script.onerror = reject
          script.onload = script.onreadystatechange = function() {
            const loadState = this.readyState
            if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
            script.onload = script.onreadystatechange = null
            resolve()
          }

          Object.keys(attr).forEach(key => {
            script.setAttribute(key, attr[key])
          })

          document.head.appendChild(script)
        }),

        getCSS: (url, id = false) => new Promise((resolve, reject) => {
          const link = document.createElement('link')
          link.rel = 'stylesheet'
          link.href = url
          if (id) link.id = id
          link.onerror = reject
          link.onload = link.onreadystatechange = function() {
            const loadState = this.readyState
            if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
            link.onload = link.onreadystatechange = null
            resolve()
          }
          document.head.appendChild(link)
        }),

        addGlobalFn: (key, fn, name = false, parent = window) => {
          const pjaxEnable = false
          if (!pjaxEnable && key.startsWith('pjax')) return

          const globalFn = parent.globalFn || {}
          const keyObj = globalFn[key] || {}
    
          if (name && keyObj[name]) return
    
          name = name || Object.keys(keyObj).length
          keyObj[name] = fn
          globalFn[key] = keyObj
          parent.globalFn = globalFn
        }
      }
    
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode
      
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/ymhui.github.io/',
  algolia: undefined,
  localSearch: {"path":"/ymhui.github.io/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '现代循环神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-26 10:14:03'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/ymhui.github.io/img/myicon.ico" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><a href="/ymhui.github.io/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/ymhui.github.io/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/ymhui.github.io/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/ymhui.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ymhui.github.io/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/ymhui.github.io/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/ymhui.github.io/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/ymhui.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://blue-whale-backend.oss-cn-nanjing.aliyuncs.com/back.jpg);"><nav id="nav"><span id="blog-info"><a href="/ymhui.github.io/" title="(๑&gt;ᴗ&lt;๑)"><span class="site-name">(๑&gt;ᴗ&lt;๑)</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/ymhui.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ymhui.github.io/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/ymhui.github.io/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/ymhui.github.io/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/ymhui.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">现代循环神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-23T09:25:08.000Z" title="发表于 2024-09-23 17:25:08">2024-09-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-26T02:14:03.254Z" title="更新于 2024-09-26 10:14:03">2024-09-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>25分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="现代循环神经网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>写在前面</h1>
<blockquote>
<p>参考书籍</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="http://www.d2l.ai">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p>
<p><a target="_blank" rel="noopener" href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p>
<h1>现代循环神经网络</h1>
<p><strong>source code</strong>: <a target="_blank" rel="noopener" href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui/DeepLearning: Deep Learning with pytorch (github.com)</a></p>
<p><strong>use git to clone</strong>: <a target="_blank" rel="noopener" href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p>
<p><code>/ModernRNN</code></p>
<blockquote>
<p>GRU_self.py	GRU_lib.py	LSTM_self.py	LSTM_lib.py	deep_rnn_lib.py	machine_translation.py	encoder_decoder.py</p>
<p><a target="_blank" rel="noopener" href="http://seq2seq.py">seq2seq.py</a></p>
</blockquote>
<h2 id="门控循环单元GRU">门控循环单元GRU</h2>
<p><strong>门控循环单元是长短期记忆LSTM的一个稍微简化的变体</strong>。我们从介绍它开始。</p>
<p>门控循环单元和普通的循环神经网络最大的区别在于：<strong>前者支持隐状态的门控</strong>。这意味着模型有专门的机制来确定应该何时更新隐状态，以及应该何时重置隐状态，并且这些机制是可学习的。</p>
<p>关于更新门、重置门、隐状态和候选隐状态，详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-modern/gru.html#reset-gate-and-update-gate">10.2. Gated Recurrent Units (GRU) — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<p>下面我们着重介绍如何实现GRU。</p>
<h3 id="从零实现GRU">从零实现GRU</h3>
<p>我们依然使用《时光机器》数据集</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hidden, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            normal((num_inputs, num_hidden)),</span><br><span class="line">            normal((num_hidden, num_hidden)),</span><br><span class="line">            torch.zeros(num_hidden, device=device)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = three()  <span class="comment"># 候选隐状态参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hidden, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    <span class="comment"># 注：附加梯度是一种集成学习技术，通过将多个弱学习器组合到一起，逐步提高模型的预测性能</span></span><br><span class="line">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_gru_state</span>(<span class="params">batch_size, num_hidden, device</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.zeros((batch_size, num_hidden), device=device),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gru</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(R * H, W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练与预测</span></span><br><span class="line">    vocab_size, num_hidden, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">1</span>  <span class="comment"># 这些参数和之前一样</span></span><br><span class="line">    model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hidden, device, get_params, init_gru_state, gru)</span><br><span class="line">    d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line">    <span class="comment"># 可视化困惑度</span></span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">time traveller                                                  </span><br><span class="line">perplexity 1.2, 23127.7 tokens/sec on cpu</span><br><span class="line">time traveller but now you begin this wo legh wime yo u gan a ju</span><br><span class="line">travelleryou can show ble i have been at work upon thisgeom</span><br></pre></td></tr></table></figure>
<img src="/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/gru_ppl.png" class="" title="image-20240923235435882">
<h3 id="简洁实现的GRU">简洁实现的GRU</h3>
<p>深度学习框架中的API包含了前面介绍的所有细节，我们可以直接实例化门控循环单元模型。</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line">    vocab_size, device = <span class="built_in">len</span>(vocab), d2l.try_gpu()</span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    num_inputs, num_hidden = vocab_size, <span class="number">256</span></span><br><span class="line">    gru_layer = nn.GRU(num_inputs, num_hidden)  <span class="comment"># 实例化GRU </span></span><br><span class="line">    model = d2l.RNNModel(gru_layer, vocab_size)</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perplexity 1.0, 16676.1 tokens/sec on cpu</span><br><span class="line">time traveller for so it will be convenient to speak of himwas e</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></table></figure>
<img src="/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/gru_lib.png" class="" title="image-20240924091819458">
<h2 id="长短期记忆网络LSTM">长短期记忆网络LSTM</h2>
<p>长期以来，<strong>隐变量模型存在着长期信息保存和短期输入缺失的问题</strong>，首次解决这一问题使用的是<strong>长短期记忆网络</strong>模型。</p>
<p>关于输入门、忘记门、输出门、候选记忆元、记忆元和隐状态，详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-modern/lstm.html#long-short-term-memory-lstm">门控记忆元</a></p>
<h3 id="从零实现LSTM">从零实现LSTM</h3>
<p>依然使用时光机器数据集。</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_lstm_params</span>(<span class="params">vocab_size, num_hidden, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            normal((num_inputs, num_hidden)),</span><br><span class="line">            normal((num_hidden, num_hidden)),</span><br><span class="line">            torch.zeros(num_hidden, device=device)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xi, W_hi, b_i = three()</span><br><span class="line">    <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = three()</span><br><span class="line">    <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = three()</span><br><span class="line">    <span class="comment"># 候选记忆元参数</span></span><br><span class="line">    W_xc, W_hc, b_c = three()</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hidden, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_lstm_state</span>(<span class="params">batch_size, num_hidden, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;初始化函数：LSTM的隐状态需要返回一个额外的记忆元，其单元值为0，形状为(批量大小，隐藏单元数)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hidden), device=device),</span><br><span class="line">            torch.zeros((batch_size, num_hidden), device=device))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处实际模型的定义与GRU格式类似</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lstm</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练和预测</span></span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">    vocab_size, num_hidden, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">    model = d2l.RNNModelScratch(vocab_size, num_hidden, device, get_lstm_params, init_lstm_state, lstm)</span><br><span class="line">    d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line">    <span class="comment"># 可视化困惑度</span></span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perplexity 1.1, 10388.7 tokens/sec on cpu</span><br><span class="line">time travellerit wollareftrev ssich aly lemesyou back an whree a</span><br><span class="line">travellerbyuccouco bain the psychologistyes so it seemed to</span><br></pre></td></tr></table></figure>
<img src="/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/lstm_self.png" class="" title="image-20240924104204139">
<h3 id="简洁实现的LSTM">简洁实现的LSTM</h3>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line">    vocab_size, num_hidden, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">    num_inputs = vocab_size</span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">    lstm_layer = nn.LSTM(num_inputs, num_hidden)</span><br><span class="line">    model = d2l.RNNModel(lstm_layer, vocab_size)</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perplexity 1.0, 10847.3 tokens/sec on cpu</span><br><span class="line">time traveller for so it will be convenient to speak of himwas e</span><br><span class="line">traveller with a slight accession ofcheerfulness really thi</span><br></pre></td></tr></table></figure>
<img src="/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/lstm_lib.png" class="" title="image-20240924110246905">
<h2 id="深度循环神经网络">深度循环神经网络</h2>
<p>到目前为止，我们只讨论了具有一个单向隐藏层的循环神经网络，事实上，<strong>我们可以将多层循环神经网络叠在一起</strong>。</p>
<p>理论部分详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-modern/deep-rnn.html#deep-recurrent-neural-networks">10.3. Deep Recurrent Neural Networks — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h3 id="简洁实现的深度循环神经网络">简洁实现的深度循环神经网络</h3>
<p>现有的API已经实现了该模型中的所有逻辑细节，方便起见，我们直接介绍简洁版本。</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">    vocab_size, num_hidden, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">    num_inputs = vocab_size</span><br><span class="line">    num_epochs, lr, device = <span class="number">500</span>, <span class="number">2</span>, d2l.try_gpu()</span><br><span class="line">    lstm_layer = nn.LSTM(num_inputs, num_hidden, num_layers)</span><br><span class="line">    model = d2l.RNNModel(lstm_layer, vocab_size)</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    d2l.train_ch8(model, train_iter, vocab, lr * <span class="number">1.0</span>, num_epochs, device)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perplexity 1.0, 8220.6 tokens/sec on cpu</span><br><span class="line">time travelleryou can show black is white by argument said filby</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></table></figure>
<img src="/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/deep_rnn.png" class="" title="image-20240924123403050">
<h2 id="双向循环神经网络">双向循环神经网络</h2>
<p>基本理论详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-modern/bi-rnn.html#bidirectional-recurrent-neural-networks">10.4. Bidirectional Recurrent Neural Networks — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h2 id="机器翻译与数据集">机器翻译与数据集</h2>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_data_nmt</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;载入\&quot;英语-法语\&quot;数据集&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;fra-eng&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;fra.txt&#x27;</span>), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> f.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_nmt</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对原始数据做一些预处理，如将不间断空格替换为一个空格，小写替换大写，单词和标点之间插入空格&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">no_space</span>(<span class="params">char, prev_char</span>):</span><br><span class="line">        <span class="keyword">return</span> char <span class="keyword">in</span> <span class="built_in">set</span>(<span class="string">&#x27;,.!?&#x27;</span>) <span class="keyword">and</span> prev_char != <span class="string">&#x27; &#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用空格替换不间断空格</span></span><br><span class="line">    <span class="comment"># 使用小写替换大写</span></span><br><span class="line">    text = text.replace(<span class="string">&#x27;\u202f&#x27;</span>, <span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27; &#x27;</span>).lower()</span><br><span class="line">    <span class="comment"># 在单词和标点符号之间插入空格</span></span><br><span class="line">    out = [<span class="string">&#x27; &#x27;</span> + char <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> no_space(char, text[i - <span class="number">1</span>]) <span class="keyword">else</span></span><br><span class="line">           char <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(text)]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词元化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_nmt</span>(<span class="params">text, num_examples=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;词元化\&quot;英语-法语\&quot;数据集&quot;&quot;&quot;</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.split(<span class="string">&#x27;\n&#x27;</span>)):</span><br><span class="line">        <span class="keyword">if</span> num_examples <span class="keyword">and</span> i &gt; num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) == <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> source, target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制每个文本序列所包含的词元数量的直方图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_list_len_pair_hist</span>(<span class="params">legend, xlabel, ylabel, xlist, ylist</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制列表长度对的直方图&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 使用原生pyplot 绘制直方图</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line">    _, _, patches = plt.hist([[<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> xlist], [<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> ylist]])</span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.ylabel(ylabel)</span><br><span class="line">    <span class="keyword">for</span> patch <span class="keyword">in</span> patches[<span class="number">1</span>].patches:</span><br><span class="line">        patch.set_hatch(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">    plt.legend(legend)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]  <span class="comment"># 截断</span></span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))  <span class="comment"># 填充</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将机器翻译的文本序列转换成小批量&quot;&quot;&quot;</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span>  l <span class="keyword">in</span> lines]</span><br><span class="line">    lines = [l + [vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">    ) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).<span class="built_in">type</span>(torch.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    reserved_tokens = [<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>]</span><br><span class="line">    <span class="comment"># 从文件中读取原始数据，并进行预处理</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    <span class="comment"># 对预处理后的数据进行分词，并按需限制样本数量</span></span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    <span class="comment"># 构建源语言和目标语言的词表，最小频率设为2</span></span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>, reserved_tokens=reserved_tokens)</span><br><span class="line">    target_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>, reserved_tokens=reserved_tokens)</span><br><span class="line">    <span class="comment"># 将分词后的文本序列转换为索引数组和有效长度数组</span></span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    target_array, target_valid_len = build_array_nmt(target, target_vocab, num_steps)</span><br><span class="line">    <span class="comment"># 组合所有数据数组，以便加载到迭代器中</span></span><br><span class="line">    data_arrays = [src_array, src_valid_len, target_array, target_valid_len]</span><br><span class="line">    <span class="comment"># 创建并返回数据迭代器，以及源语言和目标语言的词表</span></span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, target_vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;fra-eng&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;fra-eng.zip&#x27;</span>,</span><br><span class="line">                               <span class="string">&#x27;94646ad1522d915e7b0f9296181140edcf86a4f5&#x27;</span>)</span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    raw_text = read_data_nmt()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;raw data:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(raw_text[:<span class="number">75</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据预处理</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;after preprocessing:&quot;</span>)</span><br><span class="line">    text = preprocess_nmt(raw_text)</span><br><span class="line">    <span class="built_in">print</span>(text[:<span class="number">75</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词元化</span></span><br><span class="line">    source, target = tokenize_nmt(text)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;after tokenizing:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(source[:<span class="number">6</span>])</span><br><span class="line">    <span class="built_in">print</span>(target[:<span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">    show_list_len_pair_hist([<span class="string">&#x27;source&#x27;</span>, <span class="string">&#x27;target&#x27;</span>], <span class="string">&#x27;# tokens per sequence&#x27;</span>, <span class="string">&#x27;count&#x27;</span>, source, target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词表</span></span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>, reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bcs&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;vocab size:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(src_vocab))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 截断或填充文本</span></span><br><span class="line">    <span class="built_in">print</span>(truncate_pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载迭代器和词表</span></span><br><span class="line">    train_iter, src_vocab, target_vocab = load_data_nmt(batch_size=<span class="number">2</span>, num_steps=<span class="number">8</span>)</span><br><span class="line">    <span class="comment"># 可视化一部分数据</span></span><br><span class="line">    <span class="keyword">for</span> X, X_valid_len, Y, Y_valid_len <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;X:&#x27;</span>, X.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;X的有效长度:&#x27;</span>, X_valid_len)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Y:&#x27;</span>, Y.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Y的有效长度:&#x27;</span>, Y_valid_len)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">raw data:</span><br><span class="line">Go.	Va !</span><br><span class="line">Hi.	Salut !</span><br><span class="line">Run!	Cours !</span><br><span class="line">Run!	Courez !</span><br><span class="line">Who?	Qui ?</span><br><span class="line">Wow!	Ça alors !</span><br><span class="line"></span><br><span class="line">after preprocessing:</span><br><span class="line">go .	va !</span><br><span class="line">hi .	salut !</span><br><span class="line">run !	cours !</span><br><span class="line">run !	courez !</span><br><span class="line">who ?	qui ?</span><br><span class="line">wow !	ça al</span><br><span class="line">after tokenizing:</span><br><span class="line">[[&#x27;go&#x27;, &#x27;.&#x27;], [&#x27;hi&#x27;, &#x27;.&#x27;], [&#x27;run&#x27;, &#x27;!&#x27;], [&#x27;run&#x27;, &#x27;!&#x27;], [&#x27;who&#x27;, &#x27;?&#x27;], [&#x27;wow&#x27;, &#x27;!&#x27;]]</span><br><span class="line">[[&#x27;va&#x27;, &#x27;!&#x27;], [&#x27;salut&#x27;, &#x27;!&#x27;], [&#x27;cours&#x27;, &#x27;!&#x27;], [&#x27;courez&#x27;, &#x27;!&#x27;], [&#x27;qui&#x27;, &#x27;?&#x27;], [&#x27;ça&#x27;, &#x27;alors&#x27;, &#x27;!&#x27;]]</span><br></pre></td></tr></table></figure>
<img src="/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/vocab_freq.png" class="" title="image-20240924172311048">
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vocab size:</span><br><span class="line">10012</span><br><span class="line">[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]</span><br><span class="line">X: tensor([[ 9, 28,  4,  3,  1,  1,  1,  1],</span><br><span class="line">        [16, 51,  4,  3,  1,  1,  1,  1]], dtype=torch.int32)</span><br><span class="line">X的有效长度: tensor([4, 4])</span><br><span class="line">Y: tensor([[73,  0,  4,  3,  1,  1,  1,  1],</span><br><span class="line">        [35, 53,  5,  3,  1,  1,  1,  1]], dtype=torch.int32)</span><br><span class="line">Y的有效长度: tensor([4, 4])</span><br></pre></td></tr></table></figure>
<h1>编码器-解码器架构</h1>
<p>接着上面的内容来说，机器翻译是序列转换模型中的一个关键问题，其困难点主要在于输入与输出序列都是可变长的。为了解决这个问题，我们设计一个全新的架构：<strong>编码器-解码器</strong>。在这个架构中，<strong>编码器负责把变长的输入序列转化为具有固定形状的编码状态，而解码器负责把这个固定形状的编码状态再转化为变长的输出序列。</strong></p>
<h2 id="接口">接口</h2>
<p>声明几个接口</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><br><span class="line">        enc_outputs = <span class="variable language_">self</span>.encoder(enc_X, *args)</span><br><span class="line">        dec_state = <span class="variable language_">self</span>.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure>
<h2 id="序列到序列学习seq2seq">序列到序列学习seq2seq</h2>
<p>本部分我们将使用两个循环神经网络的编码器和解码器，并将其应用于序列到序列类的学习任务</p>
<p>理论部分详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-modern/seq2seq.html#sequence-to-sequence-learning-for-machine-translation">10.7. Sequence-to-Sequence Learning for Machine Translation — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> encoder_decoder <span class="keyword">import</span> Encoder</span><br><span class="line"><span class="keyword">from</span> encoder_decoder <span class="keyword">import</span> Decoder</span><br><span class="line"><span class="keyword">from</span> encoder_decoder <span class="keyword">import</span> EncoderDecoder</span><br><span class="line"><span class="keyword">from</span> machine_translation <span class="keyword">import</span> load_data_nmt</span><br><span class="line"><span class="keyword">from</span> RNN.rnn_self <span class="keyword">import</span> grad_clipping</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqEncoder</span>(<span class="title class_ inherited__">Encoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hidden, num_layers, dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqEncoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 嵌入层</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.GRU(embed_size, num_hidden, num_layers, dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="comment"># X的形状：(batch_size, num_steps, embed_size_</span></span><br><span class="line">        X = <span class="variable language_">self</span>.embedding(X)</span><br><span class="line">        <span class="comment"># 在循环神经网络模型中，第一个轴对应于时间步</span></span><br><span class="line">        X = X.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 若未提及状态，默认0</span></span><br><span class="line">        output, state = <span class="variable language_">self</span>.rnn(X)</span><br><span class="line">        <span class="comment"># output的形状:(num_steps, batch_size, num_hidden)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers, batch_size, num_hidden)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqDecoder</span>(<span class="title class_ inherited__">Decoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hidden, num_layer, dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 定义词汇嵌入层，将词汇ID转换为词嵌入向量</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="comment"># 定义GRU网络，用于处理序列数据</span></span><br><span class="line">        <span class="comment"># 输入维度为词嵌入向量维度embed_size与隐藏层单元数num_hidden之和</span></span><br><span class="line">        <span class="comment"># 隐藏层单元数为num_hidden，用于捕捉序列中的长期依赖关系</span></span><br><span class="line">        <span class="comment"># 设置多层GRU，num_layer表示GRU的层数</span></span><br><span class="line">        <span class="comment"># 添加dropout，用于在训练过程中防止过拟合</span></span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.GRU(embed_size + num_hidden, num_hidden, num_layer, dropout=dropout)</span><br><span class="line">        <span class="comment"># 定义全连接层，将GRU的输出转换为词汇大小的预测值</span></span><br><span class="line">        <span class="comment"># 输入维度为GRU的隐藏层单元数num_hidden，输出维度为词汇表大小vocab_size</span></span><br><span class="line">        <span class="variable language_">self</span>.dense = nn.Linear(num_hidden, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param enc_outputs: 编码器的输出</span></span><br><span class="line"><span class="string">        :param args: 其余参数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># X的形状：(batch_size, num_steps, embed_size)</span></span><br><span class="line">        X = <span class="variable language_">self</span>.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 广播context，使其具有与X相同的num_steps</span></span><br><span class="line">        context = state[-<span class="number">1</span>].repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        output, state = <span class="variable language_">self</span>.rnn(X_and_context, state)</span><br><span class="line">        output = <span class="variable language_">self</span>.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># output的形状:(batch_size, num_steps, vocab_size)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers, batch_size, num_hidden)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面将通过计算交叉熵损失函数来进行优化</span></span><br><span class="line"><span class="comment"># 首先定义一个遮蔽函数通过零值化来屏蔽不相关预测</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sequence_mask</span>(<span class="params">X, valid_len, value=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;屏蔽序列中不相关的项&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取当前批次中序列的最大长度</span></span><br><span class="line">    max_len = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 创建一个形状为(batch_size, max_len)的掩码，其中valid_len对应位置为True，其余为False</span></span><br><span class="line">    mask = torch.arange((max_len), dtype=torch.float32, device=X.device)[<span class="literal">None</span>, :] &lt; valid_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment"># 将不符合掩码条件的元素替换为指定的value</span></span><br><span class="line">    X[~mask] = value</span><br><span class="line">    <span class="comment"># 返回应用掩码后的序列数据</span></span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaskedSoftmaxCELoss</span>(nn.CrossEntropyLoss):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># pred形状：(batch_size, num_steps, vocab_size)</span></span><br><span class="line">    <span class="comment"># label形状：(batch_size, num_steps)</span></span><br><span class="line">    <span class="comment"># valid_length形状：(batch_size,)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pred, label, valid_len</span>):</span><br><span class="line">        <span class="comment"># 初始化与标签形状相同的权重张量，初始权重都为1</span></span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        <span class="comment"># 应用sequence_mask以根据有效长度对权重进行掩码，超出有效长度的部分权重设为0</span></span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        <span class="comment"># 设置reduction参数为&#x27;none&#x27;，确保损失函数为每个元素返回一个未减少的损失值</span></span><br><span class="line">        <span class="variable language_">self</span>.reduction = <span class="string">&#x27;none&#x27;</span></span><br><span class="line">        <span class="comment"># 调用父类的forward方法计算未加权的损失，调整预测数据的维度以适应损失函数的要求</span></span><br><span class="line">        unweighted_loss = <span class="built_in">super</span>(MaskedSoftmaxCELoss, <span class="variable language_">self</span>).forward(pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>), label)</span><br><span class="line">        <span class="comment"># 将未加权的损失与掩码权重相乘，然后在序列维度上计算加权损失的平均值</span></span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> weighted_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练序列到序列学习模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_seq2seq</span>(<span class="params">net, train_iter, lr, num_epochs, target_vocab, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">xavier_init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.GRU:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> m._flat_weights_names:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;weight&quot;</span> <span class="keyword">in</span> param:</span><br><span class="line">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line">    net.apply(xavier_init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    loas = MaskedSoftmaxCELoss()</span><br><span class="line">    net.train()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer = d2l.Timer()</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> train_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            bos = torch.tensor([target_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class="number">0</span>], device=device).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            dec_input = torch.cat([bos, Y[:, :-<span class="number">1</span>]], <span class="number">1</span>)  <span class="comment"># 强制教学</span></span><br><span class="line">            Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class="line">            l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            <span class="comment"># 这里不做l.sum()会报RuntimeError: Boolean value of Tensor with more than one value is ambiguous</span></span><br><span class="line">            grad_clipping(net, l.<span class="built_in">sum</span>())</span><br><span class="line">            num_tokens = Y_valid_len.<span class="built_in">sum</span>()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l.<span class="built_in">sum</span>(), num_tokens)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>], ))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop()&#125;</span> tokens / sec on <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化损失曲线</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line"><span class="comment"># 为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都来自前一时间步的预测词元</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_seq2seq</span>(<span class="params">net, src_sentence, src_vocab, target_vocab, num_steps, device, save_attention_weights=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span></span><br><span class="line">    net.<span class="built_in">eval</span>()  <span class="comment"># 评估模式</span></span><br><span class="line">    <span class="comment"># 将源句子转换为词元序列，并添加序列结束符</span></span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">&#x27; &#x27;</span>)] + [src_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class="line">    <span class="comment"># 记录有效长度，用于处理padding</span></span><br><span class="line">    end_valid_len = torch.tensor([<span class="built_in">len</span>(src_tokens)], device=device)</span><br><span class="line">    <span class="comment"># 确保序列长度不超过num_steps，不足则填充</span></span><br><span class="line">    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    enc_X = torch.unsqueeze(</span><br><span class="line">        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 通过编码器编码源句子</span></span><br><span class="line">    enc_outputs = net.encoder(enc_X, end_valid_len)</span><br><span class="line">    <span class="comment"># 初始化解码器状态</span></span><br><span class="line">    dec_state = net.decoder.init_state(enc_outputs, end_valid_len)</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    <span class="comment"># 解码器的输入开始于开始符号</span></span><br><span class="line">    dec_X = torch.unsqueeze(</span><br><span class="line">        torch.tensor([target_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), dim=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    output_seq, attention_weight_seq = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        <span class="comment"># 使用解码器生成下一个词元</span></span><br><span class="line">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># 使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 挤压批量轴，获取预测的词元ID</span></span><br><span class="line">        pred = dec_X.squeeze(dim=<span class="number">0</span>).<span class="built_in">type</span>(torch.int32).item()</span><br><span class="line">        <span class="comment"># 保存注意力权重</span></span><br><span class="line">        <span class="keyword">if</span> save_attention_weights:</span><br><span class="line">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class="line">        <span class="comment"># 若序列结束词元被预测，输出序列的生成就完成了</span></span><br><span class="line">        <span class="keyword">if</span> pred == target_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 累积预测的词元序列</span></span><br><span class="line">        output_seq.append(pred)</span><br><span class="line">    <span class="comment"># 将词元ID序列转换为目标句子</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(target_vocab.to_tokens(output_seq)), attention_weight_seq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测序列的评估</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bleu</span>(<span class="params">pred_seq, label_seq, k</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将序列分割成词 token</span></span><br><span class="line">    pred_tokens, label_tokens = pred_seq.split(<span class="string">&#x27; &#x27;</span>), label_seq.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    <span class="comment"># 计算预测序列和标签序列的长度</span></span><br><span class="line">    len_pred, len_label = <span class="built_in">len</span>(pred_tokens), <span class="built_in">len</span>(label_tokens)</span><br><span class="line">    <span class="comment"># 计算精度的初步分数部分</span></span><br><span class="line">    score = math.exp(<span class="built_in">min</span>(<span class="number">0</span>, <span class="number">1</span> - len_label / len_pred))</span><br><span class="line">    <span class="comment"># 遍历不同的n-gram长度</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 初始化匹配数量和标签子序列的字典</span></span><br><span class="line">        num_matches, label_subs = <span class="number">0</span>, collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="comment"># 构建标签子序列的n-gram并计数</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_label - n + <span class="number">1</span>):</span><br><span class="line">            label_subs[<span class="string">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 在预测序列中查找匹配的n-gram</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_pred - n + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 如果在标签序列中找到匹配，则增加匹配数量</span></span><br><span class="line">            <span class="keyword">if</span> label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class="number">0</span>:</span><br><span class="line">                num_matches += <span class="number">1</span></span><br><span class="line">                label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 根据匹配数量和n-gram长度更新分数</span></span><br><span class="line">        score *= math.<span class="built_in">pow</span>(num_matches / (len_pred - n + <span class="number">1</span>), math.<span class="built_in">pow</span>(<span class="number">0.5</span>, n))</span><br><span class="line">    <span class="comment"># 返回最终的BLEU分数</span></span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 实例化一个Seq2SeqEncoder对象，用于编码序列</span></span><br><span class="line">    <span class="comment"># 参数说明：</span></span><br><span class="line">    <span class="comment"># vocab_size: 词汇表大小，表示输入数据的唯一词汇数量</span></span><br><span class="line">    <span class="comment"># embed_size: 嵌入层大小，表示将词汇嵌入到多少维度的向量空间</span></span><br><span class="line">    <span class="comment"># num_hidden: 隐藏层单元数量，决定了模型的复杂度</span></span><br><span class="line">    <span class="comment"># num_layers: RNN的层数，多层可以提高模型的表达能力</span></span><br><span class="line">    encoder = Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hidden=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line">    encoder.<span class="built_in">eval</span>()  <span class="comment"># 评估模式</span></span><br><span class="line">    X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">    output, state = encoder(X)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;encoder:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;output shape:&#x27;</span>, output.shape, <span class="string">&#x27;state shape:&#x27;</span>, state.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用与上面编码器相同的超参数来实例化解码器</span></span><br><span class="line">    decoder = Seq2SeqDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hidden=<span class="number">16</span>, num_layer=<span class="number">2</span>)</span><br><span class="line">    decoder.<span class="built_in">eval</span>()</span><br><span class="line">    state = decoder.init_state(encoder(X))</span><br><span class="line">    output, state = decoder(X, state)  <span class="comment"># 获得输出，并更新state</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;decoder:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;output shape:&#x27;</span>, output.shape, <span class="string">&#x27;state shape:&#x27;</span>, state.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义损失函数</span></span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;loss demo:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(loss(torch.ones(<span class="number">3</span>, <span class="number">4</span>, <span class="number">10</span>), torch.ones((<span class="number">3</span>, <span class="number">4</span>), dtype=torch.long), torch.tensor([<span class="number">4</span>, <span class="number">2</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 现在在机器翻译数据集上，我们可以创建和训练一个循环神经网络“编码器-解码器”模型用于序列到序列的学习</span></span><br><span class="line">    embed_size , num_hidden, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">    batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">    lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line">    train_iter, src_vocab, target_vocab = load_data_nmt(batch_size, num_steps)  <span class="comment"># 用d2l的库会报编码错误</span></span><br><span class="line">    encoder = Seq2SeqEncoder(<span class="built_in">len</span>(src_vocab), embed_size, num_hidden, num_layers, dropout)</span><br><span class="line">    decoder = Seq2SeqDecoder(<span class="built_in">len</span>(target_vocab), embed_size, num_hidden, num_layers, dropout)</span><br><span class="line">    net = EncoderDecoder(encoder, decoder)</span><br><span class="line">    train_seq2seq(net, train_iter, lr, num_epochs, target_vocab, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 利用训练好的“编码器-解码器”模型，将几个英语句子翻译为法语，并计算BLEU最终结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;start translating:&quot;</span>)</span><br><span class="line">    engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&#x27;i lost .&#x27;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">    fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">        translation, attention_weight_seq = predict_seq2seq(net, eng, src_vocab, target_vocab, num_steps, device)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, bleu <span class="subst">&#123;bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">encoder:</span><br><span class="line">output shape: torch.Size([7, 4, 16]) state shape: torch.Size([2, 4, 16])</span><br><span class="line">decoder:</span><br><span class="line">output shape: torch.Size([4, 7, 10]) state shape: torch.Size([2, 4, 16])</span><br><span class="line">loss demo:</span><br><span class="line">tensor([2.3026, 1.1513, 0.0000])</span><br><span class="line">loss 0.01964012612887416, 5921.959299792854 tokens / sec on cpu</span><br></pre></td></tr></table></figure>
<img src="/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/loss_seq2seq.png" class="" title="image-20240925102613044">
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start translating:</span><br><span class="line">go . =&gt; va !, bleu 1.000</span><br><span class="line">i lost . =&gt; j&#x27;ai perdu ., bleu 1.000</span><br><span class="line">he&#x27;s calm . =&gt; il est tom bon ?, bleu 0.447</span><br><span class="line">i&#x27;m home . =&gt; je suis &lt;unk&gt; ., bleu 0.512</span><br></pre></td></tr></table></figure>
<h1>束搜索</h1>
<p>在正式介绍束搜索之前，先介绍一下贪心搜索，并探讨其存在的问题。</p>
<p>本节主要以理论知识为主。</p>
<h2 id="贪心搜索">贪心搜索</h2>
<p>理论部分详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-modern/beam-search.html#greedy-search">10.8. Greedy Search — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h2 id="穷举搜索">穷举搜索</h2>
<p>理论部分详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-modern/beam-search.html#exhaustive-search">10.8. Exhaustive Search — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h2 id="束搜索">束搜索</h2>
<p>理论部分详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-modern/beam-search.html#id1">10.8. Beam Search — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<p>(•‿•)</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://nju-ymhui.github.io/ymhui.github.io">画船听雨眠y</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://nju-ymhui.github.io/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">https://nju-ymhui.github.io/ymhui.github.io/2024/09/23/现代循环神经网络/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://nju-ymhui.github.io/ymhui.github.io" target="_blank">(๑>ᴗ<๑)</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/ymhui.github.io/tags/pytorch/">pytorch</a><a class="post-meta__tags" href="/ymhui.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-share"><div class="social-share" data-image="/ymhui.github.io/img/myicon.ico" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>真的不考虑支持一下吗 (╯︵╰,)</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/ymhui.github.io/img/qrcode.png" target="_blank"><img class="post-qr-code-img" src="/ymhui.github.io/img/qrcode.png" alt="微信支付"/></a><div class="post-qr-code-desc">微信支付</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="prev-post pull-left" href="/ymhui.github.io/2024/09/27/%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="现代卷积神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">现代卷积神经网络</div></div></a><a class="next-post pull-right" href="/ymhui.github.io/2024/09/20/%E8%B4%AA%E5%BF%83/" title="贪心"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">贪心</div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a href="/ymhui.github.io/2024/09/16/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="卷积神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-16</div><div class="title">卷积神经网络</div></div></a><a href="/ymhui.github.io/2024/09/15/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="多层感知机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-15</div><div class="title">多层感知机</div></div></a><a href="/ymhui.github.io/2024/09/20/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="循环神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-20</div><div class="title">循环神经网络</div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="/ymhui.github.io/img/myicon.ico" onerror="this.onerror=null;this.src='/ymhui.github.io/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">画船听雨眠y</div><div class="author-info-description">美好的一天从打代码开始结束</div><div class="site-data"><a href="/ymhui.github.io/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/ymhui.github.io/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/ymhui.github.io/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/NJU-ymhui"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/NJU-ymhui" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">现代循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU"><span class="toc-number">2.1.</span> <span class="toc-text">门控循环单元GRU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0GRU"><span class="toc-number">2.1.1.</span> <span class="toc-text">从零实现GRU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0%E7%9A%84GRU"><span class="toc-number">2.1.2.</span> <span class="toc-text">简洁实现的GRU</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9CLSTM"><span class="toc-number">2.2.</span> <span class="toc-text">长短期记忆网络LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0LSTM"><span class="toc-number">2.2.1.</span> <span class="toc-text">从零实现LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0%E7%9A%84LSTM"><span class="toc-number">2.2.2.</span> <span class="toc-text">简洁实现的LSTM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.3.</span> <span class="toc-text">深度循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.3.1.</span> <span class="toc-text">简洁实现的深度循环神经网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.4.</span> <span class="toc-text">双向循环神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.5.</span> <span class="toc-text">机器翻译与数据集</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">编码器-解码器架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A5%E5%8F%A3"><span class="toc-number">3.1.</span> <span class="toc-text">接口</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0seq2seq"><span class="toc-number">3.2.</span> <span class="toc-text">序列到序列学习seq2seq</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">束搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%AA%E5%BF%83%E6%90%9C%E7%B4%A2"><span class="toc-number">4.1.</span> <span class="toc-text">贪心搜索</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2"><span class="toc-number">4.2.</span> <span class="toc-text">穷举搜索</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%9F%E6%90%9C%E7%B4%A2"><span class="toc-number">4.3.</span> <span class="toc-text">束搜索</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2025/05/16/%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/" title="建造者模式">建造者模式</a><time datetime="2025-05-16T09:13:23.000Z" title="发表于 2025-05-16 17:13:23">2025-05-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2025/05/16/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/" title="工厂模式">工厂模式</a><time datetime="2025-05-16T02:57:57.000Z" title="发表于 2025-05-16 10:57:57">2025-05-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2025/05/15/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/" title="策略模式">策略模式</a><time datetime="2025-05-15T10:03:11.000Z" title="发表于 2025-05-15 18:03:11">2025-05-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2025/05/15/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" title="设计模式">设计模式</a><time datetime="2025-05-15T09:42:01.000Z" title="发表于 2025-05-15 17:42:01">2025-05-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2025/05/15/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/" title="面向对象设计原则">面向对象设计原则</a><time datetime="2025-05-15T05:13:27.000Z" title="发表于 2025-05-15 13:13:27">2025-05-15</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 画船听雨眠y</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/ymhui.github.io/js/utils.js"></script><script src="/ymhui.github.io/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    const mathElements = document.querySelectorAll('#article-container .katex')
    mathElements.length && mathElements.forEach((el) => {
      el.classList.add('katex-show')
    })
  }
  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
  }

  showKatex()
})()</script><script>(() => {
  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blogs-comments-server.vercel.app/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://blogs-comments-server.vercel.app/',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))

    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(init)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/ymhui.github.io/js/search/local-search.js"></script></div></div></body></html>