<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>循环神经网络 | (๑&gt;ᴗ&lt;๑)</title><meta name="author" content="画船听雨眠y"><meta name="copyright" content="画船听雨眠y"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="写在前面  参考书籍  Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. Dive into Deep Learning. 2020. 简介 - Dive-into-DL-PyTorch (tangshusen.me) 循环神经网络前导 source code: NJU-ymhui&#x2F;DeepLearning: Deep Learn">
<meta property="og:type" content="article">
<meta property="og:title" content="循环神经网络">
<meta property="og:url" content="https://nju-ymhui.github.io/ymhui.github.io/2024/09/20/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="(๑&gt;ᴗ&lt;๑)">
<meta property="og:description" content="写在前面  参考书籍  Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. Dive into Deep Learning. 2020. 简介 - Dive-into-DL-PyTorch (tangshusen.me) 循环神经网络前导 source code: NJU-ymhui&#x2F;DeepLearning: Deep Learn">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://nju-ymhui.github.io/ymhui.github.io/img/myicon.ico">
<meta property="article:published_time" content="2024-09-20T08:03:49.000Z">
<meta property="article:modified_time" content="2024-09-23T15:07:31.192Z">
<meta property="article:author" content="画船听雨眠y">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://nju-ymhui.github.io/ymhui.github.io/img/myicon.ico"><link rel="shortcut icon" href="/ymhui.github.io/img/myicon.ico"><link rel="canonical" href="https://nju-ymhui.github.io/ymhui.github.io/2024/09/20/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/ymhui.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{
      const saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
      
      window.btf = {
        saveToLocal: saveToLocal,
        getScript: (url, attr = {}) => new Promise((resolve, reject) => {
          const script = document.createElement('script')
          script.src = url
          script.async = true
          script.onerror = reject
          script.onload = script.onreadystatechange = function() {
            const loadState = this.readyState
            if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
            script.onload = script.onreadystatechange = null
            resolve()
          }

          Object.keys(attr).forEach(key => {
            script.setAttribute(key, attr[key])
          })

          document.head.appendChild(script)
        }),

        getCSS: (url, id = false) => new Promise((resolve, reject) => {
          const link = document.createElement('link')
          link.rel = 'stylesheet'
          link.href = url
          if (id) link.id = id
          link.onerror = reject
          link.onload = link.onreadystatechange = function() {
            const loadState = this.readyState
            if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
            link.onload = link.onreadystatechange = null
            resolve()
          }
          document.head.appendChild(link)
        }),

        addGlobalFn: (key, fn, name = false, parent = window) => {
          const pjaxEnable = false
          if (!pjaxEnable && key.startsWith('pjax')) return

          const globalFn = parent.globalFn || {}
          const keyObj = globalFn[key] || {}
    
          if (name && keyObj[name]) return
    
          name = name || Object.keys(keyObj).length
          keyObj[name] = fn
          globalFn[key] = keyObj
          parent.globalFn = globalFn
        }
      }
    
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode
      
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/ymhui.github.io/',
  algolia: undefined,
  localSearch: {"path":"/ymhui.github.io/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '循环神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-23 23:07:31'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/ymhui.github.io/img/myicon.ico" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><a href="/ymhui.github.io/archives/"><div class="headline">文章</div><div class="length-num">44</div></a><a href="/ymhui.github.io/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/ymhui.github.io/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/ymhui.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ymhui.github.io/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/ymhui.github.io/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/ymhui.github.io/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/ymhui.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://blue-whale-backend.oss-cn-nanjing.aliyuncs.com/back.jpg);"><nav id="nav"><span id="blog-info"><a href="/ymhui.github.io/" title="(๑&gt;ᴗ&lt;๑)"><span class="site-name">(๑&gt;ᴗ&lt;๑)</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/ymhui.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ymhui.github.io/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/ymhui.github.io/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/ymhui.github.io/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/ymhui.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">循环神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-20T08:03:49.000Z" title="发表于 2024-09-20 16:03:49">2024-09-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-23T15:07:31.192Z" title="更新于 2024-09-23 23:07:31">2024-09-23</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>33分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="循环神经网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>写在前面</h1>
<blockquote>
<p>参考书籍</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="http://www.d2l.ai">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p>
<p><a target="_blank" rel="noopener" href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p>
<h1>循环神经网络前导</h1>
<p><strong>source code</strong>: <a target="_blank" rel="noopener" href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui/DeepLearning: Deep Learning with pytorch (github.com)</a></p>
<p><strong>use git to clone</strong>: <a target="_blank" rel="noopener" href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p>
<p><code>/RNN</code></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://markov.py">markov.py</a>	text_preprocess.py	nl_statistics.py	random_sampling.py	sequential_partition.py	<a target="_blank" rel="noopener" href="http://util.py">util.py</a>	rnn_self.py	rnn_lib.py</p>
</blockquote>
<p>到目前为止，我们一直都默认数据来自于某种分布，并且所有数据都是<em>i.i.d</em>（独立同分布）的，但是现实并非总是如此。比如一篇文章的文字是按某种顺序出现的，视频中的图像帧也按照特定顺序出现，网站的浏览行为也是有规律可循的…<strong>因此我们需要一个全新的模型来刻画这种现象。</strong></p>
<p>本文介绍的<strong>循环神经网络</strong>可以很好地处理序列信息，通过引入状态变量存储过去的信息和当前的输入，可以给出当前的输出。</p>
<p>由于许多循环神经网络的例子都基于文本数据，因此<strong>本文着重介绍语言模型</strong>。</p>
<h2 id="序列模型">序列模型</h2>
<p>处理序列数据需要统计工具和新的深度神经网络架构。</p>
<p>我们以股市交易数据为例入门，不妨用<em>x~t~<em>表示在</em>t</em>时间步(time step)时观测到的价格（<em>注：t通常是离散的并在整数或其子集上变化</em>）；如果希望在<em>t</em>日时较为准确地预测当日价格<em>x~t~</em>，应当有<em>x~t~</em> ~ <em><strong>P</strong></em>(<em>x~t~|x~t-1~…x~1~</em>), 即在已知前t - 1日结果的前提下预测当日结果。</p>
<h3 id="自回归模型">自回归模型</h3>
<p>第一种策略，假设在现实情况下相当长的序列x~t−1~, . . . , x~1~可能是不必要的，因此我们只需要满足某个长度为τ的时间跨度，即使用观测序列x~t−1~, . . . , x~t−τ~ 。当下获得的最直接的好处就是参数的数量总是不变的，至少在t &gt; τ时如此，这就使我们能够训练一个上面提及的深度网络。这种模型被称为<strong>自回归模型</strong>，因为它们是对自己执行回归。</p>
<p>第二种策略，如图8.1.2所示，是保留一些对过去观测的总结ht，并且同时更新预测ˆx~t~和总结ht。这就产生了基于ˆx~t~ = P(x~t~ | h~t~)估计x~t~，以及公式h~t~ = g(h~t−1~, x~t−1~)更新的模型。由于ht从未被观测到，这类模型也被称为<strong>隐变量自回归模型</strong></p>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/self_reg.png" alt="image-20240920170123607"></p>
<p>现在遇到一个新的问题，如何生成训练数据？一个常见的假设是，虽然特定值x~t~会改变，但<strong>序列本身的动力学不会改变</strong>，因为新的动力学一定受新数据的影响，而我们不可能用现有的数据预测出新的动力学。因此，整个序列的估计值都将通过以下的方式获得：</p>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5Carray_data.png" alt="image-20240920171153333"></p>
<p>当处理对象离散时（比如单词）上述公式仍有效，只不过要用分类器而不是回归模型来估计<strong>P</strong></p>
<h3 id="马尔可夫模型Markov">马尔可夫模型Markov</h3>
<h4 id="理论部分">理论部分</h4>
<p>简单来说我们在上述公式的基础上，取<em>τ</em> = 1，得到一个一阶马尔可夫模型；再考虑*x~t~*仅是离散值，使用动态规划沿着马尔科夫链精确地计算结果。</p>
<p>详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#markov-models">9.1. Working with Sequences — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h4 id="训练">训练</h4>
<p>接下来我们使用正弦函数在背景噪声下生成一些序列数据。</p>
<p>然后基于这些数据做一些预测</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_data</span>(<span class="params">num</span>):</span><br><span class="line">    x = torch.arange(<span class="number">1</span>, num + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    y = torch.sin(<span class="number">0.01</span> * x) + torch.normal(<span class="number">0</span>, <span class="number">0.25</span>, (num, ))</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;初始化网络权重&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_normal_(m.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个简单的多层感知机&quot;&quot;&quot;</span></span><br><span class="line">    net = nn.Sequential(  <span class="comment"># 一个有两个全连接层的多层感知机，使用ReLU激活函数</span></span><br><span class="line">        nn.Linear(<span class="number">4</span>, <span class="number">10</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, loss, epochs, lr</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型，与前面格式一致，不再赘述&quot;&quot;&quot;</span></span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr)  <span class="comment"># Adam优化器</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;loss: <span class="subst">&#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span> :</span><br><span class="line">    t = <span class="number">1000</span></span><br><span class="line">    time, x = generate_data(t)</span><br><span class="line">    d2l.plot(time, [x], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, legend=[<span class="string">&#x27;x&#x27;</span>], xlim=[<span class="number">1</span>, t], figsize=(<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">    d2l.plt.show()</span><br><span class="line">    <span class="comment"># 接下来，我们将这个序列转换为模型的特征－标签（feature‐label）对; features-labels就是前面讲过的的自变量-因变量，还记得吗？</span></span><br><span class="line">    <span class="comment"># 基于嵌入维度τ，我们将数据映射为数据对yt = xt 和xt = [xt−τ , . . . , xt−1]。</span></span><br><span class="line">    <span class="comment"># 这比我们提供的数据样本少了τ个，因为我们没有足够的历史记录来</span></span><br><span class="line">    <span class="comment"># 描述前τ个数据样本。一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；另一个方法是用零填充序列</span></span><br><span class="line">    <span class="comment"># 使用前600个“特征－标签”对进行训练</span></span><br><span class="line">    tau = <span class="number">4</span>  <span class="comment"># 取tau = 4</span></span><br><span class="line">    <span class="comment"># 初始化特征矩阵，其中t是时间序列的长度，tau是时间步的大小</span></span><br><span class="line">    features = torch.zeros((t - tau, tau))</span><br><span class="line">    <span class="comment"># 遍历时间步，构建特征矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">        features[:, i] = x[i:t - tau + i]  <span class="comment"># 列是时间步，行是数据序列</span></span><br><span class="line">    labels = x[tau:].reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    batch_size, n_train = <span class="number">16</span>, <span class="number">600</span>  <span class="comment"># 用前600个数据对来训练</span></span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    train_iter = d2l.load_array((features[:n_train], labels[:n_train]), batch_size, is_train=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 下面使用一个相当简单的架构训练模型：</span></span><br><span class="line">    <span class="comment"># 一个拥有两个全连接层的多层感知机，ReLU激活函数，平方损失函数</span></span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    net = get_net()</span><br><span class="line">    epochs, lr = <span class="number">5</span>, <span class="number">0.01</span></span><br><span class="line">    train(net, train_iter, loss, epochs, lr)</span><br><span class="line">    <span class="comment"># 接下来开始预测</span></span><br><span class="line">    onestep_pred = net(features)  <span class="comment"># net(...)应用模型进行预测, 这里net(features)对其中的每个值依次作用产生输出，因此可以视作单步预测</span></span><br><span class="line">    d2l.plot([time, time[tau:]], [x.detach().numpy(), onestep_pred.detach().numpy()],</span><br><span class="line">             xlabel=<span class="string">&#x27;time&#x27;</span>, ylabel=<span class="string">&#x27;x&#x27;</span>, xlim=[<span class="number">1</span>, t], legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step_pred&#x27;</span>], figsize=(<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">    d2l.plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 以上均为单步预测，下面使用我们的预测(而不是原始数据)进行多步预测</span></span><br><span class="line">    multistep_pred = torch.zeros(t)</span><br><span class="line">    <span class="comment"># 用我们的预测数据填充</span></span><br><span class="line">    multistep_pred[:n_train + tau] = x[:n_train + tau]</span><br><span class="line">    <span class="comment"># 利用之前的预测值进行多步预测</span></span><br><span class="line">    <span class="comment"># f(xt) = f(xt-1, xt-2, ..., xt-tau)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_train + tau, t):</span><br><span class="line">        multistep_pred[i] = net(multistep_pred[i - tau:i].reshape((<span class="number">1</span>, -<span class="number">1</span>)))  <span class="comment"># 这步预测结果出来后会被后面继续使用</span></span><br><span class="line">    d2l.plot([time, time[n_train + tau:]], [x.detach().numpy(), multistep_pred[n_train + tau:].detach().numpy()],</span><br><span class="line">             xlabel=<span class="string">&#x27;time&#x27;</span>, ylabel=<span class="string">&#x27;x&#x27;</span>, xlim=[<span class="number">1</span>, t], legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;multi-step_pred&#x27;</span>], figsize=(<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">    d2l.plt.show()</span><br><span class="line">    <span class="comment"># 可以看到超过某个值后预测的效果很差，几乎趋于一个常数，这是由于错误的累积</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于k = 1, 4, 16, 64，通过对整个序列预测的计算，让我们更仔细地看一下k步预测的困难</span></span><br><span class="line">    max_steps = <span class="number">64</span></span><br><span class="line">    features = torch.zeros((t - tau - max_steps + <span class="number">1</span>, tau + max_steps))</span><br><span class="line">    <span class="comment"># features的列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">        features[:, i] = x[i:i + t - tau - max_steps + <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau, tau + max_steps):</span><br><span class="line">        features[:, i] = net(features[:, i - tau:i]).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可视化1, 4, 16, 64步预测的结果</span></span><br><span class="line">    steps = (<span class="number">1</span>, <span class="number">4</span>, <span class="number">16</span>, <span class="number">64</span>)</span><br><span class="line">    d2l.plot([time[tau + i - <span class="number">1</span>: t - max_steps + i] <span class="keyword">for</span> i <span class="keyword">in</span> steps],</span><br><span class="line">             [features[:, (tau + i - <span class="number">1</span>)].detach().numpy() <span class="keyword">for</span> i <span class="keyword">in</span> steps], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">             legend=[<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>-step pred&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> steps], xlim=[<span class="number">5</span>, t],</span><br><span class="line">             figsize=(<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/data.png" alt="image-20240922011313291"></p>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/onestep.png" alt="image-20240922011322697"></p>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/multistep.png" alt="image-20240922011333865"></p>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/141664steps.png" alt="image-20240922011348281"></p>
<h2 id="文本预处理">文本预处理</h2>
<p>序列数据的另一种常见形式是文本；例如，一篇文章可以被看作单词甚至是字符序列。文本预处理通常采用以下步骤：</p>
<ol>
<li>将文本作为字符串加载到内存中。</li>
<li>将字符串拆分为词元（如单词和字符）。</li>
<li>建立一个词表，将拆分的词元映射到数字索引。(因为词元的类型是字符/字符串，而模型需要的是数字)</li>
<li>将文本转换为数字索引序列，方便模型操作。</li>
</ol>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(d2l.download(<span class="string">&quot;time_machine&quot;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]  <span class="comment"># 只要英文字母且全小写</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">lines, token=<span class="string">&quot;word&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将输入行拆分为词元</span></span><br><span class="line"><span class="string">    :param lines: 输入的文本行列表</span></span><br><span class="line"><span class="string">    :param token: 次元类型</span></span><br><span class="line"><span class="string">    :return: 拆分后的列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid token flag: &quot;</span> + token)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立一个词表，记录词元到数字的映射</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        <span class="comment"># 按词元出现频率排序，降序</span></span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        <span class="variable language_">self</span>._tokens_freq = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 未知词元索引为0</span></span><br><span class="line">        <span class="variable language_">self</span>.idx2token = [<span class="string">&#x27;&lt;ink&gt;&#x27;</span>] + reserved_tokens  <span class="comment"># 索引到词元</span></span><br><span class="line">        <span class="variable language_">self</span>.token2idx = &#123;token: idx <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.idx2token)&#125;  <span class="comment"># 词元到索引</span></span><br><span class="line">        <span class="keyword">for</span> token, freq <span class="keyword">in</span> <span class="variable language_">self</span>._tokens_freq:</span><br><span class="line">            <span class="keyword">if</span> freq &lt; min_freq:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.token2idx:</span><br><span class="line">                <span class="variable language_">self</span>.idx2token.append(token)</span><br><span class="line">                <span class="variable language_">self</span>.token2idx[token] = <span class="built_in">len</span>(<span class="variable language_">self</span>.idx2token) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回词汇表中词汇的数量&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.idx2token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;将一个或多个词汇转换为对应的索引，若词汇不存在，则返回未知词汇标识&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 若tokens不是列表或元组，则直接返回该词汇的索引或未知词汇标识</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.token2idx.get(tokens, <span class="variable language_">self</span>.unk)</span><br><span class="line">        <span class="comment"># 若tokens是列表或元组，则逐个转换为索引</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_tokens</span>(<span class="params">self, indices</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;将一个或多个索引转换为对应的词汇&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 若indices不是列表或元组，则直接返回该索引对应的词汇</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.idx2token[indices]</span><br><span class="line">        <span class="comment"># 若indices是列表或元组，则逐个转换为词汇</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.idx2token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property  </span><span class="comment"># unk可以像属性一样被访问，而不需要调用方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unk</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">token_freq</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._tokens_freq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_corpus</span>(<span class="params">tokens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计词元频率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;整合所有功能，返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, token=<span class="string">&#x27;char&#x27;</span>)  <span class="comment"># 次元类型改为char</span></span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="comment"># 因为数据集中的每一个文本行不一定是一个句子或者段落</span></span><br><span class="line">    <span class="comment"># 所以展平到一个列表中</span></span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 如果限定了最大tokens的数量，我们就只取前max行</span></span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 从时光机器文本中读取数据</span></span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>, <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(lines))</span><br><span class="line">    <span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(lines[<span class="number">114</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词元化</span></span><br><span class="line">    tokens = tokenize(lines)</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens[:<span class="number">10</span>]:</span><br><span class="line">        <span class="built_in">print</span>(token)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用上面拿到的数据集构建词表，看几个高频词及其索引</span></span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;high frequency token and its index:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token2idx.items())[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 现在就可以把每一行文本转化成索引序列了</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;line text -&gt; indices:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>]:</span><br><span class="line">        <span class="built_in">print</span>(tokens[i], <span class="string">&#x27;-&gt;&#x27;</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 验证整合的功能</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;check all functions in one:&quot;</span>)</span><br><span class="line">    tokens, vocab = load_corpus_time_machine()  <span class="comment"># 接受词元索引列表和词表</span></span><br><span class="line">    <span class="built_in">print</span>((<span class="built_in">len</span>(tokens), <span class="built_in">len</span>(vocab)))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">3221</span><br><span class="line">the time machine by h g wells</span><br><span class="line">but said the medical man staring hard at a coal in the fire if</span><br><span class="line">[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[&#x27;i&#x27;]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;traveller&#x27;, &#x27;for&#x27;, &#x27;so&#x27;, &#x27;it&#x27;, &#x27;will&#x27;, &#x27;be&#x27;, &#x27;convenient&#x27;, &#x27;to&#x27;, &#x27;speak&#x27;, &#x27;of&#x27;, &#x27;him&#x27;]</span><br><span class="line">[&#x27;was&#x27;, &#x27;expounding&#x27;, &#x27;a&#x27;, &#x27;recondite&#x27;, &#x27;matter&#x27;, &#x27;to&#x27;, &#x27;us&#x27;, &#x27;his&#x27;, &#x27;grey&#x27;, &#x27;eyes&#x27;, &#x27;shone&#x27;, &#x27;and&#x27;]</span><br><span class="line">high frequency token and its index:</span><br><span class="line">[(&#x27;&lt;ink&gt;&#x27;, 0), (&#x27;the&#x27;, 1), (&#x27;i&#x27;, 2), (&#x27;and&#x27;, 3), (&#x27;of&#x27;, 4), (&#x27;a&#x27;, 5), (&#x27;to&#x27;, 6), (&#x27;was&#x27;, 7), (&#x27;in&#x27;, 8), (&#x27;that&#x27;, 9)]</span><br><span class="line">line text -&gt; indices:</span><br><span class="line">[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;] -&gt; [1, 19, 50, 40, 2183, 2184, 400]</span><br><span class="line">[&#x27;twinkled&#x27;, &#x27;and&#x27;, &#x27;his&#x27;, &#x27;usually&#x27;, &#x27;pale&#x27;, &#x27;face&#x27;, &#x27;was&#x27;, &#x27;flushed&#x27;, &#x27;and&#x27;, &#x27;animated&#x27;, &#x27;the&#x27;] -&gt; [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]</span><br><span class="line">check all functions in one:</span><br><span class="line">(170580, 28)</span><br></pre></td></tr></table></figure>
<h2 id="语言模型和数据集">语言模型和数据集</h2>
<p>模型原理部分参考<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-neural-networks/language-model.html">9.3. Language Models — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h3 id="自然语言统计">自然语言统计</h3>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    lines = d2l.read_time_machine()</span><br><span class="line">    tokens = d2l.tokenize(lines)</span><br><span class="line">    corpus = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    vocab = d2l.Vocab(corpus)</span><br><span class="line">    <span class="built_in">print</span>(vocab.token_freqs[:<span class="number">10</span>])</span><br><span class="line">    frequencies = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> vocab.token_freqs]</span><br><span class="line">    d2l.plot(frequencies, xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;n(x)&#x27;</span>, xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">    d2l.plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看一下二元语法出现的概率</span></span><br><span class="line">    bigram_tokens = [pair <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">1</span>], corpus[<span class="number">1</span>:])]</span><br><span class="line">    bigram_vocab = d2l.Vocab(bigram_tokens)</span><br><span class="line">    <span class="built_in">print</span>(bigram_vocab.token_freqs[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 再来看一下三元组</span></span><br><span class="line">    trigram_tokens = [triple <span class="keyword">for</span> triple <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">2</span>], corpus[<span class="number">1</span>:-<span class="number">1</span>], corpus[<span class="number">2</span>:])]</span><br><span class="line">    trigram_vocab = d2l.Vocab(trigram_tokens)</span><br><span class="line">    <span class="built_in">print</span>(trigram_vocab.token_freqs[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对比一下三种组合的出现概率图</span></span><br><span class="line">    bigram_freq = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> bigram_vocab.token_freqs]</span><br><span class="line">    trigram_freq = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> trigram_vocab.token_freqs]</span><br><span class="line">    d2l.plot([frequencies, bigram_freq, trigram_freq], xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;n(x)&#x27;</span>, xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">             legend=[<span class="string">&#x27;single&#x27;</span>, <span class="string">&#x27;double&#x27;</span>, <span class="string">&#x27;triple&#x27;</span>])</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[(&#x27;the&#x27;, 2261), (&#x27;i&#x27;, 1267), (&#x27;and&#x27;, 1245), (&#x27;of&#x27;, 1155), (&#x27;a&#x27;, 816), (&#x27;to&#x27;, 695), (&#x27;was&#x27;, 552), (&#x27;in&#x27;, 541), (&#x27;that&#x27;, 443), (&#x27;my&#x27;, 440)]</span><br><span class="line">[((&#x27;of&#x27;, &#x27;the&#x27;), 309), ((&#x27;in&#x27;, &#x27;the&#x27;), 169), ((&#x27;i&#x27;, &#x27;had&#x27;), 130), ((&#x27;i&#x27;, &#x27;was&#x27;), 112), ((&#x27;and&#x27;, &#x27;the&#x27;), 109), ((&#x27;the&#x27;, &#x27;time&#x27;), 102), ((&#x27;it&#x27;, &#x27;was&#x27;), 99), ((&#x27;to&#x27;, &#x27;the&#x27;), 85), ((&#x27;as&#x27;, &#x27;i&#x27;), 78), ((&#x27;of&#x27;, &#x27;a&#x27;), 73)]</span><br><span class="line">[((&#x27;the&#x27;, &#x27;time&#x27;, &#x27;traveller&#x27;), 59), ((&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;), 30), ((&#x27;the&#x27;, &#x27;medical&#x27;, &#x27;man&#x27;), 24), ((&#x27;it&#x27;, &#x27;seemed&#x27;, &#x27;to&#x27;), 16), ((&#x27;it&#x27;, &#x27;was&#x27;, &#x27;a&#x27;), 15), ((&#x27;here&#x27;, &#x27;and&#x27;, &#x27;there&#x27;), 15), ((&#x27;seemed&#x27;, &#x27;to&#x27;, &#x27;me&#x27;), 14), ((&#x27;i&#x27;, &#x27;did&#x27;, &#x27;not&#x27;), 14), ((&#x27;i&#x27;, &#x27;saw&#x27;, &#x27;the&#x27;), 13), ((&#x27;i&#x27;, &#x27;began&#x27;, &#x27;to&#x27;), 13)]</span><br></pre></td></tr></table></figure>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/nls.png" alt="image-20240922133225914"></p>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/three_cmp.png" alt="image-20240922202317964"></p>
<h3 id="读取长序列数据">读取长序列数据</h3>
<p>因为序列数据本质上是连续的，因此我们在处理数据时<strong>需要解决读取长序列数据的问题</strong>。</p>
<p>一种处理办法是：<strong>当序列过长而不能被模型一次性处理时，拆分这样的序列方便模型读取</strong>。假设我们将使用神经网络来训练语言模型，<strong>模型中的网络一次处</strong><br>
<strong>理具有预定义长度（例如n个时间步）的一个小批量序列</strong>。现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。</p>
<p>首先，由于文本序列可以是任意长的，例如整本《时光机器》，于是任意长的序列可以被我们划分为具有相同时间步数的子序列。当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。假设网络一次只处理具有n个时间步的子序列。</p>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/long_spilit.png" alt="image-20240922204539384"></p>
<p>上图画出了从原始文本序列获得子序列的所有不同的方式，其中n = 5，并且每个时间步的词元对应于一个字符。</p>
<p>那么，我们应该选择哪一个呢？如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的；因此，我们可以从随机偏移量开始划分序列，以同时获得覆盖性（coverage）和随机性（randomness）。下面介绍两个策略：<strong>随机采样，顺序分区</strong></p>
<h4 id="随机采样">随机采样</h4>
<p>在此策略下，每个样本都是在原始的长序列上任意捕获的子序列。在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。<strong>对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元</strong>，因此标签是移位了一个词元的原始序列。</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">seq_fata_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param corpus:</span></span><br><span class="line"><span class="string">    :param batch_size: 每个小批量中子序列样本的数量</span></span><br><span class="line"><span class="string">    :param num_steps: 每个子序列中预定义的时间步数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从语料库中随机选择一个片段作为开始, 切片内容包括num_steps - 1</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    <span class="comment"># 计算基于当前语料库长度和序列长度能够生成的序列数量</span></span><br><span class="line">    <span class="comment"># 减去1，是因为我们需要考虑标签</span></span><br><span class="line">    num_sequences = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># 创建一个列表，包含所有序列的起始索引，即长度为num_steps的子序列的起始索引</span></span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_sequences * num_steps, num_steps))</span><br><span class="line">    <span class="comment"># 在随机抽样的迭代过程中，</span></span><br><span class="line">    <span class="comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span></span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个辅助函数，根据给定的起始位置从语料库中提取序列</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="comment"># 返回从pos位置开始的长度为num_steps的序列</span></span><br><span class="line">        <span class="keyword">return</span> corpus[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    num_batches = num_sequences // batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_batches * batch_size, batch_size):</span><br><span class="line">        <span class="comment"># initial_indices包含子序列的随机起始索引</span></span><br><span class="line">        initial_indices_per_batch = initial_indices[i: i + batch_size]  <span class="comment"># 从打乱顺序的起始索引列表中获取当前批次的起始索引</span></span><br><span class="line">        <span class="comment"># 根据当前批次中每个序列的起始索引，创建X（输入序列）和Y（目标序列）</span></span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="comment"># 生成并提供输入和目标序列的张量表示</span></span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 生成一个0 ~ 34的序列，并设置批量大小 = 2，时间步数 = 5</span></span><br><span class="line">    <span class="comment"># 这样可以生成(35 - 1) // 5 = 6个特征-标签子序列对</span></span><br><span class="line">    seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;每个小批量中有两个子序列对:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_fata_iter_random(seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;第%d个\&quot;特征-标签\&quot;子序列对小批量&quot;</span> % i)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;X: <span class="subst">&#123;X&#125;</span>, \nY: <span class="subst">&#123;Y&#125;</span>&#x27;</span>)</span><br><span class="line">        i += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">每个小批量中有两个子序列对:</span><br><span class="line">第1个&quot;特征-标签&quot;子序列对小批量</span><br><span class="line">X: tensor([[0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9]]), </span><br><span class="line">Y: tensor([[ 1,  2,  3,  4,  5],</span><br><span class="line">        [ 6,  7,  8,  9, 10]])</span><br><span class="line">第2个&quot;特征-标签&quot;子序列对小批量</span><br><span class="line">X: tensor([[20, 21, 22, 23, 24],</span><br><span class="line">        [10, 11, 12, 13, 14]]), </span><br><span class="line">Y: tensor([[21, 22, 23, 24, 25],</span><br><span class="line">        [11, 12, 13, 14, 15]])</span><br><span class="line">第3个&quot;特征-标签&quot;子序列对小批量</span><br><span class="line">X: tensor([[15, 16, 17, 18, 19],</span><br><span class="line">        [25, 26, 27, 28, 29]]), </span><br><span class="line">Y: tensor([[16, 17, 18, 19, 20],</span><br><span class="line">        [26, 27, 28, 29, 30]])</span><br></pre></td></tr></table></figure>
<h4 id="顺序分区">顺序分区</h4>
<p>在迭代过程中，除了对原始序列可以随机抽样外，我们还<strong>可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的</strong>。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为<strong>顺序分区</strong>。</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;顺序分区策略生成小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始划分序列</span></span><br><span class="line">    <span class="comment"># 生成一个随机偏移量，用于乱序数据</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>, num_steps)</span><br><span class="line">    <span class="comment"># 计算基于批次大小可处理的令牌数量</span></span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    <span class="comment"># 从乱序后的数据中创建输入序列Xs</span></span><br><span class="line">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class="line">    <span class="comment"># 从乱序后的数据中创建目标序列Ys，相比Xs向后移动了一个位置</span></span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>: offset + <span class="number">1</span> + num_tokens])</span><br><span class="line">    <span class="comment"># 将序列Xs和Ys重塑为批次大小，以便于训练</span></span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 计算批次的数量</span></span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps</span><br><span class="line">    <span class="comment"># 遍历所有序列，生成批次数据</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        <span class="comment"># X是当前批次的输入序列，长度为num_steps</span></span><br><span class="line">        X = Xs[:, i: i + num_steps]</span><br><span class="line">        <span class="comment"># Y是当前批次的目标序列，长度为num_steps</span></span><br><span class="line">        Y = Ys[:, i: i + num_steps]</span><br><span class="line">        <span class="comment"># 产出当前批次的输入序列X和目标序列Y</span></span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 数据配置与随机采样策略一致</span></span><br><span class="line">    seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_sequential(seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;X: <span class="subst">&#123;X&#125;</span>,\nY: <span class="subst">&#123;Y&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">X: tensor([[ 2,  3,  4,  5,  6],</span><br><span class="line">        [18, 19, 20, 21, 22]]),</span><br><span class="line">Y: tensor([[ 3,  4,  5,  6,  7],</span><br><span class="line">        [19, 20, 21, 22, 23]])</span><br><span class="line">X: tensor([[ 7,  8,  9, 10, 11],</span><br><span class="line">        [23, 24, 25, 26, 27]]),</span><br><span class="line">Y: tensor([[ 8,  9, 10, 11, 12],</span><br><span class="line">        [24, 25, 26, 27, 28]])</span><br><span class="line">X: tensor([[12, 13, 14, 15, 16],</span><br><span class="line">        [28, 29, 30, 31, 32]]),</span><br><span class="line">Y: tensor([[13, 14, 15, 16, 17],</span><br><span class="line">        [29, 30, 31, 32, 33]])</span><br></pre></td></tr></table></figure>
<p><strong>将两种策略整合出辅助类</strong></p>
<p><code>util.py</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SeqDataLoader</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_steps, use_random, max_tokens</span>):</span><br><span class="line">        <span class="keyword">if</span> use_random:</span><br><span class="line">            <span class="variable language_">self</span>.data_iter_fn = d2l.seq_data_iter_random</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.data_iter_fn = d2l.seq_data_iter_sequential</span><br><span class="line">        <span class="variable language_">self</span>.corpus, <span class="variable language_">self</span>.vocab = d2l.load_corpus_time_machine(max_tokens)</span><br><span class="line">        <span class="variable language_">self</span>.batch_size, <span class="variable language_">self</span>.num_steps = batch_size, num_steps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data_iter_fn(<span class="variable language_">self</span>.corpus, batch_size=<span class="variable language_">self</span>.batch_size, num_steps=<span class="variable language_">self</span>.num_steps)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_time_machine</span>(<span class="params">batch_size, num_steps, use_random=<span class="literal">False</span>, max_tokens = <span class="number">1e5</span></span>):</span><br><span class="line">    data_iter = SeqDataLoader(batch_size, num_steps, use_random, max_tokens)</span><br><span class="line">    <span class="keyword">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></table></figure>
<h1>循环神经网络</h1>
<p>下面我们开始正式介绍循环神经网络！</p>
<p>理论部分详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#recurrent-neural-networks">9.4. Recurrent Neural Networks — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<p>无隐状态的神经网络：比如一个有单隐藏层的多层感知机</p>
<p>有隐状态的循环神经网络：<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#recurrent-neural-networks-with-hidden-states">…</a></p>
<h2 id="基于循环神经网络的字符级语言模型、困惑度">基于循环神经网络的字符级语言模型、困惑度</h2>
<p>详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#rnn-based-character-level-language-models">9.4. Recurrent Neural Networks — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h2 id="从零实现循环神经网络">从零实现循环神经网络</h2>
<p>此处将从头开始基于循环神经网络实现字符级语言模型，在时光机器数据集上训练。</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hidden, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    初始化神经网络的模型参数</span></span><br><span class="line"><span class="string">    :param vocab_size 语言模型的输入输出来自同一个词表，因此他们具有相同的维度即词表大小</span></span><br><span class="line"><span class="string">    :param num_hidden 隐藏层单元数，可调的超参数</span></span><br><span class="line"><span class="string">    :param device</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = normal((num_inputs, num_hidden))</span><br><span class="line">    W_hh = normal((num_hidden, num_hidden))</span><br><span class="line">    b_h = torch.zeros(num_hidden, device=device)</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hidden, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_rnn_state</span>(<span class="params">batch_size, num_hidden, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;初始化时返回隐状态，返回值全0填充&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.zeros((batch_size, num_hidden), device=device),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># rnn函数定义了如何在一个时间步内计算隐状态和输出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rnn</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    <span class="comment"># inputs形状: (时间步数量，批量大小，词表大小)</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment"># X的形状: (批量大小，词表大小)</span></span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        <span class="comment"># 使用tanh激活函数</span></span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 封装上述函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModelScratch</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从零实现循环神经网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hidden, device, get_params, init_state, forward_fn</span>):</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size, <span class="variable language_">self</span>.num_hidden = vocab_size, num_hidden</span><br><span class="line">        <span class="variable language_">self</span>.params = get_params(vocab_size, num_hidden, device)</span><br><span class="line">        <span class="variable language_">self</span>.init_state, <span class="variable language_">self</span>.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = F.one_hot(X.T, <span class="variable language_">self</span>.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.forward_fn(X, state, <span class="variable language_">self</span>.params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.init_state(batch_size, <span class="variable language_">self</span>.num_hidden, device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch8</span>(<span class="params">prefix, num_pred, net, vocab, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测字符串prefix后面的内容&quot;&quot;&quot;</span></span><br><span class="line">    state = net.begin_state(batch_size=<span class="number">1</span>, device=device)</span><br><span class="line">    outputs = [vocab[prefix[<span class="number">0</span>]]]</span><br><span class="line">    get_input = <span class="keyword">lambda</span>: torch.tensor([outputs[-<span class="number">1</span>]], device=device).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 预热期</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> prefix[<span class="number">1</span>:]:</span><br><span class="line">        _, state = net(get_input(), state)</span><br><span class="line">        outputs.append(vocab[y])</span><br><span class="line">    <span class="comment"># 预测num_pred步</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_pred):</span><br><span class="line">        y, state = net(get_input(), state)</span><br><span class="line">        outputs.append(<span class="built_in">int</span>(y.argmax(dim=<span class="number">1</span>).reshape(<span class="number">1</span>)))  <span class="comment"># 把向量转化为索引</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="keyword">for</span> i <span class="keyword">in</span> outputs])  <span class="comment"># 索引转化为token</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grad_clipping</span>(<span class="params">net, theta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        params = net.params</span><br><span class="line">    norm = torch.sqrt(<span class="built_in">sum</span>(torch.<span class="built_in">sum</span>(p.grad ** <span class="number">2</span>) <span class="keyword">for</span> p <span class="keyword">in</span> params))</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updator, device, use_random_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练网络一个迭代周期&quot;&quot;&quot;</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, d2l.Timer()</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失之和</span></span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            <span class="comment"># 如果state还没有初始化或者使用随机采样</span></span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updator, torch.optim.Optimizer):</span><br><span class="line">            updator.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updator.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 因为已经调用过mean方法</span></span><br><span class="line">            updator(batch_size=<span class="number">1</span>)</span><br><span class="line">        metric.add(y.numel() * l, y.numel())</span><br><span class="line">    <span class="comment"># 第一个返回值是困惑度perplexity，用于衡量语言模型的性能</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环神经网络模型的训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device, use_random=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型&quot;&quot;&quot;</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;perplexity&#x27;</span>, legend=[<span class="string">&#x27;train&#x27;</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updator = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updator = <span class="keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)  <span class="comment"># 接受一个参数批量大小</span></span><br><span class="line">    <span class="comment"># 预测函数</span></span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)  <span class="comment"># 接受一个参数初始序列</span></span><br><span class="line">    <span class="comment"># 训练和预测</span></span><br><span class="line">    perplexity, speed = -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        perplexity, speed = train_epoch_ch8(net, train_iter, loss, updator, device, use_random)</span><br><span class="line">        <span class="comment"># print(f&#x27;perplexity:  &#123;perplexity&#125;&#x27;)</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (perplexity,))</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化困惑度动态迭代结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;perplexity:  <span class="subst">&#123;perplexity&#125;</span>, <span class="subst">&#123;speed&#125;</span> tokens / per second, on <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predict &#x27;time traveller&#x27;:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&quot;time traveller&quot;</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&quot;traveller&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 之前一直将词元表示为一个索引, 但这样会使得模型难以学习(一个标量), 因此引入独热编码将词元映射为向量(互不相同的索引映射为互不相同的单位向量)</span></span><br><span class="line">    <span class="built_in">print</span>(F.one_hot(torch.tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>]), <span class="built_in">len</span>(vocab)))</span><br><span class="line">    <span class="comment"># 每次采样的小批量数据形状是二维张量：（批量大小，时间步数）</span></span><br><span class="line">    <span class="comment"># one_hot函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小</span></span><br><span class="line">    <span class="comment"># 转换输入的维度，以便获得形状为（时间步数，批量大小，词表大小）的输出</span></span><br><span class="line">    X = torch.arange(<span class="number">10</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="built_in">print</span>(F.one_hot(X.T, <span class="built_in">len</span>(vocab)).shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 验证一下我们手搓的循环神经网络是否输出正确的形状</span></span><br><span class="line">    num_hidden = <span class="number">512</span></span><br><span class="line">    net = RNNModelScratch(<span class="built_in">len</span>(vocab), num_hidden, d2l.try_gpu(), get_params, init_rnn_state, rnn)</span><br><span class="line">    state = net.begin_state(X.shape[<span class="number">0</span>], d2l.try_gpu())</span><br><span class="line">    Y, new_state = net(X.to(d2l.try_gpu()), state)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(new_state))</span><br><span class="line">    <span class="built_in">print</span>(new_state[<span class="number">0</span>].shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不训练直接预测</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predict without training:\ntime traveller ...? -&gt;&quot;</span>)</span><br><span class="line">    pred = predict_ch8(<span class="string">&quot;time traveller &quot;</span>, <span class="number">10</span>, net, vocab, d2l.try_gpu())  <span class="comment"># 生成离谱的 预测结果</span></span><br><span class="line">    <span class="built_in">print</span>(pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练后再预测</span></span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;not random sample:&quot;</span>)</span><br><span class="line">    train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), <span class="literal">False</span>)  <span class="comment"># 不使用随机采样</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;random sample:&quot;</span>)</span><br><span class="line">    train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), <span class="literal">True</span>)  <span class="comment"># 使用随机采样</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span><br><span class="line">         0, 0, 0, 0],</span><br><span class="line">        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span><br><span class="line">         0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span><br><span class="line">         0, 0, 0, 0]])</span><br><span class="line">torch.Size([5, 2, 28])</span><br><span class="line"></span><br><span class="line">torch.Size([10, 28])</span><br><span class="line">1</span><br><span class="line">torch.Size([2, 512])</span><br><span class="line">predict without training:</span><br><span class="line">time traveller ...? -&gt;</span><br><span class="line">time traveller jckmckmckm</span><br><span class="line">not random sample:</span><br><span class="line">perplexity:  1.0546326766270984, 14819.88615243743 tokens / per second, on cpu</span><br><span class="line">predict &#x27;time traveller&#x27;:</span><br><span class="line">time travelleryou can show black is white by argument said filby</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></table></figure>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/not_random.png" alt="image-20240923151710672"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">random sample:</span><br><span class="line">perplexity:  1.30568447689415, 14425.676215978849 tokens / per second, on cpu</span><br><span class="line">predict &#x27;time traveller&#x27;:</span><br><span class="line">time travellerit s against reason said filbywan a oft reaverathe</span><br><span class="line">travellerit s against reason said filbywhat had in an at re</span><br></pre></td></tr></table></figure>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/random.png" alt="image-20240923152727857"></p>
<h2 id="简洁实现的循环神经网络">简洁实现的循环神经网络</h2>
<p>此节中我们将使用深度学习框架中的高级API实现循环神经网络</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为一个完整的循环神经网络模型定义一个RNNModule类</span></span><br><span class="line"><span class="comment"># 由于rnn_layer只包含隐藏的循环层，因此还需要创建一个单独的输出层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = rnn_layer</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = vocab_size</span><br><span class="line">        <span class="variable language_">self</span>.num_hidden = <span class="variable language_">self</span>.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 如果RNN是双向的，num_directions应该是 2，否则是 1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.rnn.bidirectional:</span><br><span class="line">            <span class="variable language_">self</span>.num_directions = <span class="number">1</span></span><br><span class="line">            <span class="variable language_">self</span>.linear = nn.Linear(<span class="variable language_">self</span>.num_hidden, <span class="variable language_">self</span>.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.num_directions = <span class="number">2</span></span><br><span class="line">            <span class="variable language_">self</span>.linear = nn.Linear(<span class="variable language_">self</span>.num_hidden * <span class="number">2</span>, <span class="variable language_">self</span>.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, state</span>):</span><br><span class="line">        X = F.one_hot(inputs.T.long(), <span class="variable language_">self</span>.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = <span class="variable language_">self</span>.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层首先将Y的形状改为(num_steps * batch_size, num_hidden)</span></span><br><span class="line">        <span class="comment"># 它的输出形状是(num_steps * batch_size, vocab_size)</span></span><br><span class="line">        output = <span class="variable language_">self</span>.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span><br><span class="line">        <span class="comment"># LSTM: 长短期记忆网络：一种特殊的循环神经网络</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(<span class="variable language_">self</span>.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> torch.zeros((<span class="variable language_">self</span>.num_directions * <span class="variable language_">self</span>.rnn.num_layers,</span><br><span class="line">                                batch_size, <span class="variable language_">self</span>.num_hidden), device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((<span class="variable language_">self</span>.num_directions * <span class="variable language_">self</span>.rnn.num_layers, batch_size, <span class="variable language_">self</span>.num_hidden),</span><br><span class="line">                                device=device), torch.zeros((<span class="variable language_">self</span>.num_directions * <span class="variable language_">self</span>.rnn.num_layers,</span><br><span class="line">                                                             batch_size, <span class="variable language_">self</span>.num_hidden), device=device))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line">    num_hidden = <span class="number">256</span></span><br><span class="line">    <span class="comment"># 定义模型</span></span><br><span class="line">    rnn_layer = nn.RNN(<span class="built_in">len</span>(vocab), num_hidden)</span><br><span class="line">    <span class="comment"># 初始化隐状态</span></span><br><span class="line">    state = torch.zeros((<span class="number">1</span>, batch_size, num_hidden))</span><br><span class="line">    <span class="built_in">print</span>(state.shape)</span><br><span class="line"></span><br><span class="line">    X = torch.rand(size=(num_steps, batch_size, <span class="built_in">len</span>(vocab)))</span><br><span class="line">    Y, state_new = rnn_layer(X, state)  <span class="comment"># rnn_layer就是之前的net</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练与预测</span></span><br><span class="line">    device = d2l.try_gpu()</span><br><span class="line">    net = RNNModel(rnn_layer, vocab_size=<span class="built_in">len</span>(vocab))</span><br><span class="line">    net = net.to(device=device)</span><br><span class="line">    predict = d2l.predict_ch8(<span class="string">&quot;time traveller&quot;</span>, <span class="number">10</span>, net, vocab, device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predict of &#x27;time traveller&#x27; without training:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict)  <span class="comment"># 这样得到的是一个胡扯的结果，因为没有训练</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;start training:&quot;</span>)</span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">0.1</span></span><br><span class="line">    d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device=device)</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化困惑度</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([1, 32, 256])</span><br><span class="line">predict of &#x27;time traveller&#x27; without training:</span><br><span class="line">time travellerxvxcxvfxxx</span><br><span class="line">start training:</span><br><span class="line">perplexity 4.0, 50341.3 tokens/sec on cpu</span><br><span class="line">time travelleryou ong thave dery ald har he hare ard an therimet</span><br><span class="line">traveller and thas ed ane tore red ane trever tinntalle som</span><br></pre></td></tr></table></figure>
<p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/rnn_api.png" alt="image-20240923163751477"></p>
<h1>通过时间反向传播</h1>
<p>本部分为纯理论介绍，详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#backpropagation-through-time">9.7. Backpropagation Through Time — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<p>(•‿•)</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://nju-ymhui.github.io/ymhui.github.io">画船听雨眠y</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://nju-ymhui.github.io/ymhui.github.io/2024/09/20/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">https://nju-ymhui.github.io/ymhui.github.io/2024/09/20/循环神经网络/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://nju-ymhui.github.io/ymhui.github.io" target="_blank">(๑>ᴗ<๑)</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/ymhui.github.io/tags/pytorch/">pytorch</a><a class="post-meta__tags" href="/ymhui.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-share"><div class="social-share" data-image="/ymhui.github.io/img/myicon.ico" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>真的不考虑支持一下吗 (╯︵╰,)</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/ymhui.github.io/img/qrcode.png" target="_blank"><img class="post-qr-code-img" src="/ymhui.github.io/img/qrcode.png" alt="微信支付"/></a><div class="post-qr-code-desc">微信支付</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="prev-post pull-left" href="/ymhui.github.io/2024/09/20/%E8%B4%AA%E5%BF%83/" title="贪心"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">贪心</div></div></a><a class="next-post pull-right" href="/ymhui.github.io/2024/09/16/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="注意力机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">注意力机制</div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a href="/ymhui.github.io/2024/09/16/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="卷积神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-16</div><div class="title">卷积神经网络</div></div></a><a href="/ymhui.github.io/2024/09/15/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="多层感知机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-15</div><div class="title">多层感知机</div></div></a><a href="/ymhui.github.io/2024/09/16/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="注意力机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-16</div><div class="title">注意力机制</div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="/ymhui.github.io/img/myicon.ico" onerror="this.onerror=null;this.src='/ymhui.github.io/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">画船听雨眠y</div><div class="author-info-description">美好的一天从打代码开始结束</div><div class="site-data"><a href="/ymhui.github.io/archives/"><div class="headline">文章</div><div class="length-num">44</div></a><a href="/ymhui.github.io/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/ymhui.github.io/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/NJU-ymhui"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/NJU-ymhui" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">循环神经网络前导</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">序列模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.1.</span> <span class="toc-text">自回归模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8BMarkov"><span class="toc-number">2.1.2.</span> <span class="toc-text">马尔可夫模型Markov</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E9%83%A8%E5%88%86"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">理论部分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">训练</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">2.2.</span> <span class="toc-text">文本预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.</span> <span class="toc-text">语言模型和数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%BB%9F%E8%AE%A1"><span class="toc-number">2.3.1.</span> <span class="toc-text">自然语言统计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E9%95%BF%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE"><span class="toc-number">2.3.2.</span> <span class="toc-text">读取长序列数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">随机采样</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%BA%E5%BA%8F%E5%88%86%E5%8C%BA"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">顺序分区</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%97%E7%AC%A6%E7%BA%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E3%80%81%E5%9B%B0%E6%83%91%E5%BA%A6"><span class="toc-number">3.1.</span> <span class="toc-text">基于循环神经网络的字符级语言模型、困惑度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.2.</span> <span class="toc-text">从零实现循环神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.3.</span> <span class="toc-text">简洁实现的循环神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">通过时间反向传播</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2025/01/03/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/" title="软件测试">软件测试</a><time datetime="2025-01-03T06:26:03.000Z" title="发表于 2025-01-03 14:26:03">2025-01-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2025/01/02/C-%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%E2%80%94%E2%80%94%E7%AE%80%E7%AD%94%E9%A2%98%E9%83%A8%E5%88%86/" title="C++期末复习——简答题部分">C++期末复习——简答题部分</a><time datetime="2025-01-02T05:34:59.000Z" title="发表于 2025-01-02 13:34:59">2025-01-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2024/12/25/C-%E5%B8%B8%E7%94%A8map/" title="C++常用map">C++常用map</a><time datetime="2024-12-25T11:09:20.000Z" title="发表于 2024-12-25 19:09:20">2024-12-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2024/12/25/C-%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/" title="C++容器常用操作">C++容器常用操作</a><time datetime="2024-12-25T11:01:10.000Z" title="发表于 2024-12-25 19:01:10">2024-12-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2024/12/25/C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C/" title="C++字符串操作">C++字符串操作</a><time datetime="2024-12-25T09:45:56.000Z" title="发表于 2024-12-25 17:45:56">2024-12-25</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 画船听雨眠y</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/ymhui.github.io/js/utils.js"></script><script src="/ymhui.github.io/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    const mathElements = document.querySelectorAll('#article-container .katex')
    mathElements.length && mathElements.forEach((el) => {
      el.classList.add('katex-show')
    })
  }
  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
  }

  showKatex()
})()</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/ymhui.github.io/js/search/local-search.js"></script></div></div></body></html>