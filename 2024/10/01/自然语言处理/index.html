<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>自然语言处理 | (๑&gt;ᴗ&lt;๑)</title><meta name="author" content="画船听雨眠y"><meta name="copyright" content="画船听雨眠y"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="写在前面 参考书籍   Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. Dive into Deep Learning. 2020. 简介 - Dive-into-DL-PyTorch (tangshusen.me) 自然语言处理source code: NJU-ymhui&#x2F;DeepLearning: Deep Lea">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言处理">
<meta property="og:url" content="https://nju-ymhui.github.io/ymhui.github.io/2024/10/01/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/index.html">
<meta property="og:site_name" content="(๑&gt;ᴗ&lt;๑)">
<meta property="og:description" content="写在前面 参考书籍   Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. Dive into Deep Learning. 2020. 简介 - Dive-into-DL-PyTorch (tangshusen.me) 自然语言处理source code: NJU-ymhui&#x2F;DeepLearning: Deep Lea">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://nju-ymhui.github.io/ymhui.github.io/img/myicon.ico">
<meta property="article:published_time" content="2024-10-01T02:15:34.000Z">
<meta property="article:modified_time" content="2024-10-06T12:28:11.410Z">
<meta property="article:author" content="画船听雨眠y">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://nju-ymhui.github.io/ymhui.github.io/img/myicon.ico"><link rel="shortcut icon" href="/ymhui.github.io/img/myicon.ico"><link rel="canonical" href="https://nju-ymhui.github.io/ymhui.github.io/2024/10/01/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/ymhui.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{
      const saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
      
      window.btf = {
        saveToLocal: saveToLocal,
        getScript: (url, attr = {}) => new Promise((resolve, reject) => {
          const script = document.createElement('script')
          script.src = url
          script.async = true
          script.onerror = reject
          script.onload = script.onreadystatechange = function() {
            const loadState = this.readyState
            if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
            script.onload = script.onreadystatechange = null
            resolve()
          }

          Object.keys(attr).forEach(key => {
            script.setAttribute(key, attr[key])
          })

          document.head.appendChild(script)
        }),

        getCSS: (url, id = false) => new Promise((resolve, reject) => {
          const link = document.createElement('link')
          link.rel = 'stylesheet'
          link.href = url
          if (id) link.id = id
          link.onerror = reject
          link.onload = link.onreadystatechange = function() {
            const loadState = this.readyState
            if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
            link.onload = link.onreadystatechange = null
            resolve()
          }
          document.head.appendChild(link)
        }),

        addGlobalFn: (key, fn, name = false, parent = window) => {
          const pjaxEnable = false
          if (!pjaxEnable && key.startsWith('pjax')) return

          const globalFn = parent.globalFn || {}
          const keyObj = globalFn[key] || {}
    
          if (name && keyObj[name]) return
    
          name = name || Object.keys(keyObj).length
          keyObj[name] = fn
          globalFn[key] = keyObj
          parent.globalFn = globalFn
        }
      }
    
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode
      
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/ymhui.github.io/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '自然语言处理',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-06 20:28:11'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/ymhui.github.io/img/myicon.ico" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><a href="/ymhui.github.io/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/ymhui.github.io/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/ymhui.github.io/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/ymhui.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ymhui.github.io/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/ymhui.github.io/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/ymhui.github.io/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ymhui.github.io/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/ymhui.github.io/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/ymhui.github.io/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/ymhui.github.io/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/ymhui.github.io/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/ymhui.github.io/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/ymhui.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://blue-whale-backend.oss-cn-nanjing.aliyuncs.com/back.jpg);"><nav id="nav"><span id="blog-info"><a href="/ymhui.github.io/" title="(๑&gt;ᴗ&lt;๑)"><span class="site-name">(๑&gt;ᴗ&lt;๑)</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/ymhui.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ymhui.github.io/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/ymhui.github.io/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/ymhui.github.io/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ymhui.github.io/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/ymhui.github.io/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/ymhui.github.io/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/ymhui.github.io/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/ymhui.github.io/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/ymhui.github.io/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/ymhui.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">自然语言处理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-01T02:15:34.000Z" title="发表于 2024-10-01 10:15:34">2024-10-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-06T12:28:11.410Z" title="更新于 2024-10-06 20:28:11">2024-10-06</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>39分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="自然语言处理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><blockquote>
<p>参考书籍 </p>
</blockquote>
<p><a target="_blank" rel="noopener" href="http://www.d2l.ai/">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p>
<p><a target="_blank" rel="noopener" href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p>
<h1 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h1><p><strong>source code</strong>: <a target="_blank" rel="noopener" href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui&#x2F;DeepLearning: Deep Learning with pytorch (github.com)</a></p>
<p><strong>use git to clone</strong>: <a target="_blank" rel="noopener" href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p>
<p><code>/NLP</code></p>
<blockquote>
<p>word2vec_dataset.py	word2vec_pretraining.py	fasttext.py	similarity_compare.py	BERT.py	BERT_pretraining_dataset.py</p>
</blockquote>
<h2 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h2><p>在自然语言系统中，<strong>词</strong>是意义的基本单位。下面介绍一个新的概念：<strong>词向量</strong>，这是用于表示单词意义的向量。那么现在还需要<strong>将每个词映射到对应的词向量</strong>，这项技术就是<strong>词嵌入</strong>。</p>
<p>下面将介绍一些将词映射到向量的相关技术。</p>
<h3 id="为什么不再使用独热向量"><a href="#为什么不再使用独热向量" class="headerlink" title="为什么不再使用独热向量"></a>为什么不再使用独热向量</h3><p>在之前做机器翻译时曾尝试用过独热编码的向量，但现在来看，这并不是一个好的选择。一个重要原因是独热向量不能表达不同词之间的相似度，比如经常使用的余弦相似度，它对两个向量<strong>x, y</strong>使用余弦表示两个向量之间的相似度</p>
<p><img src="/ymhui.github.io/2024/09/13/compare%E5%87%BD%E6%95%B0/.png" alt="image-20241001114419714"></p>
<p>然而根据独热向量的定义可以得知，任意两个不同词的独热向量的余弦为0，即独热向量不能编码词之间的相似性。</p>
<h3 id="自监督的word2vec"><a href="#自监督的word2vec" class="headerlink" title="自监督的word2vec"></a>自监督的word2vec</h3><ul>
<li>跳元模型</li>
<li>连续词袋模型</li>
</ul>
<h4 id="跳元模型Skip-Gram"><a href="#跳元模型Skip-Gram" class="headerlink" title="跳元模型Skip-Gram"></a>跳元模型Skip-Gram</h4><p>详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#the-skip-gram-model">15.1. Skip-Gram — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h4 id="连续词袋CBOW"><a href="#连续词袋CBOW" class="headerlink" title="连续词袋CBOW"></a>连续词袋CBOW</h4><p>详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#the-continuous-bag-of-words-cbow-model">15.1. CBOW — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h2 id="近似训练"><a href="#近似训练" class="headerlink" title="近似训练"></a>近似训练</h2><h3 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h3><p>详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html#negative-sampling">15.2. Negative-Sampling — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h3 id="层序Softmax"><a href="#层序Softmax" class="headerlink" title="层序Softmax"></a>层序Softmax</h3><p>层序softmax是另一种近似训练的方法，使用二叉树，其中树的每个叶节点表示词表<em>V</em>中的一个词。</p>
<p><img src="/ymhui.github.io/2024/09/13/compare%E5%87%BD%E6%95%B0/tmax.png" alt="image-20241001122958558"></p>
<p>其原理，详见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html#hierarchical-softmax">15.2. Softmax — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h2 id="预训练词嵌入的数据集"><a href="#预训练词嵌入的数据集" class="headerlink" title="预训练词嵌入的数据集"></a>预训练词嵌入的数据集</h2><p>直接上代码。</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_ptb</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将ptb数据集加载到文本行的列表中&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;ptb&#x27;</span>)</span><br><span class="line">    <span class="comment"># 读取训练集</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;ptb.train.txt&#x27;</span>)) <span class="keyword">as</span> f:</span><br><span class="line">        raw_text = f.read()</span><br><span class="line">    <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> raw_text.split(<span class="string">&#x27;\n&#x27;</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">subsample</span>(<span class="params">sentences, vocab</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下采样高频词&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 排除未知词元&lt;unk&gt;</span></span><br><span class="line">    sentences = [[token <span class="keyword">for</span> token <span class="keyword">in</span> line <span class="keyword">if</span> vocab[token] != vocab.unk] <span class="keyword">for</span> line <span class="keyword">in</span> sentences]</span><br><span class="line">    counter = d2l.count_corpus(sentences)</span><br><span class="line">    num_tokens = <span class="built_in">sum</span>(counter.values())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果在下采样期间保留词元，返回True</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">keep</span>(<span class="params">token</span>):</span><br><span class="line">        <span class="keyword">return</span> random.uniform(<span class="number">0</span>, <span class="number">1</span>) &lt; math.sqrt(<span class="number">1e-4</span> / counter[token] * num_tokens)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [[token <span class="keyword">for</span> token <span class="keyword">in</span> line <span class="keyword">if</span> keep(token)] <span class="keyword">for</span> line <span class="keyword">in</span> sentences], counter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compare_counts</span>(<span class="params">token, sentences, subsampled</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;比较下采样前后某个token的频次&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="string">f&#x27;&quot;<span class="subst">&#123;token&#125;</span>&quot; count before subsample: <span class="subst">&#123;<span class="built_in">sum</span>(l.count(token) <span class="keyword">for</span> l <span class="keyword">in</span> sentences)&#125;</span>\n&#x27;</span></span><br><span class="line">            <span class="string">f&#x27;after subsample: <span class="subst">&#123;<span class="built_in">sum</span>(l.count(token) <span class="keyword">for</span> l <span class="keyword">in</span> subsampled)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 中心词和上下文词的提取</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_centers_and_contexts</span>(<span class="params">corpus, max_window_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回跳元模型中的中心词和上下文词&quot;&quot;&quot;</span></span><br><span class="line">    centers, contexts = [], []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> corpus:</span><br><span class="line">        <span class="comment"># 要形成”中心词-上下文词“对，每个句子至少需要有两个词</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(line) &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        centers += line</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(line)):</span><br><span class="line">            window_size = random.randint(<span class="number">1</span>, max_window_size)</span><br><span class="line">            indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">max</span>(<span class="number">0</span>, i - window_size), <span class="built_in">min</span>(<span class="built_in">len</span>(line), i + <span class="number">1</span> + window_size)))</span><br><span class="line">            <span class="comment"># 从上下文词中排除中心词</span></span><br><span class="line">            indices.remove(i)</span><br><span class="line">            contexts.append([line[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> indices])</span><br><span class="line">    <span class="keyword">return</span> centers, contexts</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了根据预定义的分布对噪声词进行采样，我们定义如下RandomGenerator类</span></span><br><span class="line"><span class="comment"># 其中采用分布通过变量sampling_weights传递</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomGenerator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据n个采样权重在&#123;1, ..., n&#125;中随机抽取&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sampling_weights</span>):</span><br><span class="line">        <span class="comment"># Exclude</span></span><br><span class="line">        <span class="variable language_">self</span>.population = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(sampling_weights) + <span class="number">1</span>))</span><br><span class="line">        <span class="variable language_">self</span>.sampling_weights = sampling_weights</span><br><span class="line">        <span class="variable language_">self</span>.candidates = []</span><br><span class="line">        <span class="variable language_">self</span>.i = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">draw</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.i == <span class="built_in">len</span>(<span class="variable language_">self</span>.candidates):</span><br><span class="line">            <span class="comment"># 缓存k个随机采样结果</span></span><br><span class="line">            <span class="variable language_">self</span>.candidates = random.choices(<span class="variable language_">self</span>.population, <span class="variable language_">self</span>.sampling_weights, k=<span class="number">10000</span>)</span><br><span class="line">            <span class="variable language_">self</span>.i = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.candidates[<span class="variable language_">self</span>.i - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于一对中心词和上下文词，我们随机抽取了K个（实验中为5个）噪声词。</span></span><br><span class="line"><span class="comment"># 根据word2vec论文中的建议，将噪声词w的采样概率P(w)设置为其在字典中的相对频率，其幂为0.75</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_negatives</span>(<span class="params">all_contexts, vocab, counter, K</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回负采样中的噪声词&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 索引为1、2、... (索引0是词表中排除的未知标记)</span></span><br><span class="line">    sampling_weights = [counter[vocab.to_tokens(i)] ** <span class="number">0.75</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(vocab))]</span><br><span class="line">    all_negatives, generator = [], RandomGenerator(sampling_weights)</span><br><span class="line">    <span class="keyword">for</span> contexts <span class="keyword">in</span> all_contexts:</span><br><span class="line">        negatives = []</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(negatives) &lt; <span class="built_in">len</span>(contexts) * K:</span><br><span class="line">            neg = generator.draw()</span><br><span class="line">            <span class="comment"># 噪声词不能是上下文词</span></span><br><span class="line">            <span class="keyword">if</span> neg <span class="keyword">not</span> <span class="keyword">in</span> contexts:  <span class="comment"># 通过这个条件保证</span></span><br><span class="line">                negatives.append(neg)</span><br><span class="line">        all_negatives.append(negatives)</span><br><span class="line">    <span class="keyword">return</span> all_negatives</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batchify</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回带有负采样的跳元模型的小批量样本&quot;&quot;&quot;</span></span><br><span class="line">    max_len = <span class="built_in">max</span>(<span class="built_in">len</span>(c) + <span class="built_in">len</span>(n) <span class="keyword">for</span> _, c, n <span class="keyword">in</span> data)</span><br><span class="line">    centres, contexts_negatives, masks, labels = [], [], [], []</span><br><span class="line">    <span class="keyword">for</span> center, context, negative <span class="keyword">in</span> data:</span><br><span class="line">        cur_len = <span class="built_in">len</span>(context) + <span class="built_in">len</span>(negative)</span><br><span class="line">        centres += [center]</span><br><span class="line">        contexts_negatives += [context + negative + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        masks += [[<span class="number">1</span>] * cur_len + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        labels += [[<span class="number">1</span>] * <span class="built_in">len</span>(context) + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(context))]</span><br><span class="line">    <span class="keyword">return</span> (torch.tensor(centres).reshape((-<span class="number">1</span>, <span class="number">1</span>)), torch.tensor(contexts_negatives),</span><br><span class="line">            torch.tensor(masks), torch.tensor(labels))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 整合一下代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_ptb</span>(<span class="params">batch_size, max_window_size, num_noise_words</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载PTB数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    sentences = read_ptb()</span><br><span class="line">    vocab = d2l.Vocab(sentences, min_freq=<span class="number">10</span>)</span><br><span class="line">    subsampled, counter = subsample(sentences, vocab)</span><br><span class="line">    corpus = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> subsampled]</span><br><span class="line">    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)</span><br><span class="line">    all_negatives = get_negatives(all_contexts, vocab, counter, num_noise_words)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">PTBDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, centers, contexts, negatives</span>):</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(centers) == <span class="built_in">len</span>(contexts) == <span class="built_in">len</span>(negatives)</span><br><span class="line">            <span class="variable language_">self</span>.centers = centers</span><br><span class="line">            <span class="variable language_">self</span>.contexts = contexts</span><br><span class="line">            <span class="variable language_">self</span>.negatives = negatives</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.centers[index], <span class="variable language_">self</span>.contexts[index], <span class="variable language_">self</span>.negatives[index]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.centers)</span><br><span class="line"></span><br><span class="line">    dataset = PTBDataset(all_centers, all_contexts, all_negatives)</span><br><span class="line">    data_iter = torch.utils.data.DataLoader(dataset, batch_size,</span><br><span class="line">                                            shuffle=<span class="literal">True</span>, collate_fn=batchify, num_workers=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> data_iter, vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 读取数据集</span></span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;ptb&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;ptb.zip&#x27;</span>, <span class="string">&#x27;319d85e578af0cdc590547f26231e4e31cdf1e42&#x27;</span>)</span><br><span class="line">    sentences = read_ptb()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;length of sentence: <span class="subst">&#123;<span class="built_in">len</span>(sentences)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建词表</span></span><br><span class="line">    vocab = d2l.Vocab(sentences, min_freq=<span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;vocab size: <span class="subst">&#123;<span class="built_in">len</span>(vocab)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 下采样</span></span><br><span class="line">    subsampled, counter = subsample(sentences, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可视化下采样前后每句话词元数量的直方图</span></span><br><span class="line">    d2l.show_list_len_pair_hist([<span class="string">&#x27;origin&#x27;</span>, <span class="string">&#x27;subsampled&#x27;</span>], <span class="string">&#x27;# tokens per sentence&#x27;</span>, <span class="string">&#x27;count&#x27;</span>, sentences, subsampled)</span><br><span class="line">    d2l.plt.show()</span><br><span class="line">    <span class="comment"># 查看一下词元高频词 the 在下采样前后的频次</span></span><br><span class="line">    <span class="built_in">print</span>(compare_counts(<span class="string">&#x27;the&#x27;</span>, sentences, subsampled))  <span class="comment"># 可看出高频词the很多都被去除了</span></span><br><span class="line">    <span class="comment"># 查看一下词元低频词 join 在下采样前后的频次</span></span><br><span class="line">    <span class="built_in">print</span>(compare_counts(<span class="string">&#x27;join&#x27;</span>, subsampled, subsampled))  <span class="comment"># 可看出低频词join被完全保留了</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 下采样之后，将词元映射到它们在语料库中的索引</span></span><br><span class="line">    corpus = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> subsampled]</span><br><span class="line">    <span class="built_in">print</span>(corpus[:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 验证一下提取中心词和上下文词的函数</span></span><br><span class="line">    tiny_set = [<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">7</span>)), <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">7</span>, <span class="number">10</span>))]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;raw data: &quot;</span>, tiny_set)</span><br><span class="line">    <span class="keyword">for</span> center, context <span class="keyword">in</span> <span class="built_in">zip</span>(*get_centers_and_contexts(tiny_set, <span class="number">2</span>)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;center word &#x27;</span>, center, <span class="string">&#x27;, its&#x27;</span>, <span class="string">&#x27;context word:&#x27;</span>, context)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在PTB数据集上进行训练时，将最大上下文窗口大小设为5</span></span><br><span class="line">    all_centers, all_contexts = get_centers_and_contexts(corpus, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用负采样进行近似训练</span></span><br><span class="line">    generator = RandomGenerator([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    <span class="built_in">print</span>([generator.draw() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 负采样</span></span><br><span class="line">    all_negatives = get_negatives(all_contexts, vocab, counter, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 小批量加载训练实例</span></span><br><span class="line">    <span class="comment"># 先测试一下batchify函数</span></span><br><span class="line">    x_1 = (<span class="number">1</span>, [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">    x_2 = (<span class="number">1</span>, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">    batch = batchify((x_1, x_2))</span><br><span class="line">    names = [<span class="string">&#x27;centers&#x27;</span>, <span class="string">&#x27;contexts_negatives&#x27;</span>, <span class="string">&#x27;masks&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> name, data <span class="keyword">in</span> <span class="built_in">zip</span>(names, batch):</span><br><span class="line">        <span class="built_in">print</span>(name, <span class="string">&#x27;=&#x27;</span>, data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检验load_data_ptb返回的数据迭代器</span></span><br><span class="line">    data_iter, vocab = load_data_ptb(<span class="number">512</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">for</span> name, data <span class="keyword">in</span> <span class="built_in">zip</span>(names, batch):</span><br><span class="line">            <span class="built_in">print</span>(name, <span class="string">&#x27;shape:&#x27;</span>, data.shape)</span><br><span class="line">        <span class="keyword">break</span>  <span class="comment"># 看一下第一个就够了</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">length of sentence: 42069</span><br><span class="line">vocab size: 6719</span><br></pre></td></tr></table></figure>

<p><img src="/ymhui.github.io/2024/09/13/compare%E5%87%BD%E6%95%B0/d_freq_before_after.png" alt="image-20241001180806375"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;the&quot; count before subsample: 50770</span><br><span class="line">after subsample: 2043</span><br><span class="line">&quot;join&quot; count before subsample: 45</span><br><span class="line">after subsample: 45</span><br><span class="line">[[], [2115, 406], [22, 5277, 3054, 1580]]</span><br><span class="line">raw data:  [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]</span><br><span class="line">center word  0 , its context word: [1, 2]</span><br><span class="line">center word  1 , its context word: [0, 2]</span><br><span class="line">center word  2 , its context word: [0, 1, 3, 4]</span><br><span class="line">center word  3 , its context word: [2, 4]</span><br><span class="line">center word  4 , its context word: [3, 5]</span><br><span class="line">center word  5 , its context word: [4, 6]</span><br><span class="line">center word  6 , its context word: [4, 5]</span><br><span class="line">center word  7 , its context word: [8, 9]</span><br><span class="line">center word  8 , its context word: [7, 9]</span><br><span class="line">center word  9 , its context word: [7, 8]</span><br><span class="line">[2, 2, 2, 3, 2, 1, 2, 1, 3, 2]</span><br><span class="line">centers = tensor([[1],</span><br><span class="line">        [1]])</span><br><span class="line">contexts_negatives = tensor([[2, 2, 3, 3, 3, 3],</span><br><span class="line">        [2, 2, 2, 3, 3, 0]])</span><br><span class="line">masks = tensor([[1, 1, 1, 1, 1, 1],</span><br><span class="line">        [1, 1, 1, 1, 1, 0]])</span><br><span class="line">labels = tensor([[1, 1, 0, 0, 0, 0],</span><br><span class="line">        [1, 1, 1, 0, 0, 0]])</span><br><span class="line">centers shape: torch.Size([512, 1])</span><br><span class="line">contexts_negatives shape: torch.Size([512, 60])</span><br><span class="line">masks shape: torch.Size([512, 60])</span><br><span class="line">labels shape: torch.Size([512, 60])</span><br></pre></td></tr></table></figure>

<h2 id="预训练word2vec"><a href="#预训练word2vec" class="headerlink" title="预训练word2vec"></a>预训练word2vec</h2><p>继续实现跳元语法模型，然后在PTB数据集上使用负采样预训练word2vec。</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> word2vec_dataset <span class="keyword">import</span> load_data_ptb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">skip_gram</span>(<span class="params">center, contexts_and_negatives, embed_v, embed_u</span>):</span><br><span class="line">    v = embed_v(center)</span><br><span class="line">    u = embed_u(contexts_and_negatives)</span><br><span class="line">    pred = torch.bmm(v, u.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练负采样的跳元模型之前，先定义损失函数</span></span><br><span class="line"><span class="comment"># 二元交叉熵损失</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SigmoidBCELoss</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 带掩码的二元交叉熵损失</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, target, mask=<span class="literal">None</span></span>):</span><br><span class="line">        out = nn.functional.binary_cross_entropy_with_logits(inputs, target, weight=mask, reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> out.mean(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在二元交叉熵损失中使用sigmoid激活函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> -math.log(<span class="number">1</span> / (<span class="number">1</span> + math.exp(-x)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练阶段代码, 注意有填充的存在</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, data_iter, lr, num_epochs, device=d2l.try_gpu(<span class="params"></span>)</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Embedding:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 规范化的损失之和，规范化的损失数</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(data_iter)</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_iter):</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            center, context_negative, mask, label = [data.to(device) <span class="keyword">for</span> data <span class="keyword">in</span> batch]</span><br><span class="line">            pred = skip_gram(center, context_negative, net[<span class="number">0</span>], net[<span class="number">1</span>])</span><br><span class="line">            l = (loss(pred.reshape(label.shape).<span class="built_in">float</span>(), label.<span class="built_in">float</span>(), mask) / mask.<span class="built_in">sum</span>(axis=<span class="number">1</span>) * mask.shape[<span class="number">1</span>])</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>],))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss: <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop():<span class="number">.1</span>f&#125;</span> tokens / sec on <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用词嵌入</span></span><br><span class="line"><span class="comment"># 在训练好word2vec模型之后，我们可以使用训练好的模型中词向量的余弦相似度来从词表中找到与输入单词语义最相似的单词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_similar_tokens</span>(<span class="params">query_token, k, embed</span>):</span><br><span class="line">    W = embed.weight.data</span><br><span class="line">    x = W[vocab[query_token]]</span><br><span class="line">    <span class="comment"># 计算余弦相似性，增加1e-9以获得数值稳定性</span></span><br><span class="line">    cos = torch.mv(W, x) / torch.sqrt(torch.<span class="built_in">sum</span>(W * W, dim=<span class="number">1</span>) * torch.<span class="built_in">sum</span>(x * x + <span class="number">1e-9</span>))</span><br><span class="line">    topk = torch.topk(cos, k=k + <span class="number">1</span>)[<span class="number">1</span>].cpu().numpy().astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> topk[<span class="number">1</span>:]:</span><br><span class="line">        <span class="comment"># 删除输入词</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;cosine sim = <span class="subst">&#123;<span class="built_in">float</span>(cos[i]):<span class="number">.3</span>f&#125;</span>: <span class="subst">&#123;vocab.to_tokens(i)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, max_window_size, num_noise_words = <span class="number">512</span>, <span class="number">5</span>, <span class="number">5</span></span><br><span class="line">    <span class="comment"># 获得数据迭代器和词表</span></span><br><span class="line">    data_iter, vocab = load_data_ptb(batch_size, max_window_size, num_noise_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造跳元模型</span></span><br><span class="line">    <span class="comment"># 嵌入层</span></span><br><span class="line">    embed = nn.Embedding(num_embeddings=<span class="number">20</span>, embedding_dim=<span class="number">4</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Parameter embedding_weight: <span class="subst">&#123;embed.weight.shape&#125;</span>, dtype = <span class="subst">&#123;embed.weight.dtype&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 嵌入层的输入是词的索引，对于任何词元索引i，其向量表示可以从嵌入层中的权重矩阵的第i行获得</span></span><br><span class="line">    <span class="comment"># 由于embed的向量维度设为4，因此当小批量词元索引的形状为（2，3）时，嵌入层返回具有形状（2，3，4）的向量</span></span><br><span class="line">    x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;embed layer:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(embed(x))</span><br><span class="line">    <span class="comment"># 查看skip_gram函数的输出现状</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of skip_gram output:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(skip_gram(torch.ones((<span class="number">2</span>, <span class="number">1</span>), dtype=torch.long), torch.ones((<span class="number">2</span>, <span class="number">4</span>), dtype=torch.long)</span><br><span class="line">                    , embed, embed).shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    loss = SigmoidBCELoss()</span><br><span class="line">    pred = torch.tensor([[<span class="number">1.1</span>, -<span class="number">2.2</span>, <span class="number">3.3</span>, -<span class="number">4.4</span>]] * <span class="number">2</span>)</span><br><span class="line">    label = torch.tensor([[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]])</span><br><span class="line">    mask = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;loss:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(loss(pred, label, mask) * mask.shape[<span class="number">1</span>] / mask.<span class="built_in">sum</span>(axis=<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 初始化模型参数</span></span><br><span class="line">    embed_size, vocab_size = <span class="number">100</span>, <span class="built_in">len</span>(vocab)</span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size),</span><br><span class="line">        nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 使用负采样来训练跳元模型</span></span><br><span class="line">    lr, num_epochs = <span class="number">0.002</span>, <span class="number">5</span></span><br><span class="line">    train(net, data_iter, lr, num_epochs, d2l.try_gpu())</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用词嵌入</span></span><br><span class="line">    get_similar_tokens(<span class="string">&#x27;chip&#x27;</span>, <span class="number">3</span>, net[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Parameter embedding_weight: torch.Size([20, 4]), dtype = torch.float32</span><br><span class="line">embed layer:</span><br><span class="line">tensor([[[ 0.9359,  1.1477, -0.9412,  1.2183],</span><br><span class="line">         [-0.5608,  0.2200, -0.1888,  0.5098],</span><br><span class="line">         [ 0.1096,  0.2246, -1.7004, -0.9497]],</span><br><span class="line"></span><br><span class="line">        [[-0.1224,  0.2635,  0.1246, -0.4876],</span><br><span class="line">         [ 0.6482,  0.5379,  2.3570,  1.1432],</span><br><span class="line">         [ 0.0884, -0.8212, -1.0040, -0.7317]]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br><span class="line">shape of skip_gram output:</span><br><span class="line">torch.Size([2, 1, 4])</span><br><span class="line">loss:</span><br><span class="line">tensor([0.9352, 1.8462])</span><br><span class="line">loss: 0.410, 123025.4 tokens / sec on cpu</span><br></pre></td></tr></table></figure>

<p><img src="/ymhui.github.io/2024/09/13/compare%E5%87%BD%E6%95%B0/d2vec_pretraining.png" alt="image-20241003155927780"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cosine sim = 0.722: microprocessor</span><br><span class="line">cosine sim = 0.703: laptop</span><br><span class="line">cosine sim = 0.691: intel</span><br></pre></td></tr></table></figure>

<h2 id="全局向量的词嵌入GloVe"><a href="#全局向量的词嵌入GloVe" class="headerlink" title="全局向量的词嵌入GloVe"></a>全局向量的词嵌入GloVe</h2><p><strong>上下文窗口内的词共现可以携带丰富的语义信息</strong>。比如“固体”更可能与“冰”一同出现而不是“水”，反观“蒸汽”则更可能和“水”一起出现。此外，可以<strong>预先计算此类共现的全局语料库统计数据：这可以提高训练效率</strong>。为了利用整个语料库中的统计信息进行词嵌入，使用<strong>全局语料库统计</strong>。</p>
<p>具体原理见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_natural-language-processing-pretraining/glove.html#skip-gram-with-global-corpus-statistics">15.5. Word Embedding with Global Vectors (GloVe) — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<p>模型见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_natural-language-processing-pretraining/glove.html#the-glove-model">15.5. GloVe Model— Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h2 id="子词嵌入"><a href="#子词嵌入" class="headerlink" title="子词嵌入"></a>子词嵌入</h2><p>有些单词可以被视作其他单词的变种，比如dog和dogs，help和helps、helping，又比如boy与boyfriend的关系和girl与girlfriend的关系一样。这种多个词之间潜在的联系有时会传达相当有用的信息，并在预测分析时提供关键的上下文；遗憾的是，<strong>word2vec和GloVe都没有对词的内部结构进行讨论</strong>。</p>
<h3 id="fastText模型"><a href="#fastText模型" class="headerlink" title="fastText模型"></a>fastText模型</h3><p>原理见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html#the-fasttext-model">15.6. fastText — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<h4 id="字节对编码"><a href="#字节对编码" class="headerlink" title="字节对编码"></a>字节对编码</h4><p>在fastText中，所有提取的子词都必须是指定的长度，例如3到6，因此词表大小不能预定义。为了在固定大小的词表中允许可变长度的子词，我们可以应用一种称为<strong>字节对编码</strong>（Byte Pair Encoding，BPE）的压缩算法来提取子词。</p>
<p><strong>字节对编码执行训练数据集的统计分析，以发现单词内的公共符号</strong>，诸如任意长度的连续字符。从长度为1的符号开始，字节对编码迭代地<strong>合并最频繁的连续符号对</strong>以<strong>产生新的更长的符号</strong>。</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回词内最频繁的连续符号对</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_max_freq_pair</span>(<span class="params">token_freq</span>):</span><br><span class="line">    pairs = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freq.items():</span><br><span class="line">        symbols = token.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(symbols) - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># &quot;pairs&quot;的键是两个连续符号的元组</span></span><br><span class="line">            pairs[symbols[i], symbols[i + <span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(pairs, key=pairs.get)  <span class="comment"># 具有最大值的&quot;pairs&quot;键</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并最频繁的连续符号对以产生新符号</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">merge_symbols</span>(<span class="params">max_freq_pair, token_freq, symbols</span>):</span><br><span class="line">    <span class="comment"># 将最频繁的符号对合并为一个新的符号，并添加到symbols列表中</span></span><br><span class="line">    symbols.append(<span class="string">&#x27;&#x27;</span>.join(max_freq_pair))</span><br><span class="line">    <span class="comment"># 初始化新的词频字典，用于存储更新后的词频</span></span><br><span class="line">    new_token_freq = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="comment"># 遍历原始的词频字典</span></span><br><span class="line">    <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freq.items():</span><br><span class="line">        <span class="comment"># 将token中的max_freq_pair替换为新的符号，并保持其他部分不变</span></span><br><span class="line">        new_token = token.replace(<span class="string">&#x27; &#x27;</span>.join(max_freq_pair), <span class="string">&#x27;&#x27;</span>.join(max_freq_pair))</span><br><span class="line">        <span class="comment"># 更新新的词频字典，记录合并后的词频</span></span><br><span class="line">        new_token_freq[new_token] = token_freq[token]</span><br><span class="line">    <span class="keyword">return</span> new_token_freq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 尝试将单词从输入参数symbols分成可能最长的子词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">segment_BPE</span>(<span class="params">tokens, symbols</span>):</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">        start, end = <span class="number">0</span>, <span class="built_in">len</span>(token)</span><br><span class="line">        cur_output = []</span><br><span class="line">        <span class="comment"># 具有符号中可能最长子字的词元段</span></span><br><span class="line">        <span class="keyword">while</span> start &lt; <span class="built_in">len</span>(token) <span class="keyword">and</span> start &lt; end:</span><br><span class="line">            <span class="keyword">if</span> token[start:end] <span class="keyword">in</span> symbols:</span><br><span class="line">                cur_output.append(token[start:end])</span><br><span class="line">                start = end</span><br><span class="line">                end = <span class="built_in">len</span>(token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                end -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> start &lt; <span class="built_in">len</span>(token):</span><br><span class="line">            cur_output.append(<span class="string">&#x27;[UNK]&#x27;</span>)</span><br><span class="line">        outputs.append(<span class="string">&#x27; &#x27;</span>.join(cur_output))</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 初始化符号词表，内容为所有英文小写字符、特殊的词尾符号&#x27;_&#x27;, 和 未知符号&#x27;&lt;unk&gt;&#x27;</span></span><br><span class="line">    symbols = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;j&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;n&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;q&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>]</span><br><span class="line">    <span class="comment"># 定义一个原始token频率字典，用于记录不同token出现的频率</span></span><br><span class="line">    raw_token_freq = &#123;<span class="string">&#x27;fast_&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;faster_&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;tall_&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;taller_&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">    <span class="comment"># 初始化一个空字典，用于存储处理后的token频率</span></span><br><span class="line">    token_freq = &#123;&#125;</span><br><span class="line">    <span class="comment"># 遍历原始token频率字典，对每个token进行处理</span></span><br><span class="line">    <span class="keyword">for</span> token, freq <span class="keyword">in</span> raw_token_freq.items():</span><br><span class="line">        <span class="comment"># 将token按字符拆分，并用空格连接，然后将其作为新的键存储在token_freq中</span></span><br><span class="line">        token_freq[<span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(token))] = raw_token_freq[token]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;token_freq:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(token_freq)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对词典token_freq的键迭代地执行字节对编码算法</span></span><br><span class="line">    num_merges = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">        max_freq_pair = get_max_freq_pair(token_freq)</span><br><span class="line">        token_freq = merge_symbols(max_freq_pair, token_freq, symbols)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;merge #<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>:&#x27;</span>, max_freq_pair)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;symbols:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(symbols)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用列表symbols中的子词（从前面提到的数据集学习）来表示另一个数据集的tokens</span></span><br><span class="line">    tokens = [<span class="string">&#x27;tallest_&#x27;</span>, <span class="string">&#x27;fatter_&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;use token in symbols to represent another dataset tokens:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(segment_BPE(tokens, symbols))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">token_freq:</span><br><span class="line">&#123;&#x27;f a s t _&#x27;: 4, &#x27;f a s t e r _&#x27;: 3, &#x27;t a l l _&#x27;: 5, &#x27;t a l l e r _&#x27;: 4&#125;</span><br><span class="line">merge #1: (&#x27;t&#x27;, &#x27;a&#x27;)</span><br><span class="line">merge #2: (&#x27;ta&#x27;, &#x27;l&#x27;)</span><br><span class="line">merge #3: (&#x27;tal&#x27;, &#x27;l&#x27;)</span><br><span class="line">merge #4: (&#x27;f&#x27;, &#x27;a&#x27;)</span><br><span class="line">merge #5: (&#x27;fa&#x27;, &#x27;s&#x27;)</span><br><span class="line">merge #6: (&#x27;fas&#x27;, &#x27;t&#x27;)</span><br><span class="line">merge #7: (&#x27;e&#x27;, &#x27;r&#x27;)</span><br><span class="line">merge #8: (&#x27;er&#x27;, &#x27;_&#x27;)</span><br><span class="line">merge #9: (&#x27;tall&#x27;, &#x27;_&#x27;)</span><br><span class="line">merge #10: (&#x27;fast&#x27;, &#x27;_&#x27;)</span><br><span class="line">symbols:</span><br><span class="line">[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;j&#x27;, &#x27;k&#x27;, &#x27;l&#x27;, &#x27;m&#x27;, &#x27;n&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;q&#x27;, &#x27;r&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;u&#x27;, &#x27;v&#x27;, &#x27;w&#x27;, &#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;_&#x27;, &#x27;[UNK]&#x27;, &#x27;ta&#x27;, &#x27;tal&#x27;, &#x27;tall&#x27;, &#x27;fa&#x27;, &#x27;fas&#x27;, &#x27;fast&#x27;, &#x27;er&#x27;, &#x27;er_&#x27;, &#x27;tall_&#x27;, &#x27;fast_&#x27;]</span><br><span class="line">use token in symbols to represent another dataset tokens:</span><br><span class="line">[&#x27;tall e s t _&#x27;, &#x27;fa t t er_&#x27;]</span><br></pre></td></tr></table></figure>

<h2 id="词的相似性和类比任务"><a href="#词的相似性和类比任务" class="headerlink" title="词的相似性和类比任务"></a>词的相似性和类比任务</h2><p>在<strong>大型语料库上预先训练的词向量可以应用于下游的自然语言处理</strong>任务。下面将展示大型语料库中预训练词向量的语义，即<strong>将预训练词向量应用到词的相似性和类比任务</strong>中。</p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了加载预训练的GloVe和fastText嵌入</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TokenEmbedding</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Glove嵌入&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_name</span>):</span><br><span class="line">        <span class="variable language_">self</span>.idx_to_token, <span class="variable language_">self</span>.idx_to_vec = <span class="variable language_">self</span>._load_embedding(embedding_name)</span><br><span class="line">        <span class="variable language_">self</span>.unknown_idx = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.token_to_idx = &#123;</span><br><span class="line">            token: idx <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.idx_to_token)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_load_embedding</span>(<span class="params">self, embedding_name</span>):</span><br><span class="line">        idx_to_token, idx_to_vec = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>], []</span><br><span class="line">        data_dir = d2l.download_extract(embedding_name)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;vec.txt&#x27;</span>), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                elems = line.rstrip().split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                token, elems = elems[<span class="number">0</span>], [<span class="built_in">float</span>(elem) <span class="keyword">for</span> elem <span class="keyword">in</span> elems[<span class="number">1</span>:]]</span><br><span class="line">                <span class="comment"># 跳过标题信息，比如fasttext首行</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(elems) &gt; <span class="number">1</span>:</span><br><span class="line">                    idx_to_token.append(token)</span><br><span class="line">                    idx_to_vec.append(elems)</span><br><span class="line">        idx_to_vec = [[<span class="number">0</span>] * <span class="built_in">len</span>(idx_to_vec[<span class="number">0</span>])] + idx_to_vec</span><br><span class="line">        <span class="keyword">return</span> idx_to_token, torch.tensor(idx_to_vec)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        indices = [<span class="variable language_">self</span>.token_to_idx.get(token, <span class="variable language_">self</span>.unknown_idx) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">        vecs = <span class="variable language_">self</span>.idx_to_vec[torch.tensor(indices)]</span><br><span class="line">        <span class="keyword">return</span> vecs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.idx_to_token)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用knn为词分类，以便根据词向量之间的余弦相似度为输入词查找语义相似的词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">knn</span>(<span class="params">W, x, k</span>):</span><br><span class="line">    <span class="comment"># 增加1e-9以获得数值稳定性</span></span><br><span class="line">    cos = torch.mv(W, x.reshape(-<span class="number">1</span>, )) / (torch.sqrt(torch.<span class="built_in">sum</span>(W * W, axis=<span class="number">1</span>) + <span class="number">1e-9</span>) * torch.sqrt((x * x).<span class="built_in">sum</span>()))</span><br><span class="line">    _, topk = torch.topk(cos, k=k)</span><br><span class="line">    <span class="keyword">return</span> topk, [cos[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> topk]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用TokenEmbedding的实例embed中预训练好的词向量来搜索相近的词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_similar_tokens</span>(<span class="params">query_token, k, embed</span>):</span><br><span class="line">    topks, cos = knn(embed.idx_to_vec, embed[[query_token]], k + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">zip</span>(topks[<span class="number">1</span>:], cos[<span class="number">1</span>:]):</span><br><span class="line">        <span class="comment"># 排除输入词</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;embed.idx_to_token[<span class="built_in">int</span>(i)]&#125;</span>: cosine similarity = <span class="subst">&#123;<span class="built_in">float</span>(c):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在词类比中，找到一个词，其向量值与vec(c) + vec(b) - vec(a)最接近</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_analogy</span>(<span class="params">token_a, token_b, token_c, embed</span>):</span><br><span class="line">    vecs = embed[[token_a, token_b, token_c]]</span><br><span class="line">    x = vecs[<span class="number">1</span>] - vecs[<span class="number">0</span>] + vecs[<span class="number">2</span>]  <span class="comment"># b - a + c</span></span><br><span class="line">    topks, cos = knn(embed.idx_to_vec, x, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> embed.idx_to_token[<span class="built_in">int</span>(topks[<span class="number">0</span>])]  <span class="comment"># 删除未知词</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 加载预训练向量</span></span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;glove.6b.50d&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;glove.6B.50d.zip&#x27;</span>,</span><br><span class="line">                                    <span class="string">&#x27;0b8703943ccdb6eb788e6f091b8946e82231bc4d&#x27;</span>)</span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;glove.6b.100d&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;glove.6B.100d.zip&#x27;</span>,</span><br><span class="line">                                     <span class="string">&#x27;cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a&#x27;</span>)</span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;glove.42b.300d&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;glove.42B.300d.zip&#x27;</span>,</span><br><span class="line">                                      <span class="string">&#x27;b5116e234e9eb9076672cfeabf5469f3eec904fa&#x27;</span>)</span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;wiki.en&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;wiki.en.zip&#x27;</span>,</span><br><span class="line">                               <span class="string">&#x27;c1816da3821ae9f43899be655002f6c723e91b88&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载50维GloVe嵌入，以创建TokenEmbedding实例</span></span><br><span class="line">    glove_6b50d = TokenEmbedding(<span class="string">&#x27;glove.6b.50d&#x27;</span>)</span><br><span class="line">    <span class="comment"># 看一下词表大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;size of vocab:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(glove_6b50d))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用预训练词向量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词相似度</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;token similarity:&quot;</span>)</span><br><span class="line">    <span class="comment"># glove_6b50d中预训练词向量的词表包含400000个词和一个特殊的未知词元。</span></span><br><span class="line">    <span class="comment"># 排除输入词和未知词元后，在词表中找到与“chip”一词语义最相似的三个词</span></span><br><span class="line">    <span class="built_in">print</span>(get_similar_tokens(<span class="string">&#x27;chip&#x27;</span>, <span class="number">3</span>, glove_6b50d))</span><br><span class="line">    <span class="comment"># 再看一下girl, beautiful</span></span><br><span class="line">    <span class="built_in">print</span>(get_similar_tokens(<span class="string">&#x27;girl&#x27;</span>, <span class="number">3</span>, glove_6b50d))</span><br><span class="line">    <span class="built_in">print</span>(get_similar_tokens(<span class="string">&#x27;beautiful&#x27;</span>, <span class="number">3</span>, glove_6b50d))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词类比</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;token compare:&quot;</span>)</span><br><span class="line">    <span class="comment"># 除了找到相似的词，还可以将词向量应用到词类比任务中，比如“man”:“woman”::“son”:“daughter”就是一个词的类比</span></span><br><span class="line">    <span class="comment"># “man”是对“woman”的类比，“son”是对“daughter”的类比</span></span><br><span class="line">    <span class="comment"># 对于单词类比a : b :: c : d，给出前三个词a、b和c，找到d。</span></span><br><span class="line">    <span class="comment"># 用vec(w)表示词w的向量，为了完成这个类比，将要找到一个词w，其向量与vec(c) + vec(b) − vec(a)的结果最相似、</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;man:woman::son:?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(get_analogy(<span class="string">&#x27;man&#x27;</span>, <span class="string">&#x27;woman&#x27;</span>, <span class="string">&#x27;son&#x27;</span>, glove_6b50d))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;beijing:china::tokyo:?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(get_analogy(<span class="string">&#x27;beijing&#x27;</span>, <span class="string">&#x27;china&#x27;</span>, <span class="string">&#x27;tokyo&#x27;</span>, glove_6b50d))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;bad:worst::big:?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(get_analogy(<span class="string">&#x27;bad&#x27;</span>, <span class="string">&#x27;worst&#x27;</span>, <span class="string">&#x27;big&#x27;</span>, glove_6b50d))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">size of vocab:</span><br><span class="line">400001</span><br><span class="line">token similarity:</span><br><span class="line">chips: cosine similarity = 0.856</span><br><span class="line">intel: cosine similarity = 0.749</span><br><span class="line">electronics: cosine similarity = 0.749</span><br><span class="line">None</span><br><span class="line">boy: cosine similarity = 0.933</span><br><span class="line">woman: cosine similarity = 0.907</span><br><span class="line">mother: cosine similarity = 0.835</span><br><span class="line">None</span><br><span class="line">lovely: cosine similarity = 0.921</span><br><span class="line">gorgeous: cosine similarity = 0.893</span><br><span class="line">wonderful: cosine similarity = 0.830</span><br><span class="line">None</span><br><span class="line">token compare:</span><br><span class="line">man:woman::son:?</span><br><span class="line">daughter</span><br><span class="line">beijing:china::tokyo:?</span><br><span class="line">japan</span><br><span class="line">bad:worst::big:?</span><br><span class="line">biggest</span><br></pre></td></tr></table></figure>

<h2 id="Transformers的双向编码器表示-BERT"><a href="#Transformers的双向编码器表示-BERT" class="headerlink" title="Transformers的双向编码器表示(BERT)"></a>Transformers的双向编码器表示(BERT)</h2><p>介绍到现在，上文提到的所有词嵌入模型都是<strong>上下文无关</strong>的，而现在，我们引入上下文敏感模型。</p>
<h3 id="上下文无关-敏感模型"><a href="#上下文无关-敏感模型" class="headerlink" title="上下文无关&#x2F;敏感模型"></a>上下文无关&#x2F;敏感模型</h3><p>考虑之前使用的那些词嵌入模型word2vec和GloVe，它们都<strong>将相同的预训练向量分配给同一个词，而不考虑词的上下文</strong>（如果有的话）。形式上，任何词元x的上下文无关表示是函数f(x)，其仅将x作为其输入。考虑到自然语言中丰富的多义现象和复杂的语义，上下文无关表示具有明显的局限性。因为同一个词在不同的上下文中可能表达截然不同的意思。</p>
<p>这推动了上下文敏感模型的出现，其中词的表征取决于上下文，即词元<em>x</em>的上下文敏感表示函数<em>f(x, c(x))<em>，取决于</em>x</em>及其上下文*c(x)*。</p>
<h3 id="特定任务-不可知任务"><a href="#特定任务-不可知任务" class="headerlink" title="特定任务&#x2F;不可知任务"></a>特定任务&#x2F;不可知任务</h3><p>现有的各种自然语言处理的解决方案都<strong>依赖于一个特定于任务的架构</strong>，然而为每一个任务设计一个特定的架构是一件很困难的事情。<strong>GPT模型为上下文的敏感表示设计了通用的任务无关模型</strong>；<strong>GPT建立在Transformer解码器的基础上</strong>，预训练了一个用于表示文本序列的语言模型。当将GPT应用于下游任务时，语言模型的输出将被送到一个附加的线性输出层，以预测任务的标签。</p>
<p>然而，由于语言模型的<strong>自回归特性</strong>，<strong>GPT只能向前看</strong>（从左到右）。在“i went to the bank to deposit cash”（我去银行存现金）和“i went to the bank to sit down”（我去河岸边坐下）的上下文中，由于“bank”对其左边的上下文敏感，GPT将返回“bank”的相同表示，尽管它有不同的含义。</p>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>将两个最好的（上下文敏感模型和不可知任务）结合起来。</p>
<p>原理见<a target="_blank" rel="noopener" href="https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html#bert-combining-the-best-of-both-worlds">15.8. Bidirectional Encoder Representations from Transformers (BERT) — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get_tokens_and_segments将一个句子或两个句子作为输入，然后返回BERT输入序列的标记及其相应的片段索引。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_tokens_and_segments</span>(<span class="params">tokens_a, tokens_b=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取输入序列的词元及其片段索引&quot;&quot;&quot;</span></span><br><span class="line">    tokens = [<span class="string">&#x27;&lt;cls&gt;&#x27;</span>] + tokens_a + [<span class="string">&#x27;&lt;sep&gt;&#x27;</span>]</span><br><span class="line">    <span class="comment"># 0和1分别标记A和B</span></span><br><span class="line">    segments = [<span class="number">0</span>] * (<span class="built_in">len</span>(tokens_a) + <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> tokens_b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        tokens += tokens_b + [<span class="string">&#x27;&lt;sep&gt;&#x27;</span>]</span><br><span class="line">        segments += [<span class="number">1</span>] * (<span class="built_in">len</span>(tokens_b) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> tokens, segments</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># BERTEncoder使用片段嵌入和可学习的位置嵌入</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTEncoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hidden, norm_shape, ffn_num_input, ffn_num_hidden, num_heads,</span></span><br><span class="line"><span class="params">                 num_layers, dropout, max_len=<span class="number">1000</span>, key_size=<span class="number">768</span>, query_size=<span class="number">768</span>, value_size=<span class="number">768</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTEncoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.token_embedding = nn.Embedding(vocab_size, num_hidden)</span><br><span class="line">        <span class="variable language_">self</span>.segment_embedding = nn.Embedding(<span class="number">2</span>, num_hidden)</span><br><span class="line">        <span class="variable language_">self</span>.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            <span class="variable language_">self</span>.blks.add_module(<span class="string">f&quot;<span class="subst">&#123;i&#125;</span>&quot;</span>, d2l.EncoderBlock(</span><br><span class="line">                key_size, query_size, value_size, num_hidden, norm_shape, ffn_num_input, ffn_num_hidden, num_heads, dropout, <span class="literal">True</span></span><br><span class="line">            ))</span><br><span class="line">        <span class="comment"># 在BERT中，位置嵌入是可学习的，因此创建一个足够长的位置嵌入参数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, max_len, num_hidden))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens, segments, valid_lens</span>):</span><br><span class="line">        <span class="comment"># X的形状保持不变: (批量大小，最大序列长度，num_hidden)</span></span><br><span class="line">        X = <span class="variable language_">self</span>.token_embedding(tokens) + <span class="variable language_">self</span>.segment_embedding(segments)</span><br><span class="line">        X = X + <span class="variable language_">self</span>.pos_embedding.data[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> <span class="variable language_">self</span>.blks:</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现下面的MaskLM类来预测BERT预训练的掩蔽语言模型任务中的掩蔽标记</span></span><br><span class="line"><span class="comment"># 预测使用单隐藏层的多层感知机（self.mlp）。在前向推断中，它需要两个输入：BERTEncoder的编码结果和用于预测的词元位置</span></span><br><span class="line"><span class="comment"># 输出这些位置的预测结果</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaskLM</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT的掩蔽语言模型任务&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hidden, num_inputs=<span class="number">768</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MaskLM, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(num_inputs, num_hidden),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.LayerNorm(num_hidden),</span><br><span class="line">            nn.Linear(num_hidden, vocab_size)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, pred_positions</span>):</span><br><span class="line">        num_pred_positions = pred_positions.shape[<span class="number">1</span>]</span><br><span class="line">        pred_positions = pred_positions.reshape(-<span class="number">1</span>)</span><br><span class="line">        batch_size = X.shape[<span class="number">0</span>]</span><br><span class="line">        batch_idx = torch.arange(<span class="number">0</span>, batch_size)</span><br><span class="line">        <span class="comment"># 假设batch_size=2，num_pred_positions=3</span></span><br><span class="line">        <span class="comment"># 那么batch_idx是[0, 0, 0, 1, 1, 1]</span></span><br><span class="line">        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)</span><br><span class="line">        masked_X = X[batch_idx, num_pred_positions]</span><br><span class="line">        masked_X = masked_X.reshape((batch_size, num_pred_positions, -<span class="number">1</span>))</span><br><span class="line">        mlm_Y_hat = <span class="variable language_">self</span>.mlp(masked_X)</span><br><span class="line">        <span class="keyword">return</span> mlm_Y_hat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下一句预测模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NextSentencePred</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT的下一句预测任务&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(NextSentencePred, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Linear(num_inputs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># X的形状(batch_size, num_hidden)</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.output(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 整合代码实现完整的BERT</span></span><br><span class="line"><span class="comment"># 在预训练BERT时，最终的损失函数时掩蔽语言模型损失函数和下一句预测损失函数的线性组合</span></span><br><span class="line"><span class="comment"># 现在通过实例化三个类：BERTEncoder、MaskLM和NextSentencePred来定义BERT模型</span></span><br><span class="line"><span class="comment"># 前向推断返回编码后的BERT表示encoded_X、掩蔽语言模型预测mlm_Y_hat和下一句预测nsp_Y_hat</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers,</span></span><br><span class="line"><span class="params">                 dropout, max_len=<span class="number">1000</span>, key_size=<span class="number">768</span>, query_size=<span class="number">768</span>, value_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 hid_in_features=<span class="number">768</span>, mlm_in_features=<span class="number">768</span>, nsp_in_features=<span class="number">768</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = BERTEncoder(</span><br><span class="line">            vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers,</span><br><span class="line">            dropout, max_len=max_len, key_size=key_size, query_size=query_size, value_size=value_size</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.hidden = nn.Sequential(</span><br><span class="line">            nn.Linear(hid_in_features, num_hiddens),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)</span><br><span class="line">        <span class="variable language_">self</span>.nsp = NextSentencePred(nsp_in_features)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens, segments, valid_lens=<span class="literal">None</span>, pred_positions=<span class="literal">None</span></span>):</span><br><span class="line">        encoded_X = <span class="variable language_">self</span>.encoder(tokens, segments, valid_lens)</span><br><span class="line">        <span class="keyword">if</span> pred_positions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mlm_Y_hat = <span class="variable language_">self</span>.mlm(encoded_X, pred_positions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mlm_Y_hat = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 用于下一句预测的多层感知机分类器的隐藏层，0是&quot;&lt;cls&gt;&quot;标记的索引</span></span><br><span class="line">        nsp_Y_hat = <span class="variable language_">self</span>.nsp(<span class="variable language_">self</span>.hidden(encoded_X[:, <span class="number">0</span>, :]))</span><br><span class="line">        <span class="keyword">return</span> encoded_X, mlm_Y_hat, nsp_Y_hat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 假设词表大小为10000，为了演示BERTEncoder的前向推断，让我们创建一个实例并初始化它的参数</span></span><br><span class="line">    vocab_size, num_hidden, ffn_num_hidden, num_heads = <span class="number">10000</span>, <span class="number">768</span>, <span class="number">1024</span>, <span class="number">4</span></span><br><span class="line">    norm_shape, ffn_num_input, num_layers, dropout = [<span class="number">768</span>], <span class="number">768</span>, <span class="number">2</span>, <span class="number">0.2</span></span><br><span class="line">    encoder = BERTEncoder(vocab_size, num_hidden, norm_shape, ffn_num_input, ffn_num_hidden, num_heads, num_layers, dropout)</span><br><span class="line">    tokens = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">2</span>, <span class="number">8</span>))</span><br><span class="line">    segments = torch.tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">    encoder_X = encoder(tokens, segments, <span class="literal">None</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of encoded_X:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(encoder_X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预训练任务</span></span><br><span class="line">    <span class="comment"># 预训练任务的原理见 https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html#pretraining-tasks</span></span><br><span class="line">    <span class="comment"># 演示MaskLM的前向推断</span></span><br><span class="line">    mlm = MaskLM(vocab_size, num_hidden)</span><br><span class="line">    mlm_positions = torch.tensor([[<span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>], [<span class="number">6</span>, <span class="number">1</span>, <span class="number">5</span>]])</span><br><span class="line">    mlm_Y_hat = mlm(encoder_X, mlm_positions)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of mlm_Y_hat:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mlm_Y_hat.shape)</span><br><span class="line">    <span class="comment"># 通过掩码下的预测词元mlm_Y的真实标签mlm_Y_hat，我们可以计算在BERT预训练中的遮蔽语言模型任务的交叉熵损失</span></span><br><span class="line">    mlm_Y = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]])</span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    mlm_l = loss(mlm_Y_hat.reshape((-<span class="number">1</span>, vocab_size)), mlm_Y.reshape(-<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of mlm_l:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mlm_l.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 下一句预测</span></span><br><span class="line">    encoder_X = torch.flatten(encoder_X, start_dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># NSP的输入形状为(batch_size, num_hidden)</span></span><br><span class="line">    nsp = NextSentencePred(encoder_X.shape[-<span class="number">1</span>])</span><br><span class="line">    nsp_Y_hat = nsp(encoder_X)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of nsp_Y_hat:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(nsp_Y_hat.shape)</span><br><span class="line">    <span class="comment"># 还可以计算两个二元分类的交叉熵损失</span></span><br><span class="line">    nsp_y = torch.tensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    nsp_l = loss(nsp_Y_hat, nsp_y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of nsp_l:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(nsp_l.shape)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">shape of encoded_X:</span><br><span class="line">torch.Size([2, 8, 768])</span><br><span class="line">shape of mlm_Y_hat:</span><br><span class="line">torch.Size([2, 3, 10000])</span><br><span class="line">shape of mlm_l:</span><br><span class="line">torch.Size([6])</span><br><span class="line">shape of nsp_Y_hat:</span><br><span class="line">torch.Size([2, 2])</span><br><span class="line">shape of nsp_l:</span><br><span class="line">torch.Size([2])</span><br></pre></td></tr></table></figure>

<h2 id="用于预训练BERT的数据集"><a href="#用于预训练BERT的数据集" class="headerlink" title="用于预训练BERT的数据集"></a>用于预训练BERT的数据集</h2><p>为了训练BERT模型，我们需要以理想的格式生成数据集，以便<strong>进行两个预训练任务：遮蔽语言模型和下一句预测</strong>。根据经验，在定制的数据集上对BERT进行预训练更有效，为了方便演示，使用较小的语料库<strong>WikiText-2</strong></p>
<p>与PTB数据集相比，WikiText-2</p>
<ul>
<li>保留了原来的标点符号，适合于下一句预测；</li>
<li>保留了原来的大小写和数字；</li>
<li>大了一倍以上。</li>
</ul>
<blockquote>
<p>code</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_read_wiki</span>(<span class="params">data_dir</span>):</span><br><span class="line">    file_name = os.path.join(data_dir, <span class="string">&#x27;wiki.train.tokens&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_name, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="comment"># 大写转小写</span></span><br><span class="line">    paragraphs = [line.strip().lower().split(<span class="string">&#x27; . &#x27;</span>) <span class="keyword">for</span> line <span class="keyword">in</span> lines <span class="keyword">if</span> <span class="built_in">len</span>(line.split(<span class="string">&#x27; . &#x27;</span>)) &gt;= <span class="number">2</span>]</span><br><span class="line">    random.shuffle(paragraphs)</span><br><span class="line">    <span class="keyword">return</span> paragraphs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为预训练定义辅助函数</span></span><br><span class="line"><span class="comment"># 生成下一句预测任务的数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_next_sentence</span>(<span class="params">sentence, next_sentence, paragraphs</span>):</span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">        is_next = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># paragraphs 是三重列表的嵌套</span></span><br><span class="line">        next_sentence = random.choice(random.choice(paragraphs))</span><br><span class="line">        is_next = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> sentence, next_sentence, is_next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从输入paragraph生成用于下一句预测的训练样本，paragraph是句子列表，每个句子是词元列表</span></span><br><span class="line"><span class="comment"># 自变量max_len指定预训练期间的BERT输入序列的最大长度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_nsp_data_from_paragraph</span>(<span class="params">paragraph, paragraphs, vocab, max_len</span>):</span><br><span class="line">    nsp_data_from_paragraph = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(paragraph) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 从相邻的两个句子中获取NSP数据的样本</span></span><br><span class="line">        tokens_a, tokens_b, is_next = _get_next_sentence(paragraph[i], paragraph[i + <span class="number">1</span>], paragraphs)</span><br><span class="line">        <span class="comment"># 考虑1个&#x27;&lt;cls&gt;&#x27;词元和2个&#x27;&lt;sep&gt;&#x27;词元</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(tokens_a) + <span class="built_in">len</span>(tokens_b) + <span class="number">3</span> &gt; max_len:</span><br><span class="line">            <span class="comment"># +3表示&lt;cls&gt;和两个&lt;sep&gt;的长度</span></span><br><span class="line">            <span class="comment"># 如果加上特殊词元后的序列长度超过最大长度，则跳过该样本</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 获取BERT输入所需的词元序列和段落标记序列</span></span><br><span class="line">        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)</span><br><span class="line">        <span class="comment"># 将准备好的样本添加到NSP数据列表中</span></span><br><span class="line">        nsp_data_from_paragraph.append((tokens, segments, is_next))</span><br><span class="line">    <span class="keyword">return</span> nsp_data_from_paragraph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成遮蔽语言模型任务的数据</span></span><br><span class="line"><span class="comment"># 下面实现的函数返回可能替换后的输入词元、发生预测的词元索引和这些预测的标签</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_replace_mlm_tokens</span>(<span class="params">tokens, candidate_pred_positions, num_mlm_pred, vocab</span>):</span><br><span class="line">    <span class="comment"># 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的&quot;&lt;mask&gt;&quot;或随机词元</span></span><br><span class="line">    mlm_input_tokens = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    pred_positions_and_labels = []</span><br><span class="line">    <span class="comment"># 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测</span></span><br><span class="line">    random.shuffle(candidate_pred_positions)</span><br><span class="line">    <span class="keyword">for</span> mlm_pred_position <span class="keyword">in</span> candidate_pred_positions:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(pred_positions_and_labels) &gt;= num_mlm_pred:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        masked_token = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 80%的概率将词替换为“&lt;mask&gt;”词元</span></span><br><span class="line">        <span class="keyword">if</span> random.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">            masked_token = <span class="string">&#x27;&lt;mask&gt;&#x27;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 10%保持不变</span></span><br><span class="line">            <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">                masked_token = tokens[mlm_pred_position]</span><br><span class="line">            <span class="comment"># 10%随即替换</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                masked_token = random.choice(vocab.idx_to_token)</span><br><span class="line">        mlm_input_tokens[mlm_pred_position] = masked_token</span><br><span class="line">        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))</span><br><span class="line">    <span class="keyword">return</span> mlm_input_tokens, pred_positions_and_labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下函数将BERT输入序列(tokens)作为输入并返回输出词元的索引、发生预测的词元索引以及这些预测的标签索引</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_mlm_data_from_tokens</span>(<span class="params">tokens, vocab</span>):</span><br><span class="line">    candidate_pred_positions = []</span><br><span class="line">    <span class="comment"># tokens是一个字符串列表</span></span><br><span class="line">    <span class="keyword">for</span> i, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">        <span class="comment"># 在遮蔽语言模型任务中不会预测特殊词元</span></span><br><span class="line">        <span class="keyword">if</span> token <span class="keyword">in</span> [<span class="string">&#x27;&lt;cls&gt;&#x27;</span>, <span class="string">&#x27;&lt;sep&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        candidate_pred_positions.append(i)</span><br><span class="line">    <span class="comment"># 遮蔽语言模型任务中只预测15%的词元</span></span><br><span class="line">    num_mlm_pred = <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">round</span>(<span class="built_in">len</span>(tokens) * <span class="number">0.15</span>))</span><br><span class="line">    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_pred, vocab)</span><br><span class="line">    pred_positions_and_labels = <span class="built_in">sorted</span>(pred_positions_and_labels, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    pred_positions = [v[<span class="number">0</span>] <span class="keyword">for</span> v <span class="keyword">in</span> pred_positions_and_labels]</span><br><span class="line">    mlm_pred_labels = [v[<span class="number">1</span>] <span class="keyword">for</span> v <span class="keyword">in</span> pred_positions_and_labels]</span><br><span class="line">    <span class="keyword">return</span> vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将文本转换为预训练数据集, 将特殊的&lt;mask&gt;词元附加到输入</span></span><br><span class="line"><span class="comment"># 参数examples来自两个预训练任务辅助函数_get_nsp_data_from_paragraph和_get_mlm_data_from_tokens的输出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_pad_bert_inputs</span>(<span class="params">examples, max_len, vocab</span>):</span><br><span class="line">    max_num_mlm_pred = <span class="built_in">round</span>(max_len * <span class="number">0.15</span>)</span><br><span class="line">    all_token_ids, all_segments, valid_lens = [], [], []</span><br><span class="line">    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []</span><br><span class="line">    nsp_labels = []</span><br><span class="line">    <span class="keyword">for</span> (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) <span class="keyword">in</span> examples:</span><br><span class="line">        all_token_ids.append(torch.tensor(token_ids + [vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (max_len - <span class="built_in">len</span>(token_ids)), dtype=torch.long))</span><br><span class="line">        all_segments.append(torch.tensor(segments + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(segments)), dtype=torch.long))</span><br><span class="line">        <span class="comment"># valid_lens不包括&#x27;&lt;pad&gt;&#x27;的计数</span></span><br><span class="line">        valid_lens.append(torch.tensor(<span class="built_in">len</span>(token_ids), dtype=torch.float32))</span><br><span class="line">        all_pred_positions.append(torch.tensor(pred_positions + [<span class="number">0</span>] * (max_num_mlm_pred - <span class="built_in">len</span>(pred_positions)),</span><br><span class="line">                                           dtype=torch.long))</span><br><span class="line">        <span class="comment"># 填充词元的预测将通过乘以 0权重 在损失中过滤掉</span></span><br><span class="line">        all_mlm_weights.append(</span><br><span class="line">            torch.tensor([<span class="number">1.0</span>] * <span class="built_in">len</span>(mlm_pred_label_ids) + [<span class="number">0.0</span>] * (max_num_mlm_pred - <span class="built_in">len</span>(pred_positions)),</span><br><span class="line">                         dtype=torch.float32)</span><br><span class="line">        )</span><br><span class="line">        all_mlm_labels.append(</span><br><span class="line">            torch.tensor(mlm_pred_label_ids + [<span class="number">0</span>] * (max_num_mlm_pred - <span class="built_in">len</span>(mlm_pred_label_ids)),</span><br><span class="line">                         dtype=torch.long)</span><br><span class="line">        )</span><br><span class="line">        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))</span><br><span class="line">    <span class="keyword">return</span> (all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels,</span><br><span class="line">            nsp_labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># WikiText数据集，词元化时出现次数少于5次的不频繁词元将被过滤，使用d2l.tokenize进行词元化</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_WikiTextDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, paragraphs, max_len</span>):</span><br><span class="line">        <span class="comment"># 输入paragraphs[i]是代表段落的句子字符串列表</span></span><br><span class="line">        <span class="comment"># 而输出paragraphs[i]是代表段落的句子列表，其中每句都是词元列表</span></span><br><span class="line">        paragraphs = [d2l.tokenize(paragraph, token=<span class="string">&#x27;word&#x27;</span>) <span class="keyword">for</span> paragraph <span class="keyword">in</span> paragraphs]</span><br><span class="line">        sentences = [sentence <span class="keyword">for</span> paragraph <span class="keyword">in</span> paragraphs <span class="keyword">for</span> sentence <span class="keyword">in</span> paragraph]</span><br><span class="line">        <span class="variable language_">self</span>.vocab = d2l.Vocab(sentences, min_freq=<span class="number">5</span>, reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="string">&#x27;&lt;cls&gt;&#x27;</span>, <span class="string">&#x27;&lt;sep&gt;&#x27;</span>])</span><br><span class="line">        <span class="comment"># 获取下一个句子预测任务的数据</span></span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">for</span> paragraph <span class="keyword">in</span> paragraphs:</span><br><span class="line">            examples.extend(_get_nsp_data_from_paragraph(paragraph, paragraphs, <span class="variable language_">self</span>.vocab, max_len))</span><br><span class="line">        <span class="comment"># 获取遮蔽语言模型任务的数据</span></span><br><span class="line">        examples = [(_get_mlm_data_from_tokens(tokens, <span class="variable language_">self</span>.vocab) + (segments, is_next))</span><br><span class="line">                    <span class="keyword">for</span> tokens, segments, is_next <span class="keyword">in</span> examples]</span><br><span class="line">        <span class="comment"># 填充输入</span></span><br><span class="line">        (<span class="variable language_">self</span>.all_token_ids, <span class="variable language_">self</span>.all_segments, <span class="variable language_">self</span>.valid_lens, <span class="variable language_">self</span>.all_pred_positions, <span class="variable language_">self</span>.all_mlm_weights,</span><br><span class="line">         <span class="variable language_">self</span>.all_mlm_labels, <span class="variable language_">self</span>.nsp_labels) = _pad_bert_inputs(examples, max_len, <span class="variable language_">self</span>.vocab)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="variable language_">self</span>.all_token_ids[idx], <span class="variable language_">self</span>.all_segments[idx], <span class="variable language_">self</span>.valid_lens[idx], <span class="variable language_">self</span>.all_pred_positions[idx],</span><br><span class="line">                <span class="variable language_">self</span>.all_mlm_weights[idx], <span class="variable language_">self</span>.all_mlm_labels[idx], <span class="variable language_">self</span>.nsp_labels[idx])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.all_token_ids)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集并生成预训练样本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_wiki</span>(<span class="params">batch_size, max_len</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载WikiText-2数据集&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;wikitext-2&#x27;</span>, <span class="string">&#x27;wikitext-2&#x27;</span>)</span><br><span class="line">    paragraphs = _read_wiki(data_dir)</span><br><span class="line">    train_set = _WikiTextDataset(paragraphs, max_len)</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, train_set.vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 在WikiText-2 数据集中，每行代表一个段落，其中在任意标点符号及其前面的词元之间插入空格</span></span><br><span class="line">    <span class="comment"># 保留至少有两句话的段落</span></span><br><span class="line">    <span class="comment"># 使用分号作为分隔符来拆分句子</span></span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;wikitext-2&#x27;</span>] = (</span><br><span class="line">        <span class="string">&#x27;https://s3.amazonaws.com/research.metamind.io/wikitext/&#x27;</span></span><br><span class="line">        <span class="string">&#x27;wikitext-2-v1.zip&#x27;</span>, <span class="string">&#x27;3c914d17d80b1459be871a5039ac23e752a53cbe&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看一下小批量BERT预训练样本的形状</span></span><br><span class="line">    <span class="comment"># 在每个BERT输入序列中，为遮蔽语言模型任务预测10(64 * 0.15)个位置</span></span><br><span class="line">    batch_size, max_len = <span class="number">512</span>, <span class="number">64</span></span><br><span class="line">    train_iter, vocab = load_data_wiki(batch_size, max_len)</span><br><span class="line">    <span class="keyword">for</span> (token_X, segments_X, valid_lens_X, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;shape of token_X, segments_X, valid_lens_X, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(token_X.shape, segments_X.shape, valid_lens_X.shape, pred_positions_X.shape, mlm_weights_X.shape,</span><br><span class="line">              mlm_Y.shape, nsp_y.shape)</span><br><span class="line">        <span class="keyword">break</span>  <span class="comment"># 看一个就行</span></span><br><span class="line">    <span class="comment"># 看一下词表大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;vocab size:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>output</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TBD</span><br><span class="line">似乎有点bug，zip文件没法从官网下载下来</span><br></pre></td></tr></table></figure>

<h2 id="预训练BERT"><a href="#预训练BERT" class="headerlink" title="预训练BERT"></a>预训练BERT</h2><p>TBD</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://nju-ymhui.github.io/ymhui.github.io">画船听雨眠y</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://nju-ymhui.github.io/ymhui.github.io/2024/10/01/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">https://nju-ymhui.github.io/ymhui.github.io/2024/10/01/自然语言处理/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://nju-ymhui.github.io/ymhui.github.io" target="_blank">(๑>ᴗ<๑)</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/ymhui.github.io/tags/pytorch/">pytorch</a><a class="post-meta__tags" href="/ymhui.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-share"><div class="social-share" data-image="/ymhui.github.io/img/myicon.ico" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>真的不考虑支持一下吗 (╯︵╰,)</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/ymhui.github.io/img/qrcode.png" target="_blank"><img class="post-qr-code-img" src="/ymhui.github.io/img/qrcode.png" alt="微信支付"/></a><div class="post-qr-code-desc">微信支付</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="next-post pull-full" href="/ymhui.github.io/2024/09/27/%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="现代卷积神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">现代卷积神经网络</div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a href="/ymhui.github.io/2024/09/15/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="多层感知机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-15</div><div class="title">多层感知机</div></div></a><a href="/ymhui.github.io/2024/09/15/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="线性神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-15</div><div class="title">线性神经网络</div></div></a><a href="/ymhui.github.io/2024/09/16/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="卷积神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-16</div><div class="title">卷积神经网络</div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="/ymhui.github.io/img/myicon.ico" onerror="this.onerror=null;this.src='/ymhui.github.io/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">画船听雨眠y</div><div class="author-info-description">美好的一天从打代码开始结束</div><div class="site-data"><a href="/ymhui.github.io/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/ymhui.github.io/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/ymhui.github.io/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/NJU-ymhui"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/NJU-ymhui" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="toc-number">1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">自然语言处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-number">2.1.</span> <span class="toc-text">词嵌入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%86%8D%E4%BD%BF%E7%94%A8%E7%8B%AC%E7%83%AD%E5%90%91%E9%87%8F"><span class="toc-number">2.1.1.</span> <span class="toc-text">为什么不再使用独热向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%9B%91%E7%9D%A3%E7%9A%84word2vec"><span class="toc-number">2.1.2.</span> <span class="toc-text">自监督的word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B7%B3%E5%85%83%E6%A8%A1%E5%9E%8BSkip-Gram"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">跳元模型Skip-Gram</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9E%E7%BB%AD%E8%AF%8D%E8%A2%8BCBOW"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">连续词袋CBOW</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%91%E4%BC%BC%E8%AE%AD%E7%BB%83"><span class="toc-number">2.2.</span> <span class="toc-text">近似训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="toc-number">2.2.1.</span> <span class="toc-text">负采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%82%E5%BA%8FSoftmax"><span class="toc-number">2.2.2.</span> <span class="toc-text">层序Softmax</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.3.</span> <span class="toc-text">预训练词嵌入的数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83word2vec"><span class="toc-number">2.4.</span> <span class="toc-text">预训练word2vec</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E5%B1%80%E5%90%91%E9%87%8F%E7%9A%84%E8%AF%8D%E5%B5%8C%E5%85%A5GloVe"><span class="toc-number">2.5.</span> <span class="toc-text">全局向量的词嵌入GloVe</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%90%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-number">2.6.</span> <span class="toc-text">子词嵌入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#fastText%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.6.1.</span> <span class="toc-text">fastText模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%97%E8%8A%82%E5%AF%B9%E7%BC%96%E7%A0%81"><span class="toc-number">2.6.1.1.</span> <span class="toc-text">字节对编码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%92%8C%E7%B1%BB%E6%AF%94%E4%BB%BB%E5%8A%A1"><span class="toc-number">2.7.</span> <span class="toc-text">词的相似性和类比任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformers%E7%9A%84%E5%8F%8C%E5%90%91%E7%BC%96%E7%A0%81%E5%99%A8%E8%A1%A8%E7%A4%BA-BERT"><span class="toc-number">2.8.</span> <span class="toc-text">Transformers的双向编码器表示(BERT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3-%E6%95%8F%E6%84%9F%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.8.1.</span> <span class="toc-text">上下文无关&#x2F;敏感模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%AE%9A%E4%BB%BB%E5%8A%A1-%E4%B8%8D%E5%8F%AF%E7%9F%A5%E4%BB%BB%E5%8A%A1"><span class="toc-number">2.8.2.</span> <span class="toc-text">特定任务&#x2F;不可知任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT"><span class="toc-number">2.8.3.</span> <span class="toc-text">BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83BERT%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.9.</span> <span class="toc-text">用于预训练BERT的数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83BERT"><span class="toc-number">2.10.</span> <span class="toc-text">预训练BERT</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2024/10/01/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" title="自然语言处理">自然语言处理</a><time datetime="2024-10-01T02:15:34.000Z" title="发表于 2024-10-01 10:15:34">2024-10-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2024/09/27/%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="现代卷积神经网络">现代卷积神经网络</a><time datetime="2024-09-27T10:54:02.000Z" title="发表于 2024-09-27 18:54:02">2024-09-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="现代循环神经网络">现代循环神经网络</a><time datetime="2024-09-23T09:25:08.000Z" title="发表于 2024-09-23 17:25:08">2024-09-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2024/09/20/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="循环神经网络">循环神经网络</a><time datetime="2024-09-20T08:03:49.000Z" title="发表于 2024-09-20 16:03:49">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/ymhui.github.io/2024/09/16/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="注意力机制">注意力机制</a><time datetime="2024-09-16T14:59:39.000Z" title="发表于 2024-09-16 22:59:39">2024-09-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By 画船听雨眠y</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/ymhui.github.io/js/utils.js"></script><script src="/ymhui.github.io/js/main.js"></script><div class="js-pjax"></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>