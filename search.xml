<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>软件测试</title>
      <link href="/ymhui.github.io/2025/01/03/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/"/>
      <url>/ymhui.github.io/2025/01/03/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<h1>回归测试</h1><p>回归测试作为一种有效的方法，可有效保证代码修改的正确性并避免<br>代码修改对被测程序其他模块产生副作用；回归测试一般占软件产品测试预算的80%以上，占软件维护预算的50%以上</p><h2 id="现有用例策略的问题">现有用例策略的问题</h2><p><strong>重复执行已有测试用例</strong></p><ul><li>用例庞大</li><li>用例冗余</li><li>用例失效</li><li>用例缺失</li></ul><h2 id="回归测试优化">回归测试优化</h2><ol><li>测试用例修复</li><li><strong>测试用例选择</strong></li><li>测试用例扩充</li><li>测试用例缩减</li><li><strong>测试用例优先级</strong></li></ol><h1>随机测试</h1><p>大数定律：简单地说，就是随着样本数的增加，样本的平均值会越来越接近于期望值</p><h1>变异测试</h1><p>TBD</p><h1>测试预言问题</h1><p>测试预言是自动化测试中不可或缺的部分</p><h2 id="什么是测试预言？">什么是测试预言？</h2><ul><li>是文档：体现被测单元的预期功能</li><li>是机制：验证待测程序的行为是否符合预期</li><li>是程序：判定程序的执行是否违反了某种正确性政策</li><li>是约束</li><li>是映射</li></ul><p>断言、神经网络的输出都是测试预言的一种</p><h2 id="隐式预言or显式预言">隐式预言or显式预言</h2><p>隐式预言检测显著缺陷，显式预言检测功能性缺陷</p><h2 id="预言问题">预言问题</h2><p>给定系统的输入，如何找到能够正确辨别出符合期望的正确行为与发现潜在的不正确行为测试预言的挑战性难题</p><h2 id="差分测试">差分测试</h2><p>基本思想：通过将同一测试用例运行到一系列相似功能的应用中观察执行差异来检测bug</p><h3 id="定义">定义</h3><p>一种常用的软件测试技术，通过向一系列类似的应用程序（或同一应用程序的不同实现）提供相同的输入，根据这些相似程序执行结果是否存在差异来判定是否检测到缺陷</p><h3 id="流程">流程</h3><p>利用相似/竞品软件系统进行测试</p><ul><li>确定一组相似的待测程序</li><li>生成测试输入和输出</li><li>正确性验证</li></ul><h3 id="应用">应用</h3><ul><li>针对编译器</li><li>针对调试器</li><li>针对深度学习框架</li><li>模糊测试 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="bold">+</mtext></mrow><annotation encoding="application/x-tex">\textbf{+}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.1333em;"></span><span class="mord text"><span class="mord textbf">+</span></span></span></span></span> 变异差分（MuFuzz）</li></ul><h2 id="蜕变测试">蜕变测试</h2><h3 id="动机">动机</h3><p>两大测试难题：</p><ul><li>可靠测试集问题</li><li>预言问题</li></ul><h3 id="本质">本质</h3><p>充分利用成功测试用例</p><h3 id="三要素">三要素</h3><ul><li>蜕变关系：一组待测算法/功能的必要属性，核心</li><li>蜕变集合：表达了蜕变关系的一组测试输入组成的集合</li><li>蜕变测试过程：应用蜕变进行测试的一般流程</li></ul><h2 id="总结">总结</h2><p>辨析蜕变测试和差分测试</p><h1>测试用例优先级（重要）</h1><p>Test Case Prioritization，TCP</p><p>通过设定特定优先级准则（执行时间，代码覆盖等），对测试用例进行优先级排序以优化其执行次序，旨在最大化优先级目标，例如最大化测试用例集的早期缺陷检测速率</p><h2 id="算法">算法</h2><h3 id="基于贪心的TCP策略">基于贪心的TCP策略</h3><h4 id="全局贪心">全局贪心</h4><ul><li>每轮选择覆盖最多代码单元的测试用例</li><li>多个用例相同则随机选择</li></ul><h4 id="增量贪心">增量贪心</h4><ul><li>每轮优先挑选覆盖最多，<font color="red">且未被已选择用例覆盖代码单元的</font>测试用例。(即该覆盖最多指的是<font color="red">覆盖最多新的</font>)</li><li>所有代码单元均已被覆盖则重置优先级排序过程</li><li>多个用例相同随机选择</li></ul><h4 id="算法流程模拟">算法流程模拟</h4><ol><li>初始化覆盖矩阵和语句覆盖数组c，所有代码单元均未被覆盖，c为全0数组</li></ol><ol start="2"><li>根据覆盖矩阵和单元覆盖数组 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span>，计算待排序用例覆盖值，选择测试用例 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">t_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li></ol><ol start="3"><li>选择测试用例 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">t_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，更新语句覆盖数组，以及待排序测试用例覆盖值</li></ol><ol start="4"><li>选择测试用例 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">t_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，更新覆盖数组，计算剩余测试用例覆盖新的语句数量</li></ol><ol start="5"><li>选择测试用例 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">t_4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，更新覆盖数组，计算剩余测试用例覆盖新的语句数量</li></ol><ol start="6"><li>所有语句均被覆盖，重置语句覆盖数组，计算剩余测试用例覆盖值</li></ol><ol start="7"><li>选择测试用例 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">t_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，更新覆盖数组，计算剩余测试用例覆盖新的语句数量</li></ol><ol start="8"><li>无待排序用例，排序结束</li></ol><h4 id="贪心策略改进——字典排序">贪心策略改进——字典排序</h4><p>选择<strong>使得有序覆盖数组的字典序更大</strong>的测试用例</p><h3 id="基于相似性的TCP策略">基于相似性的TCP策略</h3><p>原理：故障通常聚集在一起</p><p>每轮优先与已选择测试用例集差异性最大的测试用例。让测试用例均匀地分布在输入域中</p><h4 id="测试用例距离">测试用例距离</h4><p>首先要会算测试用例之间的距离 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mi>a</mi><mi>c</mi><mi>c</mi><mi>a</mi><mi>r</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">Jaccard</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mord mathnormal">a</span><span class="mord mathnormal">cc</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">d</span></span></span></span></p><p>设 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">U(t_1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">U(t_2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 为测试用例 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">t_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">t_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 所覆盖的代码单元集合，那么这两个测试用例之间的距离计算如下：</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mi>a</mi><mi>c</mi><mi>c</mi><mi>a</mi><mi>r</mi><mi>d</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><mtext> </mtext><msub><mi>t</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi mathvariant="normal">∣</mi><mi>U</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mtext> </mtext><mo>∩</mo><mtext> </mtext><mi>U</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mi>U</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mtext> </mtext><mo>∪</mo><mtext> </mtext><mi>U</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Jaccard(t_1,~t_2) = 1 - \frac{|U(t_1) ~\cap ~U(t_2)|}{|U(t_1) ~\cup ~U(t_2)|}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mord mathnormal">a</span><span class="mord mathnormal">cc</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace nobreak"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mbin mtight">∪</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mord mtight">∣</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mbin mtight">∩</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mord mtight">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><p>然后是测试用例和测试用例集之间的距离计算：分别使用最小距离、平均距离和最大距离度量方式计算待选择用例 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">t_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 与已选择用例集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 的距离</p><h4 id="算法流程模拟-2">算法流程模拟</h4><h3 id="基于搜索的TCP策略">基于搜索的TCP策略</h3><p>探索用例优先级排序组合的状态空间，以此找到检测错误更快的用例序列。</p><h4 id="步骤">步骤</h4><ol><li>种群构造：生成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 个测试用例序列，解的编码形式为优先级排序的测试用例编号位置。如初始时对6个测试用例构造2个个体:</li></ol><ol start="2"><li>交叉：使用单点交叉的方式。随机生成切割点，互相交换两个用例序列切割点后部分的片段，仅交换相同测试用例的部分。如个体1和个体2通过交叉算子生成个体3和个体4：</li></ol><ol start="3"><li>变异：对种群中的个体进行基因值的改变操作。以一定概率选择测试用例，并随机生成两个测试用例位置，进行互换，产生新的测试用例序列。如示例对个体1进行变异算子生成新的个体：</li></ol><ol start="4"><li><p>评估值计算：以语句覆盖为例，给定程序包含 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> 个语句 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>𝑠</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>s</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">M = \{𝑠_1,s_2,\cdots, s_m\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> 个测试用例 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">T = \{t_1,t_2,\cdots,t_n\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> , <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">T&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> 为某一次搜索中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span> 的一个优先级序列，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">TS_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 为该测试用例序列 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">T&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> 中第一个覆盖语句 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的测试用例下标，那么其适应度计算为：</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>P</mi><mi>S</mi><mi>C</mi><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi>T</mi><msub><mi>S</mi><mn>1</mn></msub><mtext> </mtext><mo>+</mo><mtext> </mtext><mi>T</mi><msub><mi>S</mi><mn>2</mn></msub><mtext> </mtext><mo>+</mo><mtext> </mtext><mo>⋯</mo><mtext> </mtext><mo>+</mo><mtext> </mtext><mi>T</mi><msub><mi>S</mi><mi>m</mi></msub></mrow><mrow><mi>n</mi><mtext> </mtext><mo>∗</mo><mtext> </mtext><mi>m</mi></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>n</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">APSC = 1 - \frac{TS_1 ~+~ TS_2 ~+~ \cdots ~+~ TS_m}{n~*~m} + \frac{1}{2n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.07153em;">PSC</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2334em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8884em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mbin mtight">∗</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0576em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mbin mtight">+</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0576em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mbin mtight">+</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="minner mtight">⋯</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mbin mtight">+</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.0576em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p></li></ol><h3 id="基于机器学习的TCP策略">基于机器学习的TCP策略</h3><ul><li><p>编译器场景中的测试用例优先级</p></li><li><p>对测试用例特征进行学习，根据预测的缺陷检测概率进行优先级排序。</p></li></ul><h4 id="步骤-2">步骤</h4><ol><li>特征提取：设计并提取测试程序中源码特征。</li><li>缺陷模型：建立模型预测测试程序检测缺陷的概率。</li><li>开销模型：建立模型预测测试程序的运行时间。</li><li>测试优先级：基于单位时间内检测缺陷能力进行优先级排序。</li></ol><h2 id="APFD（必考）">APFD（必考）</h2><p>平均故障百分比 Average Percentage of Fault Detected</p><p>其取值范围介于 0~100%之间，取值越高，则缺陷检测速度越快</p><p>算法：给定程序包含 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> 个故障 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐹</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>f</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>f</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>f</mi><mi>m</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">𝐹 = \{f_1, f_2, \cdots, f_m\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> 个测试用例 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝑇</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">𝑇 = \{t_1, t_2, \cdots, t_n\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> ，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">T&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> 为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span> 的一个优先级序列，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><msub><mi>F</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">TF_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 为该测试用例序列 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">T&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> 中第一个检测到故障 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的测试用例下标，则该优先级序列 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">T&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> 的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>P</mi><mi>F</mi><mi>D</mi></mrow><annotation encoding="application/x-tex">APFD</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em;">PF</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span> 值计算公式为</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>P</mi><mi>F</mi><mi>D</mi><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi>T</mi><msub><mi>F</mi><mn>1</mn></msub><mtext> </mtext><mo>+</mo><mtext> </mtext><mi>T</mi><msub><mi>F</mi><mn>2</mn></msub><mtext> </mtext><mo>+</mo><mtext> </mtext><mo>⋯</mo><mtext> </mtext><mo>+</mo><mtext> </mtext><mi>T</mi><msub><mi>F</mi><mi>m</mi></msub></mrow><mrow><mi>n</mi><mtext> </mtext><mo>∗</mo><mtext> </mtext><mi>m</mi></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>n</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">APFD = 1 - \frac{TF_1~+~TF_2~+~\cdots~+~TF_m}{n~*~m} + \frac{1}{2n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em;">PF</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2334em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8884em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mbin mtight">∗</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mbin mtight">+</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mbin mtight">+</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="minner mtight">⋯</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mbin mtight">+</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><p>示例：</p><h2 id="算法应用">算法应用</h2><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>G</mi><mi>P</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">ChatGPT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">ha</span><span class="mord mathnormal" style="margin-right:0.13889em;">tGPT</span></span></span></span> 如是说 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>↓</mo></mrow><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mrel">↓</span></span></span></span></p><ol><li><strong>提高缺陷检测效率</strong>：在有限的时间内，优先执行那些最可能发现缺陷的测试用例，从而提高缺陷检测的效率。</li><li><strong>减少测试执行时间</strong>：通过根据优先级合理排序测试用例，减少不必要的测试执行时间，节省测试资源。</li><li><strong>增加测试覆盖率</strong>：在资源有限的情况下，优先执行能覆盖更多功能的测试用例，确保关键路径和高风险区域的测试更早完成。</li><li><strong>应对软件修改后的回归测试</strong>：在软件迭代或修复后，通过优先执行与修改相关的测试用例，确保新引入的缺陷能尽早被发现。</li></ol><hr><p>实证工作：</p><ul><li>传统动态排序技术评估</li><li>真实软件演化评估</li><li>动态与静态排序技术比较</li><li>黑盒与白盒排序技术比较</li></ul><h1>测试用例选择</h1><p>Test Case Selection，TCS</p><p>旨在从已有测试用例集中选择出所有可检测代码修改的测试用例，适用于因测试预算不足以致不能执行完所有测试用例的测试场景</p><h2 id="主要方法">主要方法</h2><ul><li>等价类划分</li><li>边界值分析</li><li>决策表测试</li><li>状态转换测试</li><li>随机测试</li><li>覆盖率分析</li><li>风险分析</li><li>回归测试</li></ul><h2 id="动态静态">动态静态</h2><ul><li><strong>静态</strong>测试用例选择是指在 <strong>不执行程序代码</strong> 或 <strong>不考虑程序实际行为</strong> 的情况下，根据事先定义的规则、设计文档、需求文档、或者代码结构来选择测试用例</li><li><strong>动态</strong>测试用例选择则是指根据 <strong>程序实际执行情况</strong>、<strong>程序行为</strong> 或 <strong>代码执行路径</strong> 来选择测试用例。它关注的是程序运行时的行为，包括覆盖不同的代码路径、条件判断、循环等</li></ul><h2 id="和测试用例优先级的区别和联系">和测试用例优先级的区别和联系</h2><table><thead><tr><th>特性</th><th><strong>测试用例优先级</strong></th><th><strong>测试用例选择</strong></th></tr></thead><tbody><tr><td><strong>定义</strong></td><td>根据重要性、风险等因素对测试用例进行排序，决定先执行哪些测试用例。</td><td>从所有可能的测试用例中，依据特定方法选择需要执行的测试用例。</td></tr><tr><td><strong>侧重点</strong></td><td>确定测试用例执行的顺序，优先执行最关键的测试用例。</td><td>确定哪些测试用例应该执行，注重覆盖系统的不同需求或行为。</td></tr><tr><td><strong>主要目标</strong></td><td>保证关键、风险高、复杂度高等测试用例优先被执行，以便及时发现潜在缺陷。</td><td>确保选择的测试用例能够覆盖关键功能、边界情况等，减少冗余并提高测试效率。</td></tr><tr><td><strong>使用阶段</strong></td><td>通常在测试计划阶段，确定测试用例执行的顺序。</td><td>在测试设计阶段，根据具体的选择方法决定哪些用例需要执行。</td></tr><tr><td><strong>执行顺序</strong></td><td>通过排序来安排测试用例执行的顺序。</td><td>通过选择和覆盖策略来确定执行的测试用例。</td></tr><tr><td><strong>输入依据</strong></td><td>依据功能的关键性、风险、复杂度等外部因素。</td><td>依据需求、设计、代码结构或状态转换等选择标准。</td></tr></tbody></table><h1>测试用例优先级 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>S</mi></mrow><annotation encoding="application/x-tex">VS</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 测试用例选择</h1><p>如上</p><h1>移动应用中的测试</h1><h2 id="基于GUI的自动化测试">基于GUI的自动化测试</h2><p>TBD</p><h2 id="众包测试">众包测试</h2><p>TBD</p><h1>AI测试</h1><p>TBD</p><h1>模糊测试</h1><p>TBD</p>]]></content>
      
      
      <categories>
          
          <category> 软件学院 </category>
          
          <category> 软件测试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件学院 </tag>
            
            <tag> 软件测试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++期末复习——简答题部分</title>
      <link href="/ymhui.github.io/2025/01/02/C-%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%E2%80%94%E2%80%94%E7%AE%80%E7%AD%94%E9%A2%98%E9%83%A8%E5%88%86/"/>
      <url>/ymhui.github.io/2025/01/02/C-%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%E2%80%94%E2%80%94%E7%AE%80%E7%AD%94%E9%A2%98%E9%83%A8%E5%88%86/</url>
      
        <content type="html"><![CDATA[<h1>C++期末简答题汇总（往年）</h1><p>大部分问题的答案在软件学院C++相关文章里应该都能找到答案，这里做一个合订本</p><h2 id="C-历史">C++历史</h2><p>？</p><h2 id="C和C-的关系">C和C++的关系</h2><ol><li>C++完全包含C语言成分，C++支持C所支持的全部编程技巧（C的超集）；同时C++还添加了OOP支持</li><li>任何C程序都能被C++用基本相同的方法编写，并具备相同的运行效率和空间</li><li>C++还引入了重载、内联函数、异常处理等功能，对C中的过程化控制及其功能进行了扩充</li><li>C++由以下4部分有机组成<ul><li>C</li><li>OOP</li><li>STL</li><li>Inside-Model</li></ul></li></ol><h2 id="表达式的值有哪些因素决定？表达式存在副作用吗">表达式的值有哪些因素决定？表达式存在副作用吗</h2><ol><li>表达式指由操作数、操作符和标点符号（比如逗号运算符）组成的序列，代表一个计算过程</li><li>表达式的值受操作符、操作数、优先级、结合性、求值次序、副作用、短路求值和类型转换（隐式or显式）影响</li><li>副作用：改变程序状态（即可能改变操作数或其他一些变量的值）</li></ol><h2 id="C-多态类型">C++多态类型</h2><ul><li>静态多态<ul><li>一名多用（函数重载、操作符重载）</li><li>模板编程（template）/ 泛型</li></ul></li><li>动态多态<ul><li>虚函数</li></ul></li></ul><h2 id="解释指针类型和引用类型的差别">解释指针类型和引用类型的差别</h2><ol><li>引用类型与指针类型都可以实现通过一个变量访问另一个变量，但访问的语法形式不同：引用采用直接访问形式，而指针采用间接访问形式</li><li>引用被创建是必须初始化（即必须绑定到变量上），而指针可以在任何时候初始化</li><li>不能有空引用，引用必须与合法变量绑定；但可以有空指针</li><li>一旦引用被初始化就不能改变绑定的变量，而指针可以随时改变指向的变量</li><li>从2 ~ 4可以看出引用比指针安全</li><li><s>传参时引用类型的实参是一个变量，而指针参数的实参是一个变量的地址（这条感觉怪怪的，难道指针就不是变量了吗？）</s></li></ol><h2 id="C-中继承与虚继承的差异">C++中继承与虚继承的差异</h2><ol><li>普通继承在面对多继承时可能会引入名冲突问题，每个继承路径都有自己的基类实例，而虚继承可以解决名冲突问题（形成一个菱形继承图“格”）</li><li>因为虚继承<strong>并没有真的继承基类的成员，而是生成了一个指向基类的指针</strong>，因此不会有名冲突问题（基类实际上只有一份，被所有虚继承的派生类共用；相当于虚基类被合并，只有一个副本）</li><li>虚继承的基类无法用<code>static_cast&lt;&gt;()</code>转成派生类</li><li>虚继承可能带来额外的性能开销</li><li>虚基类的构造函数优先于非虚基类的构造函数被调用</li></ol><h2 id="为什么实现两个版本的下标运算符重载">为什么实现两个版本的下标运算符重载</h2><p>两个版本是一个带<code>const</code>，一个不带</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">string</span> &#123;</span><br><span class="line">  <span class="type">char</span>* p;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">string</span><span class="params">(<span class="type">char</span>* p1)</span> </span>&#123;</span><br><span class="line">    p = <span class="keyword">new</span> <span class="type">char</span>[<span class="built_in">strlen</span>(p1) + <span class="number">1</span>];</span><br><span class="line">    <span class="built_in">strcpy</span>(p, p1);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 版本1</span></span><br><span class="line">  <span class="type">char</span>&amp; <span class="keyword">operator</span>[](<span class="type">int</span> i) &#123;</span><br><span class="line">    <span class="keyword">return</span> p[i];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 版本2</span></span><br><span class="line">  <span class="type">char</span> <span class="keyword">operator</span>[](<span class="type">int</span> i) <span class="type">const</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> p[i];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">string</span>() &#123;<span class="keyword">delete</span>[] p;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>需要分别对应两种<code>*this</code>指针，<code>const T* this</code>和<code>T* this</code></p><p>且若某个调用产生了两个候选函数，而一个有<code>const</code>一个没有，若调用处不修改对象，则优先选择带有<code>const</code>的函数，因为<code>const</code>保证不修改，选<code>const</code>函数使得更多代码可以在<code>const</code>上下文中使用，避免非<code>const</code>函数隐式修改。</p><h2 id="请简述右值引用与左值引用的异同">请简述右值引用与左值引用的异同</h2><p><strong>相同点：</strong></p><ul><li>本质都是引用</li><li>语法相似，左值引用是<code>&amp;</code>，右值引用是<code>&amp;&amp;</code></li><li>传递效率都很高，无论是左值还是右值引用都避免了传值时的值拷贝</li><li>两者都可以作为函数参数，允许直接操作被引用的对象</li></ul><p><strong>不同点：</strong></p><ul><li>左值引用引用的是左值对象（即有名字、可持久存在的对象），而右值引用引用的是右值对象（即无名字、临时的对象，通常是表达式的结果）<ul><li>左值是可以取地址的实体，右值通常不能取地址</li></ul></li><li>左值引用常常用于需要对现有变量进行修改或访问的地方，而右值引用往往用于临时对象操作，比如移动语义（可以提高效率，避免不必要拷贝）</li><li>左值引用提出绑定到长生命周期对象上，被引用的对象必须是有效的；右值引用通常绑定到短生命周期的临时对象上，右值引用延长了右值对象的生命周期（只要右值引用还在，该对象就还能访问）</li><li>右值引用可以用于移动语义，高效转移资源</li><li>在函数重载时左值引用和右值引用是不同的参数类型</li></ul><h2 id="Lambda表达式的作用并比较它与重载了函数调用操作符的函数对象的差别">Lambda表达式的作用并比较它与重载了函数调用操作符的函数对象的差别</h2><ul><li>lambda表达式作为匿名函数使用，避免了一些轻量函数使用时还要定义，避免繁琐步骤，更加简洁</li><li>lambda表达式不需要显式指明函数返回类型，函数对象需要</li><li>lambda表达式不需要先声明再使用（即用即定义），函数对象需要</li><li>lambda表达式实际上是用函数对象实现的（编译器生成一个匿名函数对象类，lambda表达式返回其实例，为右值）</li></ul><h2 id="简述C与C-混合编程时需要注意的问题">简述C与C++混合编程时需要注意的问题</h2><ul><li>C++是C的超集，而C不支持OOP，不要在混合编程中出现<code>class</code>等面向对象关键字</li><li>调C库函数时附加关键字<code>extern C</code></li><li>注意函数重载，C不支持</li><li>注意求值顺序和副作用</li><li>内存分配差异(new/malloc, delete/free)</li><li>内存占用差异</li></ul><h2 id="内联函数的作用？随意使用的问题？阐述合理使用的建议">内联函数的作用？随意使用的问题？阐述合理使用的建议</h2><ol><li>作用：<ul><li>提高可读性：编译器眼中没有函数但程序员不是，可以提高代码的可读性</li><li>提高效率：<strong>编译器不再需要进行函数调用</strong>，而是直接顺序地执行等价代码，避免了函数调用的时间开销</li></ul></li><li>问题：<ul><li>增大目标代码</li><li>病态的换页</li><li>降低指令快取装置的命中率</li></ul></li><li>建议：<ul><li>适合于频率高、简单的小段代码</li><li>不要滥用<code>inline</code></li><li>不要内联复杂的控制逻辑</li><li>不能内联递归</li><li>把内联函数扔头文件里</li></ul></li></ol><h2 id="C-关键字const的几种使用方法，并给出示例">C++关键字const的几种使用方法，并给出示例</h2><ol><li>修饰基本数据类型，表示常量</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> a = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><ol start="2"><li>修饰指针，两种，或者说三种</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> a = <span class="number">114</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span>* pa1 = &amp;a; <span class="comment">// 指针常量，指向空间的值不能改</span></span><br><span class="line"><span class="type">int</span>* <span class="type">const</span> pa2 = &amp;a; <span class="comment">// 常量指针，指针本身值不能改</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span>* <span class="type">const</span> = &amp;a; <span class="comment">// 综上</span></span><br></pre></td></tr></table></figure><ol start="3"><li>修饰（成员）函数</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">f</span><span class="params">(<span class="type">const</span>&amp; <span class="type">int</span>)</span></span>; <span class="comment">// 参数是常量</span></span><br><span class="line"><span class="built_in">f</span>(<span class="type">int</span>&amp;) <span class="type">const</span>; <span class="comment">// 函数内部不会修改成员变量，即在该函数中所有成员都是常量（编译器自动加const）</span></span><br></pre></td></tr></table></figure><ol start="4"><li>修饰对象</li></ol><p>但要保证对象的所有变量都能在声明对象时被初始化（要么内部实现声明时初始化，要么构造函数实现）</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> p = <span class="number">1</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> A a;</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> p;</span><br><span class="line">    <span class="built_in">A</span>() &#123;</span><br><span class="line">    <span class="comment">// p = 1; 注释掉也行</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> A a;</span><br></pre></td></tr></table></figure><h2 id="什么是纯虚函数、虚函数和非虚函数？合理定义三种成员函数的基本原则是什么？请给出一个你认为合理定义的实例">什么是纯虚函数、虚函数和非虚函数？合理定义三种成员函数的基本原则是什么？请给出一个你认为合理定义的实例</h2><ul><li>纯虚函数<ul><li>只有函数接口会被继承</li><li>派生类必须继承函数接口</li><li>派生类必须提供实现代码</li></ul></li><li>一般虚函数<ul><li>函数的接口及缺省的实现代码都会被继承</li><li>子类必须继承函数接口</li><li>可以缺省实现代码</li></ul></li><li>非虚函数<ul><li>函数的代码及其实现代码都会被继承</li><li>必须同时继承接口和实现代码，无法重写以提供自己的实现</li></ul></li></ul><p>原则：</p><ul><li>类的成员函数才能是虚函数</li><li>静态成员函数不能是虚函数</li><li>内联不能是虚函数</li><li>构造不能是虚函数</li><li>析构往往是虚函数</li></ul><h2 id="C-程序设计中，可以利用析构函数防止资源泄露，请给出模板auto-ptr的基本定义及使用实例">C++程序设计中，可以利用析构函数防止资源泄露，请给出模板auto_ptr的基本定义及使用实例</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">auto_ptr</span> &#123;</span><br><span class="line">  T* ptr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">auto_ptr</span>(): <span class="built_in">ptr</span>(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">auto_ptr</span><span class="params">(T* p)</span>: ptr(p) &#123;</span>&#125;</span><br><span class="line"></span><br><span class="line">  T&amp; <span class="keyword">operator</span>*() &#123;</span><br><span class="line">    <span class="keyword">return</span> *ptr;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T&amp; <span class="keyword">operator</span>*() <span class="type">const</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> *ptr;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T* <span class="keyword">operator</span>-&gt;() &#123;<span class="keyword">return</span> ptr;&#125;</span><br><span class="line"></span><br><span class="line">  ~<span class="built_in">auto_ptr</span>() &#123;</span><br><span class="line">    <span class="keyword">if</span> (ptr != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="keyword">delete</span> ptr;</span><br><span class="line">      ptr = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="请阐述OOP中引入构造函数机制的原因，并请给出控制一个类创建实例个数的手段（举例）">请阐述OOP中引入构造函数机制的原因，并请给出控制一个类创建实例个数的手段（举例）</h2><p>单例模式、或者使用一个静态成员变量记录实例化的类数量，超过则拒绝实例化。</p><p>具体略</p><h2 id="何为引用？其主要作用是什么？何时需要将一个函数的返回值类型定义为引用类型？如果随意将函数的返回值类型定义为引用类型会有什么危害？">何为引用？其主要作用是什么？何时需要将一个函数的返回值类型定义为引用类型？如果随意将函数的返回值类型定义为引用类型会有什么危害？</h2><ol><li>定义：C++通过引用将一个左值和已有的变量绑定在一起（称为引用），引用和其绑定的变量使用相同的内存，被等价的修改，相当于变量的一个别名</li><li>作用：主要用于函数传参时代替指针，形参声明引用，实参传引用，可以在函数内部修改外部变量（通过引用）</li><li>什么时候返回：当希望提高效率避免对象值拷贝时，返回引用</li><li>危害：若返回值是引用，不能把局部变量返回，这是<strong>UB</strong></li></ol><h2 id="你认为最有价值的C-程序设计应该遵守的5条规则，并简明分析其意义所在">你认为最有价值的C++程序设计应该遵守的5条规则，并简明分析其意义所在</h2><p>略</p><h2 id="C-赋予一个空类哪些成员函数">C++赋予一个空类哪些成员函数</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Empty</span> &#123;</span><br><span class="line"><span class="comment">// Empty();</span></span><br><span class="line">    <span class="comment">// ~Empty();</span></span><br><span class="line">    <span class="comment">// Empty(const Empty&amp;);</span></span><br><span class="line">    <span class="comment">// Empty&amp; operator=(const Empty&amp;);</span></span><br><span class="line">    <span class="comment">// Empty* operator&amp;();</span></span><br><span class="line">    <span class="comment">// const Empty* operator&amp;() const;</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>更详细的见<em>C++面向对象编程基础</em></p><h2 id="define和template的区别">define和template的区别</h2><ul><li>define不做类型检查，直接展开，template会类型检查</li><li>define的作用范围是全局的，除非用<code>#undef</code>结束定义</li><li>模板很好的支持代码复用</li></ul><h2 id="面向对象程序设计的特点">面向对象程序设计的特点</h2><p>封装、继承、多态，通过消息传递、对象间分工协作实现程序运转</p><h2 id="什么是ADT？试以时间类型为例简要描述">什么是ADT？试以时间类型为例简要描述</h2><p>Abstract Data Type 抽象数据类型</p><p>结构化编程需要关注抽象数据类型、联系、优先级、类型转换（强制、隐式、溢出？）、求值顺序、副作用</p><p>描述略</p>]]></content>
      
      
      <categories>
          
          <category> 软件学院 </category>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 软件学院 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++常用map</title>
      <link href="/ymhui.github.io/2024/12/25/C-%E5%B8%B8%E7%94%A8map/"/>
      <url>/ymhui.github.io/2024/12/25/C-%E5%B8%B8%E7%94%A8map/</url>
      
        <content type="html"><![CDATA[<h1>C++使用哈希表</h1><p>主要介绍<code>map</code>和<code>unordered_map</code>的使用和区别</p><h2 id="map">map</h2><ul><li><p>底层实现是 <strong>红黑树</strong>（自平衡的二叉查找树）。</p></li><li><p>因为使用了红黑树，<code>std::map</code> 的元素总是 <strong>按键的顺序</strong>（升序或降序）排列。</p></li><li><p>每次插入、删除和查找元素时，红黑树会保持自平衡，以确保查找的时间复杂度为 <strong>O(log n)</strong>。</p></li></ul><h2 id="unordered-map">unordered_map</h2><ul><li><p>底层实现是 <strong>哈希表</strong>（Hash Table）。</p></li><li><p>哈希表并不保证元素按任何顺序排列，因此插入顺序和查找顺序可能是不可预测的。</p></li><li><p>插入、删除和查找元素的平均时间复杂度是 <strong>O(1)</strong>，但在最坏情况下，可能会退化到 <strong>O(n)</strong>，例如哈希冲突过多时。</p></li></ul><h2 id="使用语法">使用语法</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">map&lt;string, <span class="type">int</span>&gt; use_map;</span><br><span class="line">unordered_map&lt;string, <span class="type">int</span>&gt; use_unordered;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    use_map[<span class="string">&quot;second&quot;</span>] = <span class="number">2</span>;</span><br><span class="line">    use_map[<span class="string">&quot;first&quot;</span>] = <span class="number">1</span>;</span><br><span class="line">    use_map[<span class="string">&quot;third&quot;</span>] = <span class="number">3</span>;</span><br><span class="line">    use_map[<span class="string">&quot;forth&quot;</span>] = <span class="number">4</span>;</span><br><span class="line">    use_map[<span class="string">&quot;fifth&quot;</span>] = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">    use_unordered[<span class="string">&quot;second&quot;</span>] = <span class="number">2</span>;</span><br><span class="line">use_unordered[<span class="string">&quot;first&quot;</span>] = <span class="number">1</span>;</span><br><span class="line">    use_unordered[<span class="string">&quot;third&quot;</span>] = <span class="number">3</span>;</span><br><span class="line">    use_unordered[<span class="string">&quot;forth&quot;</span>] = <span class="number">4</span>;</span><br><span class="line">    use_unordered[<span class="string">&quot;fifth&quot;</span>] = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> p : use_map) &#123;</span><br><span class="line">        cout &lt;&lt; p.first &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125; <span class="comment">// 总是输出fifth first forth second third </span></span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> p : use_unordered) &#123;</span><br><span class="line">        cout &lt;&lt; p.first &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125; <span class="comment">// 输出不一定是有序的</span></span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结">总结</h2><table><thead><tr><th>特性</th><th><code>std::map</code></th><th><code>std::unordered_map</code></th></tr></thead><tbody><tr><td>查找、插入、删除的复杂度</td><td>O(log n)</td><td>O(1) 平均，O(n) 最坏</td></tr><tr><td>是否保证顺序</td><td>按键排序，保证顺序，默认升序</td><td>不保证顺序</td></tr><tr><td>内存消耗</td><td>相对较高，保持树的结构</td><td>较高，依赖于哈希表和桶的数量</td></tr><tr><td>使用场景</td><td>需要顺序、排序、定制比较函数</td><td>高效查找、插入、删除，无需排序</td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++容器常用操作</title>
      <link href="/ymhui.github.io/2024/12/25/C-%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
      <url>/ymhui.github.io/2024/12/25/C-%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h1>C++容器</h1><p>本文主要介绍C++容器的常用操作及使用方法</p><h2 id="vector">vector</h2><p>非常常用的容器</p><h3 id="1-基本操作">1. <strong>基本操作</strong></h3><ul><li><p><strong>创建和初始化</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::vector&lt;<span class="type">int</span>&gt; v;  <span class="comment">// 默认空的 vector</span></span><br><span class="line"><span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">v</span><span class="params">(<span class="number">10</span>)</span></span>;  <span class="comment">// 创建一个大小为 10 的 vector，元素初始化为 0</span></span><br><span class="line"><span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">v</span><span class="params">(<span class="number">10</span>, <span class="number">5</span>)</span></span>;  <span class="comment">// 创建一个大小为 10 的 vector，所有元素初始化为 5</span></span><br><span class="line">std::vector&lt;<span class="type">int</span>&gt; v = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;  <span class="comment">// 使用列表初始化</span></span><br></pre></td></tr></table></figure></li><li><p><strong>获取大小</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">size_t</span> size = v.<span class="built_in">size</span>();  <span class="comment">// 获取 vector 的元素个数</span></span><br></pre></td></tr></table></figure></li><li><p><strong>判断是否为空</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">bool</span> is_empty = v.<span class="built_in">empty</span>();  <span class="comment">// 如果 vector 为空，返回 true</span></span><br></pre></td></tr></table></figure></li><li><p><strong>访问元素</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> first = v[<span class="number">0</span>];  <span class="comment">// 通过索引访问元素</span></span><br><span class="line"><span class="type">int</span> second = v.<span class="built_in">at</span>(<span class="number">1</span>);  <span class="comment">// 使用 at() 安全访问（超出范围会抛出异常）</span></span><br><span class="line"><span class="type">int</span> last = v.<span class="built_in">back</span>();  <span class="comment">// 获取最后一个元素</span></span><br><span class="line"><span class="type">int</span> first_element = v.<span class="built_in">front</span>();  <span class="comment">// 获取第一个元素</span></span><br></pre></td></tr></table></figure></li><li><p><strong>添加元素</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">v.<span class="built_in">push_back</span>(<span class="number">10</span>);  <span class="comment">// 向 vector 末尾添加元素</span></span><br><span class="line">v.<span class="built_in">push_back</span>(<span class="number">20</span>);</span><br></pre></td></tr></table></figure></li><li><p><strong>删除元素</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">v.<span class="built_in">pop_back</span>();  <span class="comment">// 删除 vector 最后一个元素</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="2-插入和删除元素（中间位置）">2. <strong>插入和删除元素（中间位置）</strong></h3><ul><li><p><strong>在指定位置插入元素</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">v.<span class="built_in">insert</span>(v.<span class="built_in">begin</span>() + <span class="number">1</span>, <span class="number">50</span>);  <span class="comment">// 在第 1 个位置插入元素 50</span></span><br><span class="line">v.<span class="built_in">insert</span>(v.<span class="built_in">end</span>(), <span class="number">100</span>);  <span class="comment">// 在末尾插入元素 100</span></span><br></pre></td></tr></table></figure></li><li><p><strong>插入多个元素</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">v.<span class="built_in">insert</span>(v.<span class="built_in">begin</span>() + <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>);  <span class="comment">// 在第 2 个位置插入 3 个元素，每个元素是 10</span></span><br></pre></td></tr></table></figure></li><li><p><strong>删除指定位置的元素</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">v.<span class="built_in">erase</span>(v.<span class="built_in">begin</span>() + <span class="number">2</span>);  <span class="comment">// 删除第 2 个位置的元素</span></span><br></pre></td></tr></table></figure></li><li><p><strong>删除一段元素</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">v.<span class="built_in">erase</span>(v.<span class="built_in">begin</span>() + <span class="number">1</span>, v.<span class="built_in">begin</span>() + <span class="number">3</span>);  <span class="comment">// 删除从位置 1 到 2（不包括 3）之间的元素</span></span><br></pre></td></tr></table></figure></li><li><p><strong>删除所有元素</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">v.<span class="built_in">clear</span>();  <span class="comment">// 清空 vector 所有元素</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="3-其他常用操作">3. <strong>其他常用操作</strong></h3><ul><li><p><strong>交换内容</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::vector&lt;<span class="type">int</span>&gt; v2 = &#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>&#125;;</span><br><span class="line">v.<span class="built_in">swap</span>(v2);  <span class="comment">// 交换 v 和 v2 中的元素</span></span><br></pre></td></tr></table></figure></li><li><p><strong>重新调整大小</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">v.<span class="built_in">resize</span>(<span class="number">15</span>);  <span class="comment">// 调整大小为 15，新增的元素默认初始化为 0</span></span><br><span class="line">v.<span class="built_in">resize</span>(<span class="number">10</span>, <span class="number">100</span>);  <span class="comment">// 调整大小为 10，新增的元素初始化为 100</span></span><br></pre></td></tr></table></figure></li><li><p><strong>保留空间</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">v.<span class="built_in">reserve</span>(<span class="number">20</span>);  <span class="comment">// 为 v 保留足够的空间，以避免多次内存重分配</span></span><br></pre></td></tr></table></figure></li><li><p><strong>返回存储容量</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">size_t</span> capacity = v.<span class="built_in">capacity</span>();  <span class="comment">// 获取当前已分配的内存空间容量</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="4-迭代器操作">4. <strong>迭代器操作</strong></h3><ul><li><p><strong>遍历</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> it = v.<span class="built_in">begin</span>(); it != v.<span class="built_in">end</span>(); ++it) &#123;</span><br><span class="line">    std::cout &lt;&lt; *it &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 或者使用范围 for 循环</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span>&amp; elem : v) &#123;</span><br><span class="line">    std::cout &lt;&lt; elem &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>逆序遍历</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> it = v.<span class="built_in">rbegin</span>(); it != v.<span class="built_in">rend</span>(); ++it) &#123;</span><br><span class="line">    std::cout &lt;&lt; *it &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="5-排序">5. <strong>排序</strong></h3><ul><li><p><strong>使用标准库排序</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::<span class="built_in">sort</span>(v.<span class="built_in">begin</span>(), v.<span class="built_in">end</span>());  <span class="comment">// 升序排序</span></span><br><span class="line">std::<span class="built_in">sort</span>(v.<span class="built_in">begin</span>(), v.<span class="built_in">end</span>(), std::<span class="built_in">greater</span>&lt;<span class="type">int</span>&gt;());  <span class="comment">// 降序排序</span></span><br></pre></td></tr></table></figure></li><li><p><strong>自定义排序</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::<span class="built_in">sort</span>(v.<span class="built_in">begin</span>(), v.<span class="built_in">end</span>(), [](<span class="type">int</span> a, <span class="type">int</span> b) &#123;</span><br><span class="line">    <span class="keyword">return</span> a &gt; b;  <span class="comment">// 自定义降序</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></li></ul><h3 id="6-其它有用操作">6. <strong>其它有用操作</strong></h3><ul><li><p><strong>查找元素</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> it = std::<span class="built_in">find</span>(v.<span class="built_in">begin</span>(), v.<span class="built_in">end</span>(), <span class="number">10</span>);  <span class="comment">// 查找元素 10 是否在 vector 中</span></span><br><span class="line"><span class="keyword">if</span> (it != v.<span class="built_in">end</span>()) &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Found 10 at index: &quot;</span> &lt;&lt; std::<span class="built_in">distance</span>(v.<span class="built_in">begin</span>(), it) &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>合并两个 vector</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::vector&lt;<span class="type">int</span>&gt; v1 = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;</span><br><span class="line">std::vector&lt;<span class="type">int</span>&gt; v2 = &#123;<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>&#125;;</span><br><span class="line">v<span class="number">1.</span><span class="built_in">insert</span>(v<span class="number">1.</span><span class="built_in">end</span>(), v<span class="number">2.</span><span class="built_in">begin</span>(), v<span class="number">2.</span><span class="built_in">end</span>());  <span class="comment">// 将 v2 的元素添加到 v1 的末尾</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="for-each">for_each</h2><p><code>std::for_each</code> 是一个 <strong>遍历</strong> 容器元素的算法，它对容器中的每一个元素应用给定的操作（通常是一个函数或 lambda 表达式）。它不会返回任何值，主要用来执行副作用操作。</p><p>其实也不是绝对的每一个元素，具体是从哪个元素开始到哪个元素结束<strong>取决于传进去的迭代器</strong>（不过一般都是传<code>begin()</code>和<code>end()</code>）</p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">string s = <span class="string">&quot;abcdefg&quot;</span>;</span><br><span class="line">for_each(s.<span class="built_in">begin</span>(), s.<span class="built_in">end</span>(), [](<span class="keyword">auto</span>&amp; a) &#123; <span class="comment">// 必须传引用不然改不了</span></span><br><span class="line">a = <span class="built_in">toupper</span>(a); <span class="comment">// 所有元素转大写 </span></span><br><span class="line">&#125;);</span><br><span class="line">cout &lt;&lt; s &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 把从包括第三位之后的字符转小写</span></span><br><span class="line">for_each(s.<span class="built_in">begin</span>() + <span class="number">2</span>, s.<span class="built_in">end</span>(), [](<span class="keyword">auto</span>&amp; a) &#123;</span><br><span class="line">a = <span class="built_in">tolower</span>(a); </span><br><span class="line">&#125;);</span><br><span class="line">cout &lt;&lt; s &lt;&lt; endl;</span><br></pre></td></tr></table></figure><h2 id="all-of">all_of</h2><p><code>std::all_of</code> 是一个 <strong>条件检查</strong> 算法，用于检查容器中的所有元素是否满足某个条件。如果所有元素都满足条件，它返回 <code>true</code>，否则返回 <code>false</code>。</p><p>同理也不一定是所有元素，取决于传进去的迭代器</p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">31</span>, <span class="number">21</span>&#125;;</span><br><span class="line"><span class="comment">// 判断元素是否都大于10</span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">all_of</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span> a) &#123;</span><br><span class="line">    <span class="keyword">return</span> a &gt; <span class="number">10</span>;</span><br><span class="line">&#125;)) &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;true&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;false&quot;</span> &lt;&lt; endl; <span class="comment">// output this</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 判断后三个元素是否大于10</span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">all_of</span>(arr.<span class="built_in">end</span>() - <span class="number">3</span>, arr.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span> a) &#123;</span><br><span class="line">    <span class="keyword">return</span> a &gt; <span class="number">10</span>;</span><br><span class="line">&#125;)) &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;true&quot;</span> &lt;&lt; endl; <span class="comment">// output this</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;false&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="any-of">any_of</h2><p><code>std::any_of</code>: 检查容器中是否 <strong>至少有一个</strong> 元素满足某个条件</p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">-5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>&#125;;</span><br><span class="line"><span class="comment">// 判断容器中是否有负数</span></span><br><span class="line">cout &lt;&lt; <span class="built_in">any_of</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span> a) &#123;</span><br><span class="line">    <span class="keyword">return</span> a &lt; <span class="number">0</span>;</span><br><span class="line">&#125;) &lt;&lt; endl; <span class="comment">// 1</span></span><br></pre></td></tr></table></figure><h2 id="none-of">none_of</h2><p><code>std::none_of</code>: 检查容器中是否 <strong>没有一个</strong> 元素满足某个条件</p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">-5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>&#125;;</span><br><span class="line"><span class="comment">// 判断容器中是否没有有负数</span></span><br><span class="line">cout &lt;&lt; <span class="built_in">none_of</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span> a) &#123;</span><br><span class="line">    <span class="keyword">return</span> a &lt; <span class="number">0</span>;</span><br><span class="line">&#125;) &lt;&lt; endl; <span class="comment">// 0</span></span><br></pre></td></tr></table></figure><h2 id="find-if">find_if</h2><p><code>std::find_if</code>: 查找容器中 <strong>第一个满足条件</strong> 的元素</p><p>*注：*找到的一定是第一个满足条件的</p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">-5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">-8</span>&#125;;</span><br><span class="line"><span class="comment">// 找第一个负数</span></span><br><span class="line"><span class="keyword">auto</span> it = <span class="built_in">find_if</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span> a) &#123;</span><br><span class="line"><span class="keyword">return</span> a &lt; <span class="number">0</span>; </span><br><span class="line">&#125;);</span><br><span class="line"><span class="keyword">if</span> (it != arr.<span class="built_in">end</span>()) &#123;</span><br><span class="line">    cout &lt;&lt; *it &lt;&lt; endl; <span class="comment">// -5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="count-if">count_if</h2><p><code>std::count_if</code>: 统计容器中 <strong>满足条件的元素数量</strong></p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">-5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">-8</span>&#125;;</span><br><span class="line"><span class="comment">// 统计所有负数的数量</span></span><br><span class="line">cout &lt;&lt; <span class="built_in">count_if</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span> a) &#123;</span><br><span class="line"><span class="keyword">return</span> a &lt; <span class="number">0</span>; </span><br><span class="line">&#125;) &lt;&lt; endl; <span class="comment">// 2</span></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++字符串操作</title>
      <link href="/ymhui.github.io/2024/12/25/C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C/"/>
      <url>/ymhui.github.io/2024/12/25/C-%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h1>C++字符串操作</h1><h2 id="去除两端空白符">去除两端空白符</h2><p>利用正则表达式匹配替换</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">string line = <span class="string">&quot; hello, world!\t\t  &quot;</span>;</span><br><span class="line">line = <span class="built_in">regex_replace</span>(line, <span class="built_in">regex</span>(<span class="string">&quot;^\\s+|\\s+$&quot;</span>), <span class="string">&quot;&quot;</span>); <span class="comment">// 匹配line两端的空白符，并替换为空串</span></span><br></pre></td></tr></table></figure><h2 id="按指定字符分割字符串">按指定字符分割字符串</h2><p>利用<code>std::getline</code>，其<strong>默认以换行符分割</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">vector&lt;string&gt; <span class="title">getTokens</span><span class="params">(<span class="type">const</span> string&amp; line, <span class="type">const</span> <span class="type">char</span> delimiter)</span> </span>&#123;</span><br><span class="line">    <span class="function">istringstream <span class="title">iss</span><span class="params">(line)</span></span>;</span><br><span class="line">    vector&lt;string&gt; tokens;</span><br><span class="line">    string token;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">getline</span>(iss, token, delimiter)) &#123;</span><br><span class="line">        tokens.<span class="built_in">push_back</span>(token);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> tokens;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">string line = <span class="string">&quot;abcd-5555-pppp&quot;</span>;</span><br><span class="line"><span class="built_in">getTokens</span>(line, <span class="string">&#x27;-&#x27;</span>);</span><br></pre></td></tr></table></figure><p><em>注：</em><code>istringstream</code>以空白符分割字符串来构造流且<strong>不可改</strong>，可以利用<code>getline</code>为其多传入一个分隔符参数来从流中按指定分隔符读入字符串，以达到分割字符串的目的；如果就是准备以单个空格分割字符串，也可以直接用<code>iss &gt;&gt; token</code>代替<code>getline(iss, token, ' ')</code>，注意<code>getline</code>的参数<code>' '</code>不可省，因为其<strong>默认以换行符分割</strong>。</p><h2 id="string的几种构造方式">string的几种构造方式</h2><ol><li>直接声明构造空串</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">string s; <span class="comment">// &quot;&quot;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>按字面量初始化</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">string s1 = <span class="string">&quot;cpp&quot;</span>;</span><br><span class="line"><span class="function">string <span class="title">s2</span><span class="params">(<span class="string">&quot;cpp&quot;</span>)</span></span>; <span class="comment">// 两种的效果是一样的</span></span><br></pre></td></tr></table></figure><ol start="3"><li>重复用特定字符构造</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">string <span class="title">s3</span><span class="params">(<span class="number">5</span>, <span class="string">&#x27;a&#x27;</span>)</span></span>; <span class="comment">// &quot;aaaaa&quot;</span></span><br><span class="line"><span class="function">string <span class="title">s4</span><span class="params">(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>)</span></span>; <span class="comment">// &quot;a&quot;, 可以视作字符-&gt;字符串</span></span><br></pre></td></tr></table></figure><ol start="4"><li>获得子串</li></ol><p><code>string::substr</code>方法，第一个参数为字串起始位置，第二个参数若缺省则取后面所有，否则代表子串的长度</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">string s5 = <span class="string">&quot;hello,cpp&quot;</span>;</span><br><span class="line">string hello = s<span class="number">5.</span><span class="built_in">substr</span>(<span class="number">0</span>, <span class="number">5</span>); <span class="comment">// &quot;hello&quot;</span></span><br><span class="line">string cpp = s<span class="number">5.</span><span class="built_in">substr</span>(<span class="number">6</span>); <span class="comment">// &quot;cpp&quot;</span></span><br></pre></td></tr></table></figure><h2 id="拼接字符串">拼接字符串</h2><h3 id="">+</h3><p>不改变原串</p><h3 id="append">append</h3><p>改变原串</p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">string s1 = <span class="string">&quot;abc&quot;</span>, s2 = <span class="string">&quot;def&quot;</span>;</span><br><span class="line">string s3 = s1 + s2;</span><br><span class="line">cout &lt;&lt; s1 &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; s2 &lt;&lt; endl; <span class="comment">// abc def</span></span><br><span class="line">s<span class="number">1.</span><span class="built_in">append</span>(s2);</span><br><span class="line">cout &lt;&lt; s1 &lt;&lt; endl; <span class="comment">// abcdef</span></span><br></pre></td></tr></table></figure><h2 id="查找子串">查找子串</h2><p><code>string::find</code>可用于字符串中查找子串的位置</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::string str = <span class="string">&quot;Hello, world!&quot;</span>;</span><br><span class="line"><span class="type">size_t</span> pos = str.<span class="built_in">find</span>(<span class="string">&quot;world&quot;</span>);  <span class="comment">// 查找子串 &quot;world&quot; 在字符串中的位置</span></span><br><span class="line"><span class="keyword">if</span> (pos != std::string::npos) &#123; <span class="comment">// 用这个判断是否找到</span></span><br><span class="line">    std::cout &lt;&lt; pos &lt;&lt; std::endl; <span class="comment">// 7 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="查找字符">查找字符</h2><p>仍用<code>string::find</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::string str = <span class="string">&quot;Hello, world!&quot;</span>;</span><br><span class="line"><span class="type">size_t</span> pos = str.<span class="built_in">find</span>(<span class="string">&#x27;o&#x27;</span>);  <span class="comment">// 查找字符 &#x27;o&#x27; 在字符串中第一次出现的的位置</span></span><br></pre></td></tr></table></figure><h2 id="查找最后一个子串">查找最后一个子串</h2><p><code>string::rfind</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::string str = <span class="string">&quot;Hello, world!&quot;</span>;</span><br><span class="line"><span class="type">size_t</span> pos = str.<span class="built_in">rfind</span>(<span class="string">&quot;world&quot;</span>);  <span class="comment">// 查找最后一次出现 &quot;world&quot; 的位置</span></span><br></pre></td></tr></table></figure><h2 id="字符串替换">字符串替换</h2><p><code>string::replace</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::string str = <span class="string">&quot;Hello, world!&quot;</span>;</span><br><span class="line">str.<span class="built_in">replace</span>(<span class="number">7</span>, <span class="number">5</span>, <span class="string">&quot;C++&quot;</span>);  <span class="comment">// 从位置 7 开始，替换 5 个字符为 &quot;C++&quot;</span></span><br></pre></td></tr></table></figure><h2 id="正则替换">正则替换</h2><p>先用<code>std::regex</code>正则匹配，再用<code>std::regex_replace</code>替换</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::string str = <span class="string">&quot;Hello, world!&quot;</span>;</span><br><span class="line">std::string result = std::<span class="built_in">regex_replace</span>(str, std::<span class="built_in">regex</span>(<span class="string">&quot;world&quot;</span>), <span class="string">&quot;C++&quot;</span>); <span class="comment">// 将&quot;world&quot;替换为&quot;C++&quot;</span></span><br></pre></td></tr></table></figure><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 字符串 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++泛型</title>
      <link href="/ymhui.github.io/2024/12/24/C-%E6%B3%9B%E5%9E%8B/"/>
      <url>/ymhui.github.io/2024/12/24/C-%E6%B3%9B%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1>C++泛型相关知识</h1><p>参考南软2024秋C++高级程序设计课件generic1, generic2</p><h2 id="C-程序的组织">C++程序的组织</h2><h3 id="头文件-h">头文件.h</h3><p>头文件中一般放编译时常量<code>const</code>，各种声明，别名定义<code>typedef</code>，宏，内联函数，预处理等代码，并对需要向外扩展作用域的成员加上<code>extern</code>关键字</p><h3 id="源文件-cc-cpp"><a href="http://xn--5nqy36cimg.cc">源文件.cc</a> / .cpp</h3><p>对应的，源文件中就要<strong>负责实现只在对应头文件中声明而没有定义的内容</strong>，比如函数定义、类成员定义等</p><h3 id="作用域">作用域</h3><p>分为：</p><ul><li>程序级：比如<code>#include</code>的内容</li><li>文件级：比如全局变量、函数等</li><li>函数级：比如函数的最外层局部变量</li><li>块级：比如代码块的局部变量（<code>for</code>, <code>while</code>等）</li></ul><h2 id="namespace">namespace</h2><p>两种使用方式：</p><ul><li>声明式declaration</li><li>直接使用directive</li></ul><p>设有名空间<code>L</code>:</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> L &#123;</span><br><span class="line">    <span class="type">int</span> k;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="声明式">声明式</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> L::k;</span><br><span class="line"><span class="keyword">using</span> L::f;</span><br><span class="line"></span><br><span class="line">k = <span class="number">1</span>;</span><br><span class="line"><span class="built_in">f</span>(k);</span><br></pre></td></tr></table></figure><h3 id="直接使用">直接使用</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> L;</span><br><span class="line"></span><br><span class="line">k = <span class="number">1</span>;</span><br><span class="line"><span class="built_in">f</span>(k);</span><br></pre></td></tr></table></figure><p>在约束作用域方面，用<code>namespace</code>代替<code>static</code></p><h3 id="细节">细节</h3><ol><li>namespace可以用来给一个名空间起别名</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> alias_L = L;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> alias_L;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure><ol start="2"><li>namespace的作用域是影响全局的</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> L;</span><br><span class="line"></span><br><span class="line"><span class="comment">// L中的所有成员都将变成该文件下的全局变量</span></span><br></pre></td></tr></table></figure><ol start="3"><li>namespace是开放的，即可以多次定义同一个命名空间，内容将进行合并</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> L &#123;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f2</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="type">int</span> m;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> L::k;</span><br><span class="line"><span class="keyword">using</span> L::m;</span><br><span class="line"><span class="keyword">using</span> L::f2;</span><br><span class="line"><span class="comment">//...</span></span><br></pre></td></tr></table></figure><ol start="4"><li>可以嵌套</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> L1 &#123;</span><br><span class="line">    <span class="type">int</span> a;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">namespace</span> L2 &#123;</span><br><span class="line"><span class="type">int</span> b;</span><br><span class="line">        <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="5"><li>支持对其中的函数重载</li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> A &#123;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">char</span>)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">double</span>)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">using</span> A::f;</span><br><span class="line">    <span class="built_in">f</span>(<span class="string">&#x27;1&#x27;</span>);</span><br><span class="line">    <span class="comment">// f(); // f的名空间被namespace A中的f覆盖，而A中没有重载版本f(), 因此编译错误</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最常用的还是<code>using namespace std;</code></p><h2 id="编译预处理">编译预处理</h2><p>编译预处理既便捷又危险，它与C++的作用域、类型、接口等概念格格不入，潜伏于环境中，穿透作用域</p><p>但其应用方式的确丰富，很难为其找到具有更好的结构且高效的替代品</p><h3 id="include">#include</h3><ul><li>使得接口定义成为可能</li><li>组织源代码文件</li></ul><h3 id="define">#define</h3><ul><li>定义符号常量</li><li>定义类型</li><li>泛型编程</li><li>字符串拼接</li><li>…</li></ul><p>宏定义是非常有用的</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> CONCAT(x, y) x##y <span class="comment">// 拼接字符串x和y，直接按传入的字面文本x, y进行拼接</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> TO_STRING(x) #x <span class="comment">// 将x直接转为字符串，按传入的x的字面文本</span></span></span><br><span class="line"></span><br><span class="line">cout &lt;&lt; <span class="built_in">CONCAT</span>(abc, def); <span class="comment">// abcdef</span></span><br><span class="line">cout &lt;&lt; <span class="built_in">TO_STRING</span>(<span class="number">1</span>); <span class="comment">// 1</span></span><br><span class="line">cout &lt;&lt; <span class="built_in">TO_STRING</span>(xxx); <span class="comment">// xxx</span></span><br></pre></td></tr></table></figure><h3 id="ifdef">#ifdef</h3><ul><li>版本控制</li><li>避免循环include</li></ul><h3 id="pragma">#pragma</h3><ul><li>通知编译器添加一些编译参数等</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> Optimize (<span class="string">&quot;Ofast&quot;</span>)</span></span><br></pre></td></tr></table></figure><h2 id="宏">宏</h2><p>更优雅的宏定义示例，可以简化重复但不好抽象的代码块：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ARRAY_SIZE(arr) (sizeof(arr) / sizeof(arr[0]))</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FOR_EACH(arr, fp) for (int i = 0; i &lt; ARRAY_SIZE(arr); i++) &#123;\</span></span><br><span class="line"><span class="meta">(fp)(arr[i]);\</span></span><br><span class="line"><span class="meta">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> arr[<span class="number">12</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>&#125;;</span><br><span class="line">    <span class="built_in">FOR_EACH</span>(arr, [](<span class="type">const</span> <span class="keyword">auto</span> a) &#123;</span><br><span class="line">        cout &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="泛型编程">泛型编程</h2><h3 id="C如何实现泛型编程">C如何实现泛型编程</h3><ul><li>宏</li></ul><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义通用的栈结构和操作</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DEFINE_STACK_TYPE(TYPE)                                         \</span></span><br><span class="line"><span class="meta">    typedef struct &#123;                                                   \</span></span><br><span class="line"><span class="meta">        TYPE *data;                                                    \</span></span><br><span class="line"><span class="meta">        size_t capacity;                                               \</span></span><br><span class="line"><span class="meta">        size_t size;                                                   \</span></span><br><span class="line"><span class="meta">    &#125; Stack_##TYPE;                                                    \</span></span><br><span class="line"><span class="meta">                                                                        \</span></span><br><span class="line"><span class="meta">    void Stack_##TYPE##_init(Stack_##TYPE *stack) &#123;                    \</span></span><br><span class="line"><span class="meta">        stack-&gt;capacity = 4;                                           \</span></span><br><span class="line"><span class="meta">        stack-&gt;size = 0;                                               \</span></span><br><span class="line"><span class="meta">        stack-&gt;data = (TYPE *)malloc(stack-&gt;capacity * sizeof(TYPE));  \</span></span><br><span class="line"><span class="meta">    &#125;                                                                  \</span></span><br><span class="line"><span class="meta">                                                                        \</span></span><br><span class="line"><span class="meta">    void Stack_##TYPE##_push(Stack_##TYPE *stack, TYPE value) &#123;        \</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (stack-&gt;size == stack-&gt;capacity) &#123;                          \</span></span><br><span class="line"><span class="meta">            stack-&gt;capacity *= 2;                                      \</span></span><br><span class="line"><span class="meta">            stack-&gt;data = (TYPE *)realloc(stack-&gt;data, stack-&gt;capacity * sizeof(TYPE)); \</span></span><br><span class="line"><span class="meta">        &#125;                                                              \</span></span><br><span class="line"><span class="meta">        stack-&gt;data[stack-&gt;size++] = value;                            \</span></span><br><span class="line"><span class="meta">    &#125;                                                                  \</span></span><br><span class="line"><span class="meta">                                                                        \</span></span><br><span class="line"><span class="meta">    TYPE Stack_##TYPE##_pop(Stack_##TYPE *stack) &#123;                     \</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (stack-&gt;size == 0) &#123;                                        \</span></span><br><span class="line"><span class="meta">            fprintf(stderr, <span class="string">&quot;Stack underflow!\n&quot;</span>);                     \</span></span><br><span class="line"><span class="meta">            exit(EXIT_FAILURE);                                        \</span></span><br><span class="line"><span class="meta">        &#125;                                                              \</span></span><br><span class="line"><span class="meta">        return stack-&gt;data[--stack-&gt;size];                             \</span></span><br><span class="line"><span class="meta">    &#125;                                                                  \</span></span><br><span class="line"><span class="meta">                                                                        \</span></span><br><span class="line"><span class="meta">    TYPE Stack_##TYPE##_top(Stack_##TYPE *stack) &#123;                     \</span></span><br><span class="line"><span class="meta">        <span class="keyword">if</span> (stack-&gt;size == 0) &#123;                                        \</span></span><br><span class="line"><span class="meta">            fprintf(stderr, <span class="string">&quot;Stack is empty!\n&quot;</span>);                      \</span></span><br><span class="line"><span class="meta">            exit(EXIT_FAILURE);                                        \</span></span><br><span class="line"><span class="meta">        &#125;                                                              \</span></span><br><span class="line"><span class="meta">        return stack-&gt;data[stack-&gt;size - 1];                           \</span></span><br><span class="line"><span class="meta">    &#125;                                                                  \</span></span><br><span class="line"><span class="meta">                                                                        \</span></span><br><span class="line"><span class="meta">    void Stack_##TYPE##_destroy(Stack_##TYPE *stack) &#123;                 \</span></span><br><span class="line"><span class="meta">        free(stack-&gt;data);                                             \</span></span><br><span class="line"><span class="meta">        stack-&gt;data = NULL;                                            \</span></span><br><span class="line"><span class="meta">        stack-&gt;capacity = 0;                                           \</span></span><br><span class="line"><span class="meta">        stack-&gt;size = 0;                                               \</span></span><br><span class="line"><span class="meta">    &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义具体的类型栈</span></span><br><span class="line">DEFINE_STACK_TYPE(<span class="type">int</span>)</span><br><span class="line">DEFINE_STACK_TYPE(<span class="type">double</span>)</span><br><span class="line">    </span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 整型栈</span></span><br><span class="line">    Stack_int int_stack;</span><br><span class="line">    Stack_int_init(&amp;int_stack);</span><br><span class="line"></span><br><span class="line">    Stack_int_push(&amp;int_stack, <span class="number">10</span>);</span><br><span class="line">    Stack_int_push(&amp;int_stack, <span class="number">20</span>);</span><br><span class="line">    Stack_int_push(&amp;int_stack, <span class="number">30</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Top of int stack: %d\n&quot;</span>, Stack_int_top(&amp;int_stack));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (int_stack.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Popped from int stack: %d\n&quot;</span>, Stack_int_pop(&amp;int_stack));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Stack_int_destroy(&amp;int_stack);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 浮点型栈</span></span><br><span class="line">    Stack_double double_stack;</span><br><span class="line">    Stack_double_init(&amp;double_stack);</span><br><span class="line"></span><br><span class="line">    Stack_double_push(&amp;double_stack, <span class="number">1.1</span>);</span><br><span class="line">    Stack_double_push(&amp;double_stack, <span class="number">2.2</span>);</span><br><span class="line">    Stack_double_push(&amp;double_stack, <span class="number">3.3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Top of double stack: %.1f\n&quot;</span>, Stack_double_top(&amp;double_stack));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (double_stack.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Popped from double stack: %.1f\n&quot;</span>, Stack_double_pop(&amp;double_stack));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Stack_double_destroy(&amp;double_stack);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Top of int stack: 30</span><br><span class="line">Popped from int stack: 30</span><br><span class="line">Popped from int stack: 20</span><br><span class="line">Popped from int stack: 10</span><br></pre></td></tr></table></figure><p><strong>缺点：</strong></p><ul><li>代码<strong>几乎没有可读性</strong></li><li>难以调试</li><li>需显式写出类型参数</li><li>需要手动实例化</li></ul><h3 id="C-解决方案">C++解决方案</h3><h4 id="template">template</h4><p><em>模板</em> <code>template</code></p><p>用法：在需要使用模板的地方最前面加上<code>template &lt;typename T&gt;</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdexcept&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Stack</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::vector&lt;T&gt; data; <span class="comment">// 使用 std::vector 动态存储数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 检查栈是否为空</span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">empty</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.<span class="built_in">empty</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取栈的大小</span></span><br><span class="line">    <span class="function"><span class="type">size_t</span> <span class="title">size</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.<span class="built_in">size</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 压栈操作</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">push</span><span class="params">(<span class="type">const</span> T&amp; value)</span> </span>&#123;</span><br><span class="line">        data.<span class="built_in">push_back</span>(value);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 出栈操作</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> std::<span class="built_in">underflow_error</span>(<span class="string">&quot;Stack underflow: cannot pop from an empty stack&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        data.<span class="built_in">pop_back</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取栈顶元素</span></span><br><span class="line">    <span class="function">T <span class="title">top</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> std::<span class="built_in">underflow_error</span>(<span class="string">&quot;Stack underflow: stack is empty&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> data.<span class="built_in">back</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 整型栈</span></span><br><span class="line">        Stack&lt;<span class="type">int</span>&gt; intStack;</span><br><span class="line">        intStack.<span class="built_in">push</span>(<span class="number">10</span>);</span><br><span class="line">        intStack.<span class="built_in">push</span>(<span class="number">20</span>);</span><br><span class="line">        intStack.<span class="built_in">push</span>(<span class="number">30</span>);</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Top of int stack: &quot;</span> &lt;&lt; intStack.<span class="built_in">top</span>() &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">        <span class="keyword">while</span> (!intStack.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Popped from int stack: &quot;</span> &lt;&lt; intStack.<span class="built_in">top</span>() &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">            intStack.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 浮点型栈</span></span><br><span class="line">        Stack&lt;<span class="type">double</span>&gt; doubleStack;</span><br><span class="line">        doubleStack.<span class="built_in">push</span>(<span class="number">1.1</span>);</span><br><span class="line">        doubleStack.<span class="built_in">push</span>(<span class="number">2.2</span>);</span><br><span class="line">        doubleStack.<span class="built_in">push</span>(<span class="number">3.3</span>);</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Top of double stack: &quot;</span> &lt;&lt; doubleStack.<span class="built_in">top</span>() &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">        <span class="keyword">while</span> (!doubleStack.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Popped from double stack: &quot;</span> &lt;&lt; doubleStack.<span class="built_in">top</span>() &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">            doubleStack.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 字符串栈</span></span><br><span class="line">        Stack&lt;std::string&gt; stringStack;</span><br><span class="line">        stringStack.<span class="built_in">push</span>(<span class="string">&quot;Hello&quot;</span>);</span><br><span class="line">        stringStack.<span class="built_in">push</span>(<span class="string">&quot;World&quot;</span>);</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Top of string stack: &quot;</span> &lt;&lt; stringStack.<span class="built_in">top</span>() &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">        <span class="keyword">while</span> (!stringStack.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Popped from string stack: &quot;</span> &lt;&lt; stringStack.<span class="built_in">top</span>() &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">            stringStack.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="built_in">catch</span> (<span class="type">const</span> std::exception&amp; e) &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;Error: &quot;</span> &lt;&lt; e.<span class="built_in">what</span>() &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Top of int stack: 30</span><br><span class="line">Popped from int stack: 30</span><br><span class="line">Popped from int stack: 20</span><br><span class="line">Popped from int stack: 10</span><br><span class="line">Top of double stack: 3.3</span><br><span class="line">Popped from double stack: 3.3</span><br><span class="line">Popped from double stack: 2.2</span><br><span class="line">Popped from double stack: 1.1</span><br><span class="line">Top of string stack: World</span><br><span class="line">Popped from string stack: World</span><br><span class="line">Popped from string stack: Hello</span><br></pre></td></tr></table></figure><h4 id="编译器是如何处理模板的">编译器是如何处理模板的</h4><p>示例</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> U&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(T, U)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">foo</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> U&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(T t, U u)</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure><p><code>template&lt;typename T, typename U&gt; foo(T, U);</code> =&gt; <code>template&lt;typename int, typename U&gt; foo(int, U); </code>=&gt; <code>template&lt;typename int, typename int&gt; foo(int, int); </code>=&gt; finish!</p><h3 id="concept">concept</h3><p>C++20引入了一个新的关键字<code>concept</code>，用于定义模板的 <strong>约束条件</strong>，可以明确限定模板必须满足的参数要求，从而确保类型安全，提高程序的可靠性与稳定性。</p><p>示例</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义一个简单的 concept，用来约束只有可加的类型才能传入模板</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">concept</span> Addable = <span class="built_in">requires</span>(T a, T b) &#123;</span><br><span class="line">    &#123; a + b &#125; -&gt; std::same_as&lt;T&gt;;  <span class="comment">// T 类型必须支持 a + b 操作，且结果类型是 T</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 concept 作为模板的约束条件</span></span><br><span class="line"><span class="keyword">template</span> &lt;Addable T&gt;</span><br><span class="line"><span class="function">T <span class="title">add</span><span class="params">(T a, T b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">add</span>(<span class="number">1</span>, <span class="number">2</span>) &lt;&lt; std::endl;  <span class="comment">// 输出：3</span></span><br><span class="line">    <span class="comment">// std::cout &lt;&lt; add(&quot;Hello&quot;, &quot;World&quot;) &lt;&lt; std::endl;  // 编译错误：不支持字符串的加法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>*注：*C++20的concept特性是默认禁用的，必须在编译时显式加上<code>-fconcepts</code>参数才能编译成功</p><p>示例</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ g++ -std=c++20 -fconcepts main.cpp -o main</span><br></pre></td></tr></table></figure><h2 id="元编程">元编程</h2><p>meta programing</p><p>指的是编写能够操作、生成或修改其他程序（或自身）代码的程序。简而言之，元编程是编写能够处理代码的代码。其核心概念是，程序在更高的抽象层次上工作，关注代码的结构和行为，而不仅仅是直接的逻辑</p><h3 id="关键点">关键点</h3><ul><li><p><strong>代码生成</strong>：元程序可以根据某些输入或条件生成新的代码。</p></li><li><p><strong>反射</strong>：反射是程序在运行时检查和修改自身结构或行为的能力。例如，程序可以在运行时检查自身的函数、方法或变量。</p></li><li><p><strong>宏</strong>：宏是一组在编译前生成代码的指令，用于自动化重复的任务。比如 C/C++ 中的宏允许以在编译前评估的方式编写代码。</p></li></ul><p>C++中可以利用的特性：宏（定义宏来替代重复的代码，每次根据不同的传入参数动态生成类似的代码），模板，<code>constexpr</code>，面向对象编程等</p><h2 id="Lambda表达式">Lambda表达式</h2><h3 id="语法">语法</h3><p><code>[captureList](paramList) specifiers exception -&gt; returnType &#123;body&#125;</code>其中除了<code>[]</code>, <code>()</code>, <code>&#123;&#125;</code>都是可省的</p><ul><li>captureList用于指定Lambda表达式内部如何访问其外部作用域中的变量，捕获方式有值(<code>=</code>)捕获、引用(<code>&amp;</code>)捕获和混合捕获</li><li>paramList定义Lambda表达式的参数</li><li>specifiers用于指定Lambda表达式的属性，如<code>mutable</code>(可以修改捕获的变量)</li><li>exception指定是否抛出异常，若抛出抛什么</li><li>returnType说明返回值类型</li><li>body定义函数逻辑</li></ul><p>示例</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">2</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">4</span>&#125;;</span><br><span class="line"><span class="built_in">sort</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span> a, <span class="type">const</span> <span class="keyword">auto</span> b) &#123;</span><br><span class="line"><span class="keyword">return</span> a &lt; b; <span class="comment">// 升序 </span></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> k = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">auto</span> it = <span class="built_in">upper_bound</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), k); <span class="comment">// 默认升序有序</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">sort</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span> a, <span class="type">const</span> <span class="keyword">auto</span> b) &#123;</span><br><span class="line"><span class="keyword">return</span> a &gt; b; <span class="comment">// 降序 </span></span><br><span class="line">&#125;);</span><br><span class="line">it = <span class="built_in">upper_bound</span>(arr.<span class="built_in">begin</span>(). arr.<span class="built_in">end</span>(), k, [&amp;](<span class="type">const</span> <span class="keyword">auto</span> a, <span class="type">const</span> <span class="keyword">auto</span> b) &#123; <span class="comment">// &amp; 捕获当前作用域的所有变量</span></span><br><span class="line">    <span class="keyword">return</span> a &gt; b; <span class="comment">// 指定有序是降序的</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="functional">&lt;functional&gt;</h2><p><code>&lt;functional&gt;</code>库详见 <em>C++面向对象编程进阶II</em></p><h2 id="十个问题，为面向对象编程抛砖引玉">十个问题，为面向对象编程抛砖引玉</h2><ol><li>当类中未自定义构造函数，编译器是否会提供默认构造函数，为什么？</li><li>什么时候将构造函数、析构函数定义为<code>private</code>？什么时候使用友元、<code>static</code>成员？</li><li>为什么引入成员初始化表？为什么初始化表的执行次序只与类数据成员的定义次序相关</li><li>为什么引入拷贝构造函数、移动构造函数、<code>=</code>操作符重载？</li><li>为什么需要后期（动态）绑定？<code>C++</code>如何实现<code>virtual</code>？</li><li>什么时候使用<code>virtual</code>？</li><li>public继承和非public继承分别意味着什么？</li><li>为什么<code>=</code>, <code>()</code>, <code>[]</code>, <code>-&gt;</code>不能全局重载</li><li>什么时候成员函数能返回<code>&amp;</code>？</li></ol><p>大多数情况下都可以返回<code>&amp;</code>，除去只读的情况（事实上只读可以返回一个常量引用），但以下情况需要注意：</p><ul><li><strong>不要返回局部变量的引用</strong>，因为它们的生命周期在函数返回后结束。</li><li><strong>不要返回临时对象的非 <code>const</code> 引用</strong>，因为它们的生命周期非常短。</li><li><strong>返回动态分配的对象的引用</strong> 需要非常小心，确保管理好内存的释放。</li><li><strong>返回静态成员变量的引用</strong> 是安全的，但返回非静态成员变量的引用时要确保该对象仍然存在。</li></ul><ol start="10"><li>什么时候重载<code>new</code>和<code>delete</code>？怎么重载？</li></ol>]]></content>
      
      
      <categories>
          
          <category> 软件学院 </category>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件学院 </tag>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++函数</title>
      <link href="/ymhui.github.io/2024/12/23/C-%E5%87%BD%E6%95%B0/"/>
      <url>/ymhui.github.io/2024/12/23/C-%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1>C++函数基础知识</h1><h2 id="为什么函数参数需要指针">为什么函数参数需要指针</h2><ul><li>提高传输效率：直接传地址而<strong>不需要进行原类型下的值拷贝</strong>，如果原类型很大，这将是很耗时的；而地址的拷贝总是整数</li><li>利用函数的副作用：可能需要改变原内存中的数据，利用指针才能修改</li><li>如果确定不想修改地址中的值，可以传指针常量</li></ul><h2 id="函数指针">函数指针</h2><p>指向函数的指针</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">f</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">g</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 声明函数指针</span></span><br><span class="line">    <span class="built_in">double</span> (*fp)(<span class="type">int</span>) = &amp;f; <span class="comment">// 解释：声明一个函数指针，返回值是double，带一个参数int</span></span><br><span class="line">    <span class="built_in">int</span> (*gp)() = &amp;g; <span class="comment">// 声明一个函数指针，返回值是int，不需要参数；*gp外面的()用来改变优先级，后面的()表示在描述参数，两者配合告诉编译器在声明函数指针</span></span><br><span class="line">    cout &lt;&lt; (*fp)(<span class="number">1</span>) &lt;&lt; endl; <span class="comment">// 2.2</span></span><br><span class="line">    cout &lt;&lt; (*gp)() &lt;&lt; endl; <span class="comment">// 0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">f</span><span class="params">(<span class="type">int</span> a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + <span class="number">1.2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">g</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="运行时环境">运行时环境</h2><img src="/ymhui.github.io/2024/12/23/C-%E5%87%BD%E6%95%B0/runtime.jpg" class="" title="runtime"><h2 id="函数声明原则（函数原型）">函数声明原则（函数原型）</h2><ul><li>先声明后使用</li><li>声明时可以不给出定义，定义的位置随意（但要在声明后），如果不在同一文件需要引入或扩展声明告诉编译器声明的位置</li><li>属于statement</li><li>声明参数时<strong>可以只声明类型而不写形参</strong></li><li>编译器负责检查定义时的形参类型和声明是否一致</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">double</span> <span class="title">f</span><span class="params">(<span class="type">int</span>, <span class="type">int</span>, <span class="type">double</span>)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="函数定义原则">函数定义原则</h2><ul><li>定义不允许嵌套（不像python可以def里面套def）</li><li><strong>先声明</strong>后使用</li><li>声明之后要给出定义，可以立刻给出，也可以之后给出</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">double</span> <span class="title">f</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b, <span class="type">double</span> c)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a * b * c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="函数的执行机制">函数的执行机制</h2><ol><li>建立被调用函数的栈空间</li><li>参数传递<ul><li>值传递</li><li>or 引用传递</li></ul></li><li>保存调用函数的运行状态</li><li>将控制权转交被调函数</li></ol><h2 id="函数调用">函数调用</h2><p><code>main</code>函数调用其他函数，函数调用在内存中以<strong>栈帧</strong>的形式存在</p><p>函数栈帧的基地址在寄存器<code>ebp</code>中，栈顶指针（即栈中当前活动的内存位置）在寄存器<code>esp</code>中</p><h3 id="栈帧的结构：">栈帧的结构：</h3><ul><li><strong>函数调用前</strong>：<code>EBP</code> 保存的是调用者的栈帧基地址。</li><li><strong>函数调用时</strong>：新函数的 <code>EBP</code> 被保存到栈中，然后 <code>EBP</code> 被更新为当前函数的栈帧基地址。</li><li><strong>函数返回时</strong>：栈恢复到调用函数的状态，<code>EBP</code> 恢复为调用者的栈帧基地址。</li></ul><h3 id="栈操作：">栈操作：</h3><ul><li><strong>函数调用时</strong>：<code>ESP</code> 会减小，因为函数的返回地址和参数会被推入栈中。</li><li><strong>函数返回时</strong>：<code>ESP</code> 会增大，栈空间会被清理，恢复到调用函数的状态。</li></ul><h2 id="参数传递">参数传递</h2><ul><li>传值</li><li>传引用</li></ul><h3 id="传值">传值</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> r = <span class="built_in">add</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="传引用">传引用</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span>&amp; a, <span class="type">int</span>&amp; b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> a = <span class="number">1</span>, b = <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> r = <span class="built_in">add</span>(a, b); <span class="comment">// 传引用必须要传一个左值进去</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="调用约定">调用约定</h2><ul><li>__cdecl</li><li>__stdcall</li><li>__fastcall</li><li>__thiscal</li></ul><p><code>__cdecl</code>、<code>__stdcall</code>、<code>__fastcall</code> 和 <code>__thiscall</code> 都是 <strong>调用约定</strong>（calling conventions）的一部分，定义了函数调用时，如何传递参数、返回值，以及函数如何清理栈等细节。不同的调用约定通常影响程序的性能和兼容性，并且在不同的平台和编译器之间可能会有所不同。</p><table><thead><tr><th>特性</th><th><code>__cdecl</code></th><th><code>__stdcall</code></th><th><code>__fastcall</code></th><th><code>__thiscall</code></th></tr></thead><tbody><tr><td><strong>传递参数的顺序</strong></td><td>从右到左</td><td>从右到左</td><td>前两个通过寄存器，剩余通过栈</td><td><code>this</code> 通过 ECX，其他参数通过栈</td></tr><tr><td><strong>堆栈清理</strong></td><td>调用者负责清理</td><td>被调用者负责清理</td><td>调用者负责清理</td><td>调用者负责清理</td></tr><tr><td><strong>返回值传递</strong></td><td>通过 EAX</td><td>通过 EAX</td><td>通过 EAX</td><td>通过 EAX</td></tr><tr><td><strong>常见应用</strong></td><td>C/C++ 中常用函数</td><td>Windows API 函数</td><td>高性能函数调用，少量参数的优化</td><td>C++ 成员函数（默认）</td></tr></tbody></table><h2 id="其它少见的传参方式（应该不重要）">其它少见的传参方式（应该不重要）</h2><ul><li>按名称传递 call by name</li><li>按值结果传递 call by value-result</li></ul><h3 id="按名称传递">按名称传递</h3><p><code>Call by Name</code> 是一个较为少见的参数传递方式，最初由 <code>Algol 60</code> 等编程语言引入。它的特点是：</p><ul><li><strong>参数传递</strong>：函数不会直接传递参数的值，而是将<strong>表达式</strong>（即函数参数）作为一个代码片段传递给函数。每次在函数体内使用该参数时，都会<strong>重新计算</strong>该参数的值。</li><li><strong>计算延迟</strong>：当你使用这个参数时，它将被<strong>重新求值</strong>，即使函数在多次调用中使用了相同的参数。</li></ul><p><code>C++</code>并不支持按名称传递，想要使用就需要利用宏</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> CALL_BY_NAME(x) (x * x) <span class="comment">// 传递一个表达式</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">    std::cout &lt;&lt; x &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> a = <span class="number">3</span>;</span><br><span class="line">    <span class="built_in">func</span>(<span class="built_in">CALL_BY_NAME</span>(a)); <span class="comment">// 相当于 func(a * a)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="按值结果传递">按值结果传递</h3><p><code>Call by Value-Result</code>（也称为 <strong>Call by Copy-Return</strong>）是一个更少见的传递方式，<em><strong>通常不直接出现在 C++ 中</strong></em>。它结合了 <code>Call by Value</code> 和 <code>Call by Reference</code> 的特点：</p><ul><li><strong>传递方式</strong>：在函数调用开始时，实参的值会被复制到函数的形参中；然后，函数执行时，形参会根据需要修改，但是在函数结束时，形参的值会复制回实参。</li><li><strong>特点</strong>：这种方式可以理解为，首先<strong>按值</strong>传递实参，然后<strong>按结果</strong>传递回实参，即返回值会被赋给调用者的变量。</li></ul><p><code>C++</code>模拟call by value-result：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">    x = x + <span class="number">1</span>; <span class="comment">// 这里修改了参数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> y = <span class="number">5</span>;</span><br><span class="line">    y = <span class="built_in">func</span>(y); <span class="comment">// 在函数返回时，y 的值会根据 x 的值更新</span></span><br><span class="line">    <span class="comment">// 在真正的按值结果传递中，直接func(y)后y的结果就被形参x更新了（有点类似传引用）</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="函数重载">函数重载</h2><p>也是一种多态的体现</p><h3 id="原则">原则</h3><ul><li>函数名相同但参数列表不同，参数列表不同表现为（个数、类型、顺序至少其一有区别）</li><li>切记不看返回值（因为返回值不是函数调用处必须使用的，函数调用时可能忽略返回值，那编译器就不知道该调哪个函数了）</li></ul><h3 id="匹配规则">匹配规则</h3><ul><li>严格匹配</li><li>若需要类型转换：<ul><li>优先内置转换</li><li>其次用户定义</li></ul></li></ul><p><em>详细的匹配规则见</em> <code>C++面向对象编程进阶II</code></p><h3 id="实现原理">实现原理</h3><p>编译器维护符号表</p><h2 id="默认参数">默认参数</h2><p>C++支持在函数声明时给出默认参数值，<strong>且若有，必须在声明时给出</strong></p><p>与<code>python</code> 一样，默认参数优先<strong>放在参数列表中靠右位置，且不能间断（即不能中间插个普通参数）</strong></p><p>可以<strong>匿名给出参数默认值</strong>，即声明时还是不必须写形参</p><p><em>默认参数可能会带来歧义！！！</em></p><p>示例:</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">int</span>)</span></span>; <span class="comment">// 没有默认参数的函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">int</span>, <span class="type">int</span>=<span class="number">2</span>)</span></span>; <span class="comment">// 带默认参数的函数，匿名给出默认参数，但在具体调用时因为可以不写第二个参数，导致与上面那个函数产生歧义</span></span><br><span class="line"><span class="comment">// 即f(1)该调用哪个？</span></span><br><span class="line"><span class="comment">// 上述代码编译错误</span></span><br></pre></td></tr></table></figure><p>正确例子：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">f</span><span class="params">(<span class="type">int</span>, <span class="type">int</span>=<span class="number">2</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">f</span>(<span class="number">1</span>) &lt;&lt; endl; <span class="comment">// 3</span></span><br><span class="line">    cout &lt;&lt; <span class="built_in">f</span>(<span class="number">1</span>, <span class="number">3</span>) &lt;&lt; endl;  <span class="comment">// 4</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">f</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 若调用时不传参数b，则默认b = 2</span></span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="内联函数">内联函数</h2><p><code>inline</code>关键字用于声明匿名函数</p><h3 id="编译器如何实现内联函数">编译器如何实现内联函数</h3><p>与编译器优化中的<strong>函数内联</strong>（在函数调用的地方将函数展开，用函数定义的代码替换函数调用）相同，编译器将为<code>inline</code>函数创建一段代码，在调用点以相应的代码替换</p><p>因此被<code>inline</code>修饰的函数在编译过程中就已经被替换成具体的代码了，而不再会发生函数调用，那为什么还要用内联函数？</p><h3 id="目的">目的</h3><ul><li>提高可读性：编译器眼中没有函数但程序员不是，可以提高代码的可读性</li><li>提高效率：<strong>编译器不再需要进行函数调用</strong>，而是直接顺序地执行等价代码，避免了函数调用的时间开销</li></ul><h3 id="限制">限制</h3><ul><li>内联函数不能是递归的</li><li>不能是函数指针</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> c = <span class="built_in">add</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line"><span class="comment">// 编译后变为int c = 1 + 2;</span></span><br></pre></td></tr></table></figure><h3 id="使用场景">使用场景</h3><p>适合于频率高、简单的小段代码</p><p>事实上即使不使用<code>inline</code>关键字，在开启<code>O1/O2/O3</code>或<strong>指定函数内联</strong>的编译器优化的情况下，编译器自己也会将源代码中一些简短的函数内联优化掉，从而避免频繁的函数调用。</p><h3 id="避免滥用inline">避免滥用inline</h3><p>经过计算机组成原理课程的学习，我们知道计算机硬件具有<strong>时空局部性</strong>的特点，即连续时间内更可能访问连续的空间（比如遍历数组），而大量inline的使用可能会导致代码体积的增大，使得代码本身占的空间变大，可能会导致<strong>病态的换页</strong>，降低指令快取装置的命中率。</p><p><strong>缺点总结：</strong></p><ul><li>增大目标代码</li><li>病态的换页</li><li>降低指令快取装置的命中率</li></ul><p>(•‿•)</p>]]></content>
      
      
      <categories>
          
          <category> 软件学院 </category>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 软件学院 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++介绍</title>
      <link href="/ymhui.github.io/2024/12/23/C-%E4%BB%8B%E7%BB%8D/"/>
      <url>/ymhui.github.io/2024/12/23/C-%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h1>C++介绍</h1><p>参考南软2024秋C++高级程序设计课件introduction, introduction2</p><h2 id="C-历史">C++历史</h2><p><em>今年不考，以后应该也不会考了</em></p><p>C++的历史：1979-1991</p><p>在真实世界中为其进化一门语言：1991-2006</p><p>改变世界：2006-至今</p><h2 id="C和C-的关系">C和C++的关系</h2><ol><li>C++完全包含C语言成分，C++支持C所支持的全部编程技巧（C的超集）；同时C++还添加了OOP支持</li><li>任何C程序都能被C++用基本相同的方法编写，并具备相同的运行效率和空间</li><li>C++还引入了重载、内联函数、异常处理等功能，对C中的过程化控制及其功能进行了扩充</li><li>C++由以下4部分有机组成<ul><li>C</li><li>OOP</li><li>STL</li><li>Inside-Model</li></ul></li></ol><h2 id="语言的类别">语言的类别</h2><p>C/C++是静态强类型语言，C++比C”更“静态</p><h2 id="结构化编程">结构化编程</h2><p>程序 = 数据结构 + 算法</p><h3 id="ADT">ADT</h3><p>Abstract Data Type 抽象数据类型</p><p>结构化编程需要关注抽象数据类型、联系、优先级、类型转换（强制、隐式、溢出？）、求值顺序、副作用</p><h3 id="cast">cast</h3><p>此处介绍各种类型的cast（类型转换）</p><h4 id="static-cast">static_cast</h4><p>用法：<code>static_cast&lt;&gt;()</code>，效果类似于直接用<code>(类型)</code>的强制类型转换</p><h4 id="const-cast">const_cast</h4><p>用法：<code>const_cast&lt;&gt;()</code>，用于去除const变量的修饰符<code>const</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> c = <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span>* p = <span class="built_in">const_cast</span>&lt;<span class="type">int</span>*&gt;(&amp;c);</span><br><span class="line">*p = <span class="number">20</span>;</span><br><span class="line">cout &lt;&lt; &amp;c &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; p &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; *p &lt;&lt; endl;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0012FF74 10</span><br><span class="line">0012FF74 20</span><br></pre></td></tr></table></figure><p>个人看法：在编译器眼中，<code>c</code>是一个编译时常量，<strong>可能维护了一个类似常量表的东西</strong>，凡是遇到读<code>c</code>的地方统统换成这个对应的常量，因为<code>const</code>类型的变量不允许修改，因此在编译器眼中，这段代码<code>cout &lt;&lt; &amp;c &lt;&lt; &quot;: &quot; &lt;&lt; c &lt;&lt; endl;</code>实际上是<code>cout &lt;&lt; &amp;c &lt;&lt; &quot;: &quot; &lt;&lt; 10 &lt;&lt; endl;</code>；而<code>p</code>是一个指向<code>c</code>的地址的指针，它的的确确把这个内存中的值改了，<code>const</code><strong>只是保证了</strong><code>c</code><strong>这个变量不被修改</strong>，这个变量只是用来解释一个特定内存中存储的二进制序列，<strong>并不保证内存一定不会被修改</strong>。那么编译器保证变量<code>c</code>始终是个常量的做法应该就是维护一个常量表来做替换。</p><h4 id="reinterpret-cast">reinterpret_cast</h4><p>用法：<code>reinterpret_cast&lt;&gt;()</code></p><p><code>reinterpret_cast</code> 是 C++ 中的一个强制类型转换操作符，用于在不同类型之间进行转换，尤其是指针和整数之间的转换。它是 C++ 中最强大、最<strong>危险</strong>的类型转换操作符之一。使用 <code>reinterpret_cast</code> 可以实现低级别的内存操作，绕过类型系统的类型检查。</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// float -&gt; int</span></span><br><span class="line"><span class="type">float</span> f = <span class="number">1.45</span>;</span><br><span class="line"><span class="type">int</span>* x = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">int</span>*&gt;(&amp;f);</span><br><span class="line">cout &lt;&lt; *x &lt;&lt; endl; <span class="comment">// 1</span></span><br><span class="line"><span class="comment">// 指针 -&gt; 整数</span></span><br><span class="line"><span class="type">uintptr_t</span> address = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">uintptr_t</span>&gt;(x); <span class="comment">// 把指针变成了整数，实际上就是指针本身的数值</span></span><br><span class="line">cout &lt;&lt; <span class="string">&quot;address of x: &quot;</span> &lt;&lt; address &lt;&lt; endl;</span><br><span class="line"><span class="comment">// 整数 -&gt; 指针</span></span><br><span class="line"><span class="type">int</span>* y = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">int</span>*&gt;(address); <span class="comment">// 把整数变成了指针，实际上就是给指针本身一个值（特定内存地址）</span></span><br><span class="line">cout &lt;&lt; *y &lt;&lt; endl; <span class="comment">// 1</span></span><br></pre></td></tr></table></figure><p>当然也可以在不同类型的指针之间转换，比如<code>int*</code>和<code>char*</code></p><h4 id="dynamic-cast">dynamic_cast</h4><p>用法：<code>dynamic_cast&lt;&gt;()</code></p><p>动态类型转换一般用于多态，即基类指针和派生类指针之间的转换，且<strong>基类中必须要有虚函数</strong>(<code>virtual</code>关键字)才能使用动态类型转换，<strong>面向对象编程中常用</strong>。</p><h2 id="auto">auto</h2><p><code>auto</code>关键字可以根据值的实际类型决定变量的类型</p><p>C++为什么需要auto？</p><ul><li>减少冗余：有些类型特别长，写起来容易出错，提供auto关键字可以减轻程序员的负担，把类型判断交给编译器</li><li>简化模板编程</li><li>类型推导支持泛型编程</li><li>支持返回类型推导（即可以把返回值声明为<code>auto</code>）</li><li>提高编程的灵活性，更优雅</li></ul><p>但也有缺点，在大量使用<code>auto</code>的代码中，调试起来的<strong>可读性可能会变差</strong>。</p><h2 id="union和struct">union和struct</h2><p>与<code>C</code>语言相同，只需要注意<code>struct</code>默认访问权限是<code>public</code>，<code>class</code>默认是<code>private</code></p><h2 id="tuple">tuple</h2><p><code>std::tuple</code> 是 C++11 引入的一种数据结构，用于存储多个不同类型的元素。与 <code>std::pair</code> 只能存储两个元素不同，<code>std::tuple</code> 可以存储任意数量和类型的元素。它是一个模板类，可以存储多个不同类型的数据。</p><p>使用 <code>std::get&lt;index&gt;(tuple)</code> 来访问元组中的元素，其中 <code>index</code> 是元素的索引（从 0 开始）。</p><ul><li><code>std::tuple_size</code>结合<code>decltype()</code> 用于获取元组中元素的个数。</li><li><code>std::tuple_element</code> 用于获取指定索引的元素类型。</li></ul><p><code>std::tie</code> 可以将元组的元素与一组变量进行绑定，使得你能够解构元组。常用于返回多个值的函数</p><p><code>std::make_tuple</code> 是一个方便的函数，可以用来创建一个元组，并且自动推导元素类型</p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function">tuple&lt;<span class="type">int</span>, <span class="type">double</span>, string&gt; <span class="title">t</span><span class="params">(<span class="number">1</span>, <span class="number">1.4</span>, <span class="string">&quot;514&quot;</span>)</span></span>;</span><br><span class="line"><span class="keyword">auto</span> first = <span class="built_in">get</span>&lt;<span class="number">0</span>&gt;(t), second = <span class="built_in">get</span>&lt;<span class="number">1</span>&gt;(t), third = <span class="built_in">get</span>&lt;<span class="number">2</span>&gt;(t);</span><br><span class="line"><span class="keyword">auto</span> size = tuple_size&lt;<span class="keyword">decltype</span>(t)&gt;::value; <span class="comment">// 3</span></span><br><span class="line">tuple_element&lt;<span class="number">1</span>, <span class="keyword">decltype</span>(t)&gt;::type _second_ = <span class="built_in">get</span>&lt;<span class="number">1</span>&gt;(t); <span class="comment">// tuple_element&lt;1, decltype(t)&gt;::type获得第二个元素的类型，不过不如auto方便</span></span><br><span class="line"></span><br><span class="line"><span class="function">tuple&lt;<span class="type">int</span>, <span class="type">double</span>, std::string&gt; <span class="title">getTuple</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> std::<span class="built_in">make_tuple</span>(<span class="number">1</span>, <span class="number">3.14</span>, <span class="string">&quot;Hello&quot;</span>); <span class="comment">// 创建一个元组</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">double</span> b;</span><br><span class="line">string c;</span><br><span class="line"><span class="built_in">tie</span>(a, b, c) = <span class="built_in">getTuple</span>();</span><br></pre></td></tr></table></figure><h2 id="optional">optional</h2><p><code>std::optional</code> 是 C++17 引入的标准库类型，用于表示可能为空的值。它是一种封装类型，可以表示“存在值”或“无值”的状态，避免必须要用指针中的空指针代表值不存在的概念。</p><p><code>std::optional</code>的默认构造函数是“无值”的；可以通过成员函数<code>std::optional::has_value()</code>判断是否有值</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">optional&lt;<span class="type">int</span>&gt; o1; <span class="comment">// 默认无值</span></span><br><span class="line">optional&lt;<span class="type">int</span>&gt; o2 = <span class="number">10</span>; <span class="comment">// 有值10</span></span><br><span class="line">optional&lt;<span class="type">int</span>&gt; o3 = <span class="literal">nullopt</span>; <span class="comment">// 显式表达无值</span></span><br><span class="line"></span><br><span class="line">cout &lt;&lt; o<span class="number">1.</span><span class="built_in">has_value</span>() &lt;&lt; o<span class="number">2.</span><span class="built_in">has_value</span>() &lt;&lt; o<span class="number">3.</span><span class="built_in">has_value</span>(); <span class="comment">// 010</span></span><br><span class="line"></span><br><span class="line"><span class="function">optional&lt;string&gt; <span class="title">getString</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* str)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (str != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">string</span>(str);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullopt</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">optional&lt;string&gt; <span class="title">getString</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* str, <span class="type">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (str != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">string</span>(str, length);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullopt</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="variant">variant</h2><p><code>std::variant</code>表示可以存储多种可能类型的数据，但同一时刻只能存储一种类型的值，可以通过<code>std::get</code>或<code>std::visit</code>访问存储的值</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">variant&lt;<span class="type">int</span>, <span class="type">double</span>, string&gt; v;</span><br><span class="line">v = <span class="number">1</span>;</span><br><span class="line">cout &lt;&lt; <span class="built_in">get</span>&lt;<span class="type">int</span>&gt;(v); <span class="comment">// 1</span></span><br><span class="line">v = <span class="string">&quot;14&quot;</span>;</span><br><span class="line">cout &lt;&lt; <span class="built_in">get</span>&lt;string&gt;(v); <span class="comment">// &quot;14&quot;</span></span><br><span class="line">v = <span class="number">5.14</span>;</span><br><span class="line">cout &lt;&lt; <span class="built_in">get</span>&lt;<span class="type">double</span>&gt;(v); <span class="comment">// 5.14</span></span><br></pre></td></tr></table></figure><p><code>std::holds_alternative</code> 检查当前 <code>variant</code> 是否存储了某种类型</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cout &lt;&lt; <span class="built_in">holds_alternative</span>&lt;<span class="type">int</span>&gt;(v); <span class="comment">// 1</span></span><br><span class="line">cout &lt;&lt; <span class="built_in">holds_alternative</span>&lt;<span class="type">char</span>&gt;(v); <span class="comment">// 0</span></span><br></pre></td></tr></table></figure><p><code>std::visit</code>可以访问<code>std::variant</code>中的元素并做相应的操作</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Visitor</span> &#123;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> i)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;int: &quot;</span> &lt;&lt; i &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">double</span> d)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;double: &quot;</span> &lt;&lt; d &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> std::string&amp; s)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;string: &quot;</span> &lt;&lt; s &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">v = <span class="number">42</span>;</span><br><span class="line"><span class="comment">// 使用 visit 访问并打印 variant 中存储的值</span></span><br><span class="line"><span class="built_in">visit</span>(<span class="built_in">Visitor</span>(), v);  <span class="comment">// 输出: int: 42</span></span><br><span class="line">v = <span class="number">3.14</span>;</span><br><span class="line"><span class="built_in">visit</span>(<span class="built_in">Visitor</span>(), v);  <span class="comment">// 输出: double: 3.14</span></span><br><span class="line">v = <span class="string">&quot;Hello Variant!&quot;</span>;</span><br><span class="line"><span class="built_in">visit</span>(<span class="built_in">Visitor</span>(), v);  <span class="comment">// 输出: string: Hello Variant!</span></span><br></pre></td></tr></table></figure><p>相比传统的<code>union</code>，<code>variant</code>的优点有：</p><ul><li>类型安全：不像传统的 <code>union</code>，<code>std::variant</code> 是类型安全的，编译器会确保你只在正确的类型上进行操作</li><li>灵活</li><li>简化代码</li></ul><h2 id="any">any</h2><p>类似<code>variant</code>但不需要提前声明存储的类型，可以存储任意类型；也可以视作是类型安全的<code>void*</code>（直接使用<code>void*</code>可能引发问题）</p><p>通过 <code>std::any_cast&lt;&gt;()</code> 来访问存储的数据</p><p>示例：类似<code>auto</code>？</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 使用 std::any 存储不同类型的值</span></span><br><span class="line">std::any a = <span class="number">42</span>;             <span class="comment">// 存储 int</span></span><br><span class="line">std::any b = <span class="number">3.14</span>;           <span class="comment">// 存储 double</span></span><br><span class="line">std::any c = std::<span class="built_in">string</span>(<span class="string">&quot;Hello, Any!&quot;</span>); <span class="comment">// 存储 string</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">// 提取并访问存储的值</span></span><br><span class="line">std::cout &lt;&lt; std::<span class="built_in">any_cast</span>&lt;<span class="type">int</span>&gt;(a) &lt;&lt; std::endl;            <span class="comment">// 输出: 42</span></span><br><span class="line">std::cout &lt;&lt; std::<span class="built_in">any_cast</span>&lt;<span class="type">double</span>&gt;(b) &lt;&lt; std::endl;         <span class="comment">// 输出: 3.14</span></span><br><span class="line">std::cout &lt;&lt; std::<span class="built_in">any_cast</span>&lt;std::string&gt;(c) &lt;&lt; std::endl;   <span class="comment">// 输出: Hello, Any!</span></span><br></pre></td></tr></table></figure><p><code>std::any::has_value()</code>检查 <code>std::any</code> 是否包含有效的值。如果 <code>std::any</code> 中存储了一个值，返回 <code>true</code>，否则返回 <code>false</code></p><p><code>std::any::reset()</code>清除 <code>std::any</code> 中存储的值，使其不再包含任何值</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cout &lt;&lt; std::a.<span class="built_in">has_value</span>();; <span class="comment">// 1</span></span><br><span class="line">std::a.<span class="built_in">reset</span>();</span><br><span class="line">cout &lt;&lt; std::a.<span class="built_in">has_value</span>();; <span class="comment">// 0</span></span><br></pre></td></tr></table></figure><h2 id="智能指针">智能指针</h2><h3 id="RAII">RAII</h3><p><strong>Resource Acquisition Is Initialization</strong></p><p>RAII是C++中重要的思想，即资源在初始化时分配，也可以理解成在构造函数（初始化时）中获取资源并初始化，在析构函数中释放资源（归还）。</p><p>智能指针通过多套一层来管理指针资源，减少了内存泄漏的可能性</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="type">int</span> x;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">A</span>(<span class="type">int</span> a): <span class="built_in">x</span>(a) &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">old_use</span><span class="params">(<span class="type">int</span> var)</span> </span>&#123;</span><br><span class="line">    A* a = <span class="keyword">new</span> <span class="built_in">A</span>(var);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">if</span> (...) <span class="keyword">throw</span> ...; <span class="comment">// 可能忘记释放a导致泄漏</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">delete</span> a; <span class="comment">// 容易忘记导致泄露</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">new_use</span><span class="params">(<span class="type">int</span> var)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> p = <span class="built_in">unique_ptr</span>&lt;A&gt;(<span class="keyword">new</span> <span class="built_in">A</span>(var)); <span class="comment">// 利用智能指针unique_ptr而不是裸指针A*来管理new出来的指针资源</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">if</span> (...) <span class="keyword">throw</span> ...; <span class="comment">// 不会泄露指针资源，因为退出当前函数时p的生命周期结束自动调用其析构函数，而在智能指针的析构函数中会在特定规则下主动释放资源</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// 避免了潜在的内存泄漏风险</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="unique-ptr">unique_ptr</h3><p><code>unique_ptr</code>独占资源，只能有一个指向内存的指针</p><p><code>make_unique&lt;&gt;()</code>也可以创建一个独享智能指针</p><p><code>unique_ptr</code>不允许拷贝但允许移动，可以通过<code>std::move</code>将资源的所有权从一个<code>unique_ptr</code>转移到另一个</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> p1 = <span class="built_in">unique_ptr</span>&lt;A&gt;(<span class="keyword">new</span> <span class="built_in">A</span>(<span class="number">1</span>));</span><br><span class="line"><span class="keyword">auto</span> p2 = <span class="built_in">make_unique</span>&lt;A&gt;(<span class="number">2</span>);</span><br><span class="line"><span class="comment">// 使用move转移资源</span></span><br><span class="line"><span class="keyword">auto</span> p3 = <span class="built_in">move</span>(p2);</span><br></pre></td></tr></table></figure><h3 id="shared-ptr-weak-ptr">shared_ptr, weak_ptr</h3><h4 id="shared-ptr">shared_ptr</h4><p><code>std::shared_ptr</code> 是一个智能指针，它允许多个指针共同拥有同一个对象的所有权。当最后一个 <code>shared_ptr</code> 被销毁时，它所管理的对象会自动销毁（即调用析构函数并释放内存）。它通过引用计数机制来跟踪有多少个 <code>shared_ptr</code> 指向同一个对象，但存在一个<strong>潜在的循环引用的问题</strong>。</p><p>*循环引用问题：*两个或多个对象之间相互持有对方的引用，从而导致它们的引用计数永远不会归零</p><p><code>std::shared_ptr::use_count</code> 返回 <code>shared_ptr</code> 指向同一个对象的引用计数</p><p><code>std::shared_ptr::reset</code>使当前共享指针不再指向管理的对象，引用计数 - 1</p><p>和<code>unique_ptr</code>一样可以通过<code>shared_ptr&lt;&gt;()</code>和<code>make_shared&lt;&gt;()</code>创建共享智能指针，不同的是，<code>shared_ptr</code><strong>可以直接通过赋值运算符创建一个指向同一对象的</strong><code>shared_ptr</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> sp = <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt;(<span class="keyword">new</span> <span class="built_in">int</span>(<span class="number">3</span>));</span><br><span class="line"><span class="keyword">auto</span> sp1 = <span class="built_in">make_shared</span>&lt;<span class="type">int</span>&gt;(<span class="number">1</span>);</span><br><span class="line">shared_ptr&lt;<span class="type">int</span>&gt; sp2 = sp; <span class="comment">// sp2和sp指向同一资源</span></span><br><span class="line"><span class="keyword">auto</span> sp3 = sp1; <span class="comment">// 同理，sp3和sp1指向同一资源</span></span><br><span class="line">cout &lt;&lt; sp.<span class="built_in">use_count</span>() &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; sp<span class="number">2.</span><span class="built_in">use_count</span>() &lt;&lt; endl; <span class="comment">// 2 2</span></span><br><span class="line">cout &lt;&lt; sp<span class="number">1.</span><span class="built_in">use_count</span>() &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; sp<span class="number">3.</span><span class="built_in">use_count</span>() &lt;&lt; endl; <span class="comment">// 2 2</span></span><br></pre></td></tr></table></figure><h4 id="weak-ptr">weak_ptr</h4><p>为了避免循环引用问题，引入<code>weak_ptr</code></p><p>它本身不管理对象的生命周期，也<strong>不增加对象的引用计数</strong>。<code>weak_ptr</code> <strong>只是观察一个对象，但不拥有它</strong>。可以通过 <code>std::weak_ptr</code> 来访问 <code>shared_ptr</code> 指向的对象，前提是该对象仍然存在（即至少有一个 <code>shared_ptr</code> 在管理该对象）</p><p>使用 <code>std::weak_ptr::lock</code> 可以尝试从 <code>weak_ptr</code> 获取一个 <code>shared_ptr</code>，如果<strong>对象已被销毁</strong>，<code>lock()</code> 将<strong>返回一个空的</strong> <code>shared_ptr</code></p><p>应用场景：<strong>避免循环引用</strong>，如果两个对象相互引用 <code>shared_ptr</code>，就会导致循环引用，导致内存无法释放。使用 <code>weak_ptr</code> 可以避免这个问题。</p><p>例如，两个对象互相持有 <code>shared_ptr</code>，但其中一个对象通过 <code>weak_ptr</code> 引用另一个对象，这样就不会导致引用计数增加，从而避免循环引用。</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> sp = <span class="built_in">make_shared</span>&lt;<span class="type">int</span>&gt;(<span class="number">1</span>);</span><br><span class="line">weak_ptr&lt;<span class="type">int</span>&gt; wp = sp; <span class="comment">// weak_ptr不增加引用计数</span></span><br><span class="line">cout &lt;&lt; sp.<span class="built_in">use_count</span>() &lt;&lt; endl; <span class="comment">// 1</span></span><br><span class="line"><span class="keyword">auto</span> sp1 = wp.<span class="built_in">lock</span>(); <span class="comment">// 尝试获取一个shared_ptr，成功</span></span><br><span class="line"><span class="keyword">if</span> (sp1) &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;sp1 success&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;sp1 fail&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">cout &lt;&lt; sp.<span class="built_in">use_count</span>() &lt;&lt; endl; <span class="comment">// 2</span></span><br><span class="line">sp<span class="number">1.</span><span class="built_in">reset</span>();</span><br><span class="line">sp.<span class="built_in">reset</span>(); <span class="comment">// 引用计数归0，释放资源</span></span><br><span class="line"><span class="keyword">auto</span> sp2 = wp.<span class="built_in">lock</span>(); <span class="comment">// 没有资源被管理了，返回空shared_ptr，获取失败</span></span><br><span class="line"><span class="keyword">if</span> (sp2) &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;sp2 success&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;sp2 fail&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="总结">总结</h4><table><thead><tr><th>特性</th><th><code>std::shared_ptr</code></th><th><code>std::weak_ptr</code></th></tr></thead><tbody><tr><td><strong>引用计数</strong></td><td>增加引用计数，管理对象的生命周期</td><td>不增加引用计数，不能管理对象生命周期</td></tr><tr><td><strong>访问对象</strong></td><td>直接访问对象</td><td>通过 <code>lock()</code> 转换为 <code>shared_ptr</code>，访问对象</td></tr><tr><td><strong>对象生命周期管理</strong></td><td>当最后一个 <code>shared_ptr</code> 被销毁时，自动删除对象</td><td>不管理对象生命周期，不能删除对象</td></tr><tr><td><strong>避免循环引用</strong></td><td>会导致循环引用问题</td><td>可以避免循环引用</td></tr></tbody></table><h2 id="数组">数组</h2><p>高维数组的本质：用一维数组组织，用高维数组解释</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">show</span><span class="params">(<span class="type">int</span> a[], <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        cout &lt;&lt; a[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">show</span><span class="params">(<span class="type">int</span> a[][<span class="number">2</span>], <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; j++) &#123;</span><br><span class="line">            cout &lt;&lt; a[i][j] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">show</span><span class="params">(<span class="type">int</span> a[][<span class="number">2</span>][<span class="number">3</span>], <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; j++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; <span class="number">3</span>; k++) &#123;</span><br><span class="line">                cout &lt;&lt; a[i][j][k] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            cout &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> b[<span class="number">12</span>]; <span class="comment">// 创建一个一维数组，接下来分别按照一维、二维、三维数组来解释这个内存中组织成一维的线性数组</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">12</span>; i++) &#123;</span><br><span class="line">        b[i] = i + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">show</span>(b, <span class="number">12</span>);</span><br><span class="line">    <span class="keyword">typedef</span> <span class="type">int</span> T[<span class="number">2</span>]; <span class="comment">// 按二维数组解释 2 * 6</span></span><br><span class="line">    <span class="keyword">typedef</span> <span class="type">int</span> T1[<span class="number">3</span>]; </span><br><span class="line">    <span class="keyword">typedef</span> T1 T2[<span class="number">2</span>]; <span class="comment">// 按三维数组解释 2 * 3 * 4</span></span><br><span class="line">    <span class="built_in">show</span>((T*)b, <span class="number">6</span>);</span><br><span class="line">    <span class="built_in">show</span>((T2*)b, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>事实上计算机中没有“维度”的概念，数据在其中并非立体，因此看似有空间感的高维数组，在实现层面都是以一维线性数组组织的。</p><p>(•‿•)</p>]]></content>
      
      
      <categories>
          
          <category> 软件学院 </category>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 软件学院 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++面向对象编程进阶II</title>
      <link href="/ymhui.github.io/2024/12/20/C-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6II/"/>
      <url>/ymhui.github.io/2024/12/20/C-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6II/</url>
      
        <content type="html"><![CDATA[<h1>C++面向对象编程进阶II</h1><p>参考南软2024秋C++高级程序设计课件CPP-2-3</p><h2 id="多态">多态</h2><p><strong>多态是面向对象编程语言的重要特性之一</strong></p><ul><li>同一论域中的一个元素可以有多种解释</li><li>提高语言灵活性</li><li>C++中的体现有：<ul><li>一名多用（函数重载）</li><li>模板编程（template）</li><li>虚函数</li></ul></li></ul><h2 id="操作符重载">操作符重载</h2><h3 id="C-重载类型">C++重载类型</h3><ul><li>函数重载<ul><li>函数同名但<strong>参数列表不同</strong>（后者必要条件)</li><li>静态绑定</li></ul></li><li>操作符重载<ul><li>重载的动机？<ul><li>为了自定义数据类型可以像内置(built_in)数据类型一样运算</li><li>让编译器代替程序员展开计算（从运算符展到函数）</li></ul></li><li>提高可读性</li><li>提高可扩展性</li></ul></li></ul><h3 id="函数重载的细节">函数重载的细节</h3><p>可以总结为<strong>通过规范匹配顺序来尽可能避免歧义</strong>的问题</p><h4 id="精确匹配">精确匹配</h4><p>当调用一个有多个重载版本的函数时，编译器会首先尝试进行<strong>精确匹配</strong>，即参数类型完全一致的函数签名，这种情况下的匹配最直接最合适。</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">double</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">func</span>(<span class="number">42</span>); <span class="comment">// func(int)为精确匹配的最合适结果，42直接匹配int</span></span><br></pre></td></tr></table></figure><h4 id="更好匹配">更好匹配</h4><p>当编译器找不到参数类型完全一致的函数签名时，即不存在精确匹配，编译器会选择更好匹配的重载，这种情况下通常需要发生隐式类型转换。</p><ul><li>不进行不必要的转换：即若存在精确匹配则不会尝试进行类型转换匹配</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">float</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">func</span>(<span class="number">3</span>); <span class="comment">// 直接选择func(int);因为没有必要做浮点数转换</span></span><br></pre></td></tr></table></figure><h4 id="窄化转换">窄化转换</h4><p>如果有多个候选函数的匹配都需要某种类型转换，编译器会优先选择<strong>不需要窄化转换</strong>的函数。<strong>窄化转换</strong>是指<strong>从一个范围更大的类型转换为范围更小的类型</strong>，这通常会丢失数据。</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">double</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">func</span>(<span class="number">3.14f</span>); <span class="comment">// 传入float类型，选择到double的扩展匹配func(double)</span></span><br><span class="line"><span class="comment">// 因为float到double是扩展类型转换，而到int是窄化转换，可能会丢失精度</span></span><br></pre></td></tr></table></figure><h4 id="标准转换与用户定义转换">标准转换与用户定义转换</h4><ul><li>标准转换：内建类型的转换比如<code>int-&gt;double, int-&gt;long</code></li><li>用户定义转换：例如通过<code>explicit</code>转换构造函数或类型转换运算符进行的转换</li></ul><p>编译器会优先选择标准类型转换，而不一定会选择需要通过用户定义的转换来进行的转换，因为<strong>用户定义的转换比内建类型的转换优先级较低</strong>。</p><h4 id="歧义">歧义</h4><p>若没有精确匹配且两个或更多的重载函数之间存在匹配，则编译器无法决定用哪个，导致<strong>歧义</strong>，引发<strong>编译错误</strong>；这种情况往往是没有精确匹配而需要类型转换，而有多个类型转换的版本都可以匹配</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">double</span> a)</span> </span>&#123;&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">float</span> a)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// func(3); // 编译错误，因为没有int版本的精确匹配，而两个类型转换的版本都可以匹配 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="操作符重载-2">操作符重载</h3><h4 id="语法">语法</h4><p><code>&lt;returnType&gt; operator操作符(&lt;argvList&gt;)&#123;&lt;definition&gt;&#125;</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Complex</span> &#123;</span><br><span class="line">    <span class="comment">// 复数类</span></span><br><span class="line">    <span class="type">double</span> real, imag;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Complex</span>(): <span class="built_in">real</span>(<span class="number">0</span>), <span class="built_in">imag</span>(<span class="number">0</span>) &#123;&#125;</span><br><span class="line">    <span class="built_in">Complex</span>(<span class="type">double</span> r, <span class="type">double</span> i): <span class="built_in">real</span>(r), <span class="built_in">imag</span>(i) &#123;&#125;</span><br><span class="line">    <span class="function">Complex <span class="title">add</span><span class="params">(Complex&amp; x)</span></span>; <span class="comment">// 用函数而不是操作符重载</span></span><br><span class="line">    Complex <span class="keyword">operator</span>+(Complex&amp; x) &#123;</span><br><span class="line">        <span class="comment">// 操作符重载，虽然本质上也是定义了一个函数</span></span><br><span class="line">        <span class="comment">// +是二元运算符而这个函数事实上也有两个参数，还有一个隐藏参数是当前对象this</span></span><br><span class="line">        Complex tmp;</span><br><span class="line">        tmp.real = real + x.real;</span><br><span class="line">        tmp.imag = imag + x.imag;</span><br><span class="line">        <span class="keyword">return</span> tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">Complex <span class="title">x</span><span class="params">(<span class="number">1</span>, <span class="number">2</span>)</span>, <span class="title">y</span><span class="params">(<span class="number">3</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">    Complex z;</span><br><span class="line">    z = x.<span class="built_in">add</span>(y);</span><br><span class="line">    z = x + y; <span class="comment">// 两种写法结果一样但显然这种用&#x27;+&#x27;的可读性更好</span></span><br><span class="line">    <span class="comment">// z = x + y; &lt;=&gt; z = x.operator+(y);</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重载运算符的<strong>参数数量由运算符是几元的</strong>决定</p><h4 id="不可重载的操作符">不可重载的操作符</h4><p><code>.</code>, <code>.*</code>, <code>::</code>, <code>?:</code></p><h4 id="重载原则">重载原则</h4><p>两种方式：</p><ul><li>类的成员函数</li><li>或者带有类参数的全局函数</li></ul><p>重载后的运算符仍然遵循原有的语法，即：</p><ul><li>单目/双目</li><li>优先级</li><li>结合性</li></ul><h3 id="全局函数重载运算符">全局函数重载运算符</h3><p>实现方式：对应需要重载运算符的类中为重载运算符函数分配友元，之后再在外部全局域给出实现</p><p>需注意全局函数重载时没有隐藏参数<code>this</code>了，因此要把参数声明全</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span> &#123;</span><br><span class="line">    <span class="keyword">friend</span> MyClass <span class="keyword">operator</span>操作符(&lt;argvList&gt;);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">MyClass <span class="keyword">operator</span>操作符(&lt;argvList&gt;) &#123;</span><br><span class="line"> <span class="comment">// ...   </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>注：</em><code>=</code>, <code>()</code>, <code>[]</code>, <code>-&gt;</code>不能作为全局函数重载</p><h4 id="1-不能全局重载-运算符（赋值运算符）">1. 不能全局重载**<code>=</code> 运算符（赋值运算符）**</h4><p>赋值运算符 (<code>operator=</code>) 不能作为全局函数重载的原因是，赋值运算符的操作通常是为了对<strong>类的成员进行修改</strong>，因此赋值运算符的定义必须依赖于类的内部机制。</p><ul><li>在类内部重载赋值运算符时，通常会涉及到类的成员（例如拷贝或移动构造、资源管理等），并且需要对 <code>this</code> 指针所指向的当前对象进行修改。</li><li>因为赋值操作的左右两边总是<strong>类对象</strong>，所以只能作为成员函数来实现。</li></ul><h4 id="2-不能全局重载-运算符（函数调用运算符）">2. 不能全局重载**<code>()</code> 运算符（函数调用运算符）**</h4><p><code>operator()</code> 被称为<strong>函数调用运算符</strong>，通常用于模拟对象的“可调用性”。这意味着一个类可以通过定义这个运算符，使得其对象能够像函数一样被调用。</p><ul><li><strong><code>operator()</code></strong> 的实现依赖于对象的状态，且通常是为了给类的对象提供类似函数的调用语法。</li><li>这个运算符的调用通常依赖于类的成员，因此它也必须是类的成员函数。</li></ul><h4 id="3-不能全局重载-运算符（下标运算符）">3. 不能全局重载**<code>[]</code> 运算符（下标运算符）**</h4><p>下标运算符 (<code>operator[]</code>) 用于数组访问，通常需要对对象的特定成员进行操作。在 C++ 中，<code>operator[]</code> 必须是成员函数，因为它操作的是类对象的数据（比如访问某个特定的元素）。</p><ul><li><code>operator[]</code> 必须访问类对象的内部状态（例如数组或容器），因此它通常是类的成员函数。</li><li>虽然可以定义一个全局的 <code>operator[]</code>，但是这不符合常见的用法和设计模式，且无法直接访问类内部数据。</li></ul><h4 id="4-不能全局重载-运算符（成员指针访问运算符）">4. 不能全局重载**<code>-&gt;</code> 运算符（成员指针访问运算符）**</h4><p><code>operator-&gt;</code> 用于指针或指向成员对象的访问。其主要作用是允许对象表现得像指针一样，便于通过 <code>-&gt;</code> 来访问对象的成员。与其他运算符一样，<code>operator-&gt;</code> 操作的是对象的成员，因此只能在类内定义。</p><ul><li><code>operator-&gt;</code> 让对象模拟指针的行为，通常用于类封装的智能指针或容器类中。</li><li>它需要访问类的内部数据，且通常是指针类型的成员，因此必须在类的内部定义。</li></ul><h3 id="操作符重载注意点">操作符重载注意点</h3><ol><li>二元运算符注意是否满足交换等价：当运算符两边的操作数类型不一致（比如内建类型和自定义类型），注意实现两个版本的操作符重载以满足不同的操作数顺序</li></ol><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CL</span> &#123;</span><br><span class="line">    <span class="type">int</span> count;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">friend</span> CL <span class="keyword">operator</span>+(<span class="type">int</span> i, CL&amp; a);</span><br><span class="line">    <span class="keyword">friend</span> CL <span class="keyword">operator</span>+(CL&amp; a, <span class="type">int</span> i);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    CL cl;</span><br><span class="line">    cl + <span class="number">10</span>; </span><br><span class="line">    <span class="number">10</span> + cl; <span class="comment">// 分别调用两个版本的+重载</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>永远不要重载逻辑运算符<code>&amp;&amp;</code>和<code>||</code>：否则你会<strong>失去短路求值的特性</strong></li></ol><p>因为操作符重载本质上是函数调用，那么在<strong>传参的时候就会把两个操作数的结果都算出来才能传进去</strong>，那么就没有短路求值的特性了。</p><h3 id="单目操作符重载">单目操作符重载</h3><ul><li>类成员函数：隐含<code>this</code>参数因此不需要显式声明参数</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">&lt;returnType&gt; <span class="keyword">operator</span>单目操作符();</span><br></pre></td></tr></table></figure><ul><li>全局函数重载：需要显式声明一个操作数</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">&lt;returnType&gt; <span class="keyword">operator</span>单目操作符(&lt;arg&gt;);</span><br></pre></td></tr></table></figure><h3 id="自增运算符重载">自增运算符重载</h3><p><code>a++</code> vs <code>++a</code></p><p>编译器是如何区分前缀运算和后缀运算的呢？</p><h4 id="虚拟参数（dummy-argument">虚拟参数（dummy argument)</h4><p>指定义的时候只写类型而不写形参名，告诉编译器这是一个虚拟参数只用来占位（和其它函数区分）而不作实际使用</p><p>示例，前缀++和后缀++的实现：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Counter</span> &#123;</span><br><span class="line">    <span class="type">int</span> value;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Counter</span>() &#123;value = <span class="number">0</span>;&#125;</span><br><span class="line">    Counter&amp; <span class="keyword">operator</span>++() &#123;</span><br><span class="line">        <span class="comment">// ++a, 前缀++</span></span><br><span class="line">        value++;</span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  Counter <span class="keyword">operator</span>++(<span class="type">int</span>) &#123; <span class="comment">// dummy argument</span></span><br><span class="line">        <span class="comment">// a++, 后缀++</span></span><br><span class="line">        Counter tmp = *<span class="keyword">this</span>;</span><br><span class="line">        value++;</span><br><span class="line">        <span class="keyword">return</span> tmp;</span><br><span class="line">    &#125;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="特殊操作符的重载">特殊操作符的重载</h2><h3 id="">=</h3><p>默认的赋值操作符重载函数：</p><ul><li>逐个成员赋值</li><li>对于对象成员，该赋值行为是递归进行的（即调用对应对象的赋值操作）</li></ul><p><strong>赋值操作符的重载不能继承</strong></p><ul><li>赋值操作的重载通常涉及深拷贝或资源管理，因此其实现必须严格控制源对象和目标对象的状态；而继承意味着<strong>派生类可能会有额外的资源</strong>，<strong>继承的重载赋值操作无法处理这些资源</strong>，因此需要显式定义</li><li>如果基类没有显式重载赋值操作符，那么派生类会生成一个默认的赋值操作符，但默认的赋值操作往往无法正确处理指针资源（<strong>因为是浅拷贝</strong>）</li></ul><p>派生类正确重载赋值操作符：</p><ul><li>调用基类赋值操作符</li><li>处理派生类新增成员的赋值和资源管理</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> x;</span><br><span class="line">    <span class="built_in">A</span>(<span class="type">int</span> i): <span class="built_in">x</span>(<span class="number">1</span>) &#123;&#125;</span><br><span class="line">A&amp; <span class="keyword">operator</span>=(<span class="type">const</span> A&amp; other) &#123;</span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span> != &amp;other) &#123;</span><br><span class="line">            <span class="comment">// 避免自赋值</span></span><br><span class="line">            x = other.x;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>: <span class="keyword">public</span> A &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span>* data;</span><br><span class="line">    <span class="built_in">B</span>(<span class="type">int</span> x, <span class="type">int</span> value): <span class="built_in">A</span>(x), <span class="built_in">data</span>(<span class="keyword">new</span> <span class="built_in">int</span>(value)) &#123;&#125;</span><br><span class="line">    B&amp; <span class="keyword">operator</span>=(<span class="type">const</span> B&amp; other) &#123;</span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span> != &amp;other) &#123;</span><br><span class="line">            A::<span class="keyword">operator</span>=(other);</span><br><span class="line">            <span class="comment">// 随便处理一下data代表资源管理</span></span><br><span class="line">            *data = *(other.data);</span><br><span class="line">        &#125;      </span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line">~<span class="built_in">B</span>() &#123;</span><br><span class="line">        <span class="keyword">delete</span> data;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">B <span class="title">b1</span><span class="params">(<span class="number">2</span>, <span class="number">3</span>)</span>, <span class="title">b2</span><span class="params">(<span class="number">3</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">    b2 = b1;</span><br><span class="line">    <span class="keyword">delete</span> b<span class="number">2.</span>data; <span class="comment">// 自定义赋值重载实现深拷贝，delete后不会出现悬挂指针</span></span><br><span class="line">    b<span class="number">2.</span>data = <span class="literal">nullptr</span>;</span><br><span class="line">    std::cout &lt;&lt; *b<span class="number">1.</span>data &lt;&lt; endl; <span class="comment">// 行为正常</span></span><br><span class="line">    <span class="comment">// 若在派生类B中没有显式重载赋值操作，那么默认的赋值会执行资源浅拷贝，b2的data指针和b1的data指针指向同一内存</span></span><br><span class="line">    <span class="comment">// 这将导致delete后出现悬挂指针</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>注：<em>处理</em></em>”赋值“<strong>类操作的时候一定要注意</strong>特判自赋值**情况！！！</p><h3 id="-2">[]</h3><p><code>[]</code>（下标运算符）的重载需要实现两个版本</p><p>示例:</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">string</span> &#123;</span><br><span class="line">  <span class="type">char</span>* p;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">string</span>(<span class="type">char</span>* p1) &#123;</span><br><span class="line">        p = <span class="keyword">new</span> <span class="type">char</span>[<span class="built_in">strlen</span>(p1) + <span class="number">1</span>];</span><br><span class="line">        <span class="built_in">strcpy</span>(p, p1);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 版本1</span></span><br><span class="line">    <span class="type">char</span>&amp; <span class="keyword">operator</span>[](<span class="type">int</span> i) &#123;</span><br><span class="line">        <span class="keyword">return</span> p[i];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 版本2</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span> <span class="keyword">operator</span>[](<span class="type">int</span> i) <span class="type">const</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> p[i];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">virtual</span> ~<span class="built_in">string</span>() &#123;<span class="keyword">delete</span>[] p;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">string <span class="title">s</span><span class="params">(<span class="string">&quot;abcd&quot;</span>)</span></span>;</span><br><span class="line">    s[<span class="number">2</span>] = <span class="string">&#x27;b&#x27;</span>; <span class="comment">// 调用版本1</span></span><br><span class="line">    std::cout &lt;&lt; s[<span class="number">2</span>]; <span class="comment">// 调用版本2</span></span><br><span class="line">    <span class="function"><span class="type">const</span> string <span class="title">cs</span><span class="params">(<span class="string">&quot;const&quot;</span>)</span></span>;</span><br><span class="line">    std::cout &lt;&lt; cs[<span class="number">0</span>]; <span class="comment">// 调用版本2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>若某个调用产生了两个候选函数，而一个有<code>const</code>一个没有，若调用处不修改对象，则优先选择带有<code>const</code>的函数，因为<code>const</code>保证不修改，选<code>const</code>函数使得更多代码可以在<code>const</code>上下文中使用，避免非<code>const</code>函数隐式修改。</p><p>注意到<code>char&amp; operator[](int i)</code>和<code>const char operator[](int i) const</code>的参数列表好像是一样的只有返回值不一样，应该会重载失败；但<strong>实际上两者的参数列表是不一样的</strong>，因为后者在函数定义前加上了<code>const</code>修饰，而类成员函数有一个隐藏参数<code>this</code>，加上了<code>const</code>后该隐藏参数会变为<code>const string* this</code>，而前者的隐藏参数仍是<code>string* this</code>, 因此两者的参数列表实际上是不一样的，可以重载成功。</p><h4 id="多维数组">多维数组</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Array2D</span> &#123;</span><br><span class="line">    <span class="type">int</span> n1, n2;</span><br><span class="line">    <span class="type">int</span> *p;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Array2D</span>(<span class="type">int</span> l, <span class="type">int</span> c): <span class="built_in">n1</span>(l), <span class="built_in">n2</span>(c) &#123;</span><br><span class="line">        p = <span class="keyword">new</span> <span class="type">int</span>[n1 * n2];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">virtual</span> ~<span class="built_in">Array2D</span>() &#123;<span class="keyword">delete</span>[] p;&#125;</span><br><span class="line"><span class="type">int</span>* <span class="keyword">operator</span>[](<span class="type">int</span> i) &#123;</span><br><span class="line">        <span class="keyword">return</span> p + i * n2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对于上述代码，可以通过像正常访问二维数组一样访问Array2D类型的变量</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">Array2D <span class="title">data</span><span class="params">(<span class="number">4</span>, <span class="number">5</span>)</span></span>;</span><br><span class="line">    std::cout &lt;&lt; data[<span class="number">2</span>][<span class="number">3</span>]; <span class="comment">// data[2]返回一个int*的指针，可以再通过[3]下标运算访问指针指向的空间（即数组的元素）</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 若再声明一个Array1D类型的类</span></span><br><span class="line"><span class="function"><span class="keyword">class</span> <span class="title">Array1D</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="type">int</span> *q;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Array1D</span>(<span class="type">int</span> *p) &#123;q = p;&#125;</span><br><span class="line">    <span class="type">int</span>&amp; <span class="keyword">operator</span>[](<span class="type">int</span> j) &#123;</span><br><span class="line">        <span class="keyword">return</span> q[j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Array2D</span> &#123;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// 将int* p换成如下内容</span></span><br><span class="line">    Array1D* data;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Array1D&amp; <span class="keyword">operator</span>[](<span class="type">int</span> j) &#123;</span><br><span class="line">        <span class="keyword">return</span> data[j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 则这样写之后除了可以通过data[i][j]访问Array2D的元素，还可以通过如下方式</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">Array2D <span class="title">data</span><span class="params">(<span class="number">4</span>, <span class="number">5</span>)</span></span>;</span><br><span class="line">    std::cout &lt;&lt; data.<span class="keyword">operator</span>[](<span class="number">2</span>).<span class="keyword">operator</span>[](<span class="number">3</span>); <span class="comment">// 若没有声明Array1D是不能这样写的，因为data.operator[](2)将返回一个基本类型指针而非对象</span></span><br><span class="line">    <span class="comment">// 而指针是不能通过.运算符访问成员的</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="-3">()</h3><p>作为优先级的<code>()</code>是不能重载的，因为重载不改变优先级；可以重载<strong>作为函数调用</strong>或<strong>类型转换</strong>的<code>()</code></p><h4 id="函数调用">函数调用</h4><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Func</span> &#123;</span><br><span class="line">    <span class="type">double</span> para;</span><br><span class="line">    <span class="type">int</span> lower, upper;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">double</span>, <span class="type">int</span>, <span class="type">int</span>)</span></span>; <span class="comment">// 重载函数调用运算符，给该类声明一个函数调用行为</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Func f;</span><br><span class="line">    <span class="built_in">f</span>(<span class="number">2.4</span>, <span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="类型转换">类型转换</h4><p>基本数据类型和自定义类都可以</p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Rational</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> n, d;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Rational</span>(<span class="type">int</span> n1, <span class="type">int</span> n2): <span class="built_in">n</span>(n1), <span class="built_in">d</span>(n2) &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">operator</span> <span class="title">double</span><span class="params">()</span> </span>&#123;<span class="keyword">return</span> (<span class="type">double</span>)n / d;&#125; <span class="comment">// ()前加上double和之前的函数调用重载进行区分</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">Rational <span class="title">r</span><span class="params">(<span class="number">1</span>, <span class="number">2</span>)</span></span>;</span><br><span class="line">    <span class="type">double</span> x = r; <span class="comment">// 自动触发类型转换(因为要从Rational-&gt;double), 返回1 / 2 = 0.5</span></span><br><span class="line">    x = x + r; <span class="comment">// 再次触发类型转换Rational-&gt;double，返回1 / 2 = 0.5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重载类型转换运算符<code>()</code>之后，在涉及需要类型转换的地方将自动按照对应的存在规则进行转换。从而可以减少混合计算中需要定义的操作符重载函数的数量。</p><h3 id="-4">-&gt;</h3><p>智能指针的实现必须重载该运算符；<code>-&gt;</code>是二元运算符，但是重载时按一元操作符存在描述，这是因为编译器并不关心<code>-&gt;</code>后面的东西是什么，只关心应该用<code>-&gt;</code>访问谁，即<code>-&gt;</code>前面的操作数。</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SmartPtr</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    T* ptr;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">SmartPtr</span>(T* p): <span class="built_in">ptr</span>(p) &#123;&#125;</span><br><span class="line">    T* <span class="keyword">operator</span>-&gt;() &#123;</span><br><span class="line">        <span class="keyword">return</span> ptr;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    A* a= <span class="keyword">new</span> A;</span><br><span class="line">    SmartPtr* p = <span class="keyword">new</span> <span class="built_in">SmartPtr</span>(a);</span><br><span class="line">    p-&gt;<span class="built_in">f</span>(); <span class="comment">// 实际上调用的是A::f</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即将所有<code>SmartPtr -&gt;</code> 都翻译成 <code>T* -&gt;</code></p><p>局限性：必须符合编译器控制的生命周期，即需要小心管理指针资源</p><h4 id="RAII">RAII</h4><p><strong>在初始化时分配资源，即构造函数中获取资源，析构函数中释放资源</strong>，避免裸资源（只有类自己持有该资源的引用/指针）泄漏。这样可以利用编译器控制的生命周期（<code>new</code>自动调用构造函数，<code>delete</code>自动调用析构函数）来管理资源，避免手动时的遗漏</p><h3 id="new-delete">new, delete</h3><p>什么时候需要重载new和delete</p><ul><li>频繁调用系统的存储管理，影响效率(e.g. 线程池)</li><li>程序自身管理内存，提高效率</li></ul><h4 id="重载方法">重载方法</h4><ul><li>调用系统存储分配（<code>malloc</code>）, 申请一块较大的内存</li><li>针对该内存，自己管理存储分配、去配</li><li>通过重载<code>new</code>和<code>delete</code>来实现</li><li>重载的<code>new</code>和<code>delete</code>是静态成员</li><li>重载的<code>new</code>和<code>delete</code>遵循类访问控制权限，可继承</li></ul><p>*注：*全局的<code>new</code>和<code>delete</code>不能重载，但为特定类定义的<code>new</code>与<code>delete</code>会在<code>new</code>/<code>delete</code>对应类的时候自动触发转换</p><h4 id="重载new">重载new</h4><ul><li><code>void* operator new(size_t size, ...)</code><ul><li>返回类型：<code>void*</code></li><li>第一个参数：<code>size_t</code>，系统自动计算对象的大小，并传值给size</li><li>其他参数：可有可无</li><li><code>A* p = new(...) A</code>, <code>...</code>表示传给new的其他参数；有一个隐藏参数<code>sizeof(A)</code>，即<code>A* a = new A</code>实际上是<code>A* a = new(sizeof(A)) A</code></li></ul></li><li>new的重载可以有多个</li><li>如果重载了new，那么通过new动态创建的类实例将不会再调用内置的默认new</li></ul><h4 id="一种new的用法：定位new（placement-new）">一种new的用法：定位new（placement new）</h4><p>定位<code>new</code>允许在已分配的内存上创建对象，用法：</p><p><code>new(place) Type</code>，place是已分配内存的地址（指针）</p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span>* memory = <span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(A));</span><br><span class="line">A* a = <span class="built_in">new</span>(memory) <span class="built_in">A</span>(); <span class="comment">// 调用A的默认构造函数</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">~<span class="built_in">a</span>(); <span class="comment">// 请显式调用析构函数而不是使用delete</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="built_in">free</span>(memory);</span><br></pre></td></tr></table></figure><p><strong>为什么要显式调用析构函数而不是使用delete？</strong></p><p>因为<code>delete</code>会尝试归还内存，而这个对象仅是在已申请的内存上创建的，<strong>我们可能只是想销毁对象而不是归还内存</strong>，以此避免使用<code>delete</code>而是显式调用析构函数。最后用<code>free</code>释放内存。</p><h4 id="重载delete">重载delete</h4><ul><li><code>void operator delete(void* p, size_t size)</code><ul><li>返回类型<code>void</code></li><li>第一个参数：<code>void*</code>, 表示被销毁对象的地址</li><li>第二个参数：可有可无，若有则必须是<code>size_t</code>类型，表示被撤销对象的大小</li></ul></li><li><code>delete</code><strong>的重载只能有一个</strong></li><li>如果重载了<code>delete</code>，那么通过<code>delete</code>销毁对象时将不再调用内置的默认<code>delete</code></li></ul><p><em>同样也有定位 delete</em></p><h2 id="模板">模板</h2><p>为了让模板在其他模块也能使用，C++一般把模块的完整定义写在头文件里（<strong>因为若模块B想要使用模块A的模板的一个实例，而模块A并未使用该实例，则模块B无法使用，编译错误</strong>）</p><p>模板（泛型）会进行类型检查，比宏安全</p><h2 id="异常捕获">异常捕获</h2><p><code>try</code>, <code>throw</code>, <code>catch</code></p><h3 id="throw">throw</h3><p><code>throw &lt;expr&gt;</code></p><p><code>throw</code>是把结果返回到上层调用者，所以一般需要进行<strong>值拷贝</strong></p><h3 id="catch">catch</h3><p><code>catch</code>完还在当前层，所以如果catch一个类（异常抛出一个类），一般写类引用，即<code>catch(Obj&amp;)</code></p><p>需要注意的是，<code>throw</code>的结果会一直向上层传，直到某一级被<code>catch</code>捕获，或者直到<code>main</code>都没有被捕获则程序<code>abort</code></p><p>因此若catch一个类（比如写了很多种不同的异常分别对应一个派生类，继承自异常基类），<strong>一般把基类写在最后，防止所有异常都被基类捕获了。</strong></p><h2 id="智能指针">智能指针</h2><p>智能指针多套一层可以很方便地管理资源（内存），因为可以在析构函数中主动释放管理的指针（内存），不必再在外部手动释放，符合<strong>RAII的思想</strong>。</p><h2 id="特殊-override-的技巧">特殊&quot;override&quot;的技巧</h2><p>全局函数或部分非虚成员函数没法override，可以写另一个“虚化接口”过渡，传入一个多态类，在该接口中调用多态成员函数（虚函数），从而实现多态。示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">B</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">D</span>: <span class="keyword">public</span> B &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局函数不能override，但是可以利用一个“虚化接口过渡</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(B* pb)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(B* pb)</span></span>; <span class="comment">// 虚化接口</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(B* pb)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="built_in">f</span>(pb);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(B* pb)</span> </span>&#123;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">    pb-&gt;<span class="built_in">f</span>(); <span class="comment">// 调用多态成员函数</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>更多例子：全局重定向运算符（<code>&lt;&lt;</code>，<code>&gt;&gt;</code>）的重写</p><h2 id="C-多态的坑">C++多态的坑</h2><p>数组多态容易出问题，<strong>主要是地址计算导致的</strong>。因为数组的访问往往通过下标运算符实现，即<code>start_addr + i * sizeof(Obj)</code>，但是<strong>派生类和基类的<code>sizeof</code>结果往往是不一样的</strong>，但是<strong>C++又是先编译再运行</strong>的语言且编译完成后不保留类型信息（不似<code>python</code>这种解释型语言在运行过程中获取具体类型信息），所以<code>sizeof(Obj)</code><strong>在编译时就确定了</strong>，<strong>后续涉及多态的地方只要派生类和基类的大小不一样就会出问题</strong>，<em>数组访问到错误的地址</em>。因此不要用数组存多态实例，而要使用支持多态的容器，比如<code>vector</code>。</p><h2 id="constexpr">constexpr</h2><p><code>constexpr</code>是C++11引入的关键字，随着C++14、C++17和C++20的发布该特性进一步增强。具体来说：</p><p><code>constexpr</code>告诉编译器这是一个常量，其值可以在编译时确定（类似编译器优化中的常量传播？）；例如可以配合enum使用、提高可读性的同时提升性能（提升性能不太清楚原因，我觉得<code>constexpr</code>关键字要求编译器做到的东西开-O2/-O3优化都能做到）</p><h2 id="function-库">&lt;function&gt;库</h2><p><code>std::function&lt;returnType(argsType...)&gt; f(functPtr | lambda);</code>可以声明一个函数，其功能由传入的<code>funcptr</code>或<code>lambda</code>表达式决定</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;function&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">std::function&lt;<span class="title">bool</span><span class="params">(<span class="type">int</span>, <span class="type">int</span>)</span>&gt; <span class="title">add</span><span class="params">([](<span class="type">int</span> a, <span class="type">int</span> b) &#123;</span></span></span><br><span class="line"><span class="params"><span class="function">       <span class="keyword">return</span> a + b; </span></span></span><br><span class="line"><span class="params"><span class="function">    &#125;)</span></span>;</span><br><span class="line">    <span class="type">int</span> c = <span class="built_in">add</span>(<span class="number">2</span>, <span class="number">3</span>); <span class="comment">// c = 5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="lambda表达式">lambda表达式</h2><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">[](<span class="type">int</span> a, <span class="type">int</span> b) &#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line">[&amp;](<span class="type">int</span> a, <span class="type">int</span> b) &#123; <span class="comment">// &amp;用于按引用捕获当前作用域的所有变量</span></span><br><span class="line">    <span class="keyword">return</span> a - b; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">5</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>&#125;;</span><br><span class="line"><span class="built_in">sort</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span> a, <span class="type">const</span> <span class="keyword">auto</span> b) &#123;</span><br><span class="line"><span class="keyword">return</span> a &lt; b; <span class="comment">// 按升序排列数组arr </span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="NULL-or-nullptr">NULL or nullptr</h2><p>考虑两个重载函数：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">char</span>*)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">f</span>(<span class="number">0</span>); <span class="comment">// 该调用哪个？有歧义</span></span><br></pre></td></tr></table></figure><p>如上的调用形式如果不加约定就会引发歧义，而<code>C</code>语言是通过<strong>宏</strong>定义的NULL，<strong>NULL就是整数0</strong>（C语言不存在重载也就没有歧义），但C++亟需解决这个问题，因此C++引入了新的空指针<code>nullptr</code>，并规定<code>nullptr</code>是<code>char *</code>类型的，即：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">f</span>(<span class="number">0</span>); <span class="comment">// =&gt; f(int)</span></span><br><span class="line"><span class="built_in">f</span>(<span class="literal">nullptr</span>); <span class="comment">// =&gt; f(char*)</span></span><br><span class="line"><span class="comment">// 而NULL还是0</span></span><br><span class="line"><span class="built_in">f</span>(<span class="literal">NULL</span>); <span class="comment">// =&gt; f(int)</span></span><br></pre></td></tr></table></figure><p>因此C++中使用空指针的地方尽量用<code>nullptr</code>而不是<code>NULL</code></p><p>(•‿•)</p>]]></content>
      
      
      <categories>
          
          <category> 软件学院 </category>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 面向对象 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++面向对象编程进阶I</title>
      <link href="/ymhui.github.io/2024/12/20/C-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6I/"/>
      <url>/ymhui.github.io/2024/12/20/C-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6I/</url>
      
        <content type="html"><![CDATA[<h1>C++面向对象编程进阶知识I</h1><p>参考南软2024秋C++高级程序设计课件CPP-2-2</p><h2 id="继承">继承</h2><p>派生类对应<code>Java</code>的子类，基类对应<code>Java</code>的父类</p><h3 id="单继承">单继承</h3><p>注意声明时不需要强调类之间的继承关系，定义时才需要，声明时带上继承关系会报<strong>编译错误</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// class Student: public Person; // 编译错误</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span>: <span class="keyword">public</span> Person &#123;</span><br><span class="line">  <span class="comment">// ...  </span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>继承方式有：<code>public, private, protected</code></p><h3 id="三种继承方式的差异">三种继承方式的差异</h3><table><thead><tr><th>继承方式</th><th>基类成员的访问权限</th><th>外部可访问性</th><th>使用场景</th></tr></thead><tbody><tr><td><strong>公有继承</strong></td><td>公有成员 -&gt; 公有，保护成员 -&gt; 保护，私有成员不可访问</td><td>继承的公有成员对外可见，保护成员不可见</td><td>表示派生类是基类的一种特殊类型，符合“is-a”关系</td></tr><tr><td><strong>保护继承</strong></td><td>公有成员 -&gt; 保护，保护成员 -&gt; 保护，私有成员不可访问</td><td>外部无法访问任何基类成员，内部可访问</td><td>用于继承实现，内部使用基类成员，不暴露给外部</td></tr><tr><td><strong>私有继承</strong></td><td>公有成员 -&gt; 私有，保护成员 -&gt; 私有，私有成员不可访问</td><td>外部无法访问任何基类成员，内部可访问</td><td>用于实现“has-a”关系，派生类是基类的一部分，但对外不可见</td></tr></tbody></table><p>示例</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> publicVar;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="type">int</span> protectedVar;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> privateVar;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; publicVar &lt;&lt; std::endl;  <span class="comment">// 可以访问公有成员</span></span><br><span class="line">        std::cout &lt;&lt; protectedVar &lt;&lt; std::endl;  <span class="comment">// 可以访问保护成员</span></span><br><span class="line">        <span class="comment">// std::cout &lt;&lt; privateVar &lt;&lt; std::endl;  // 错误，不能访问私有成员</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">protected</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; publicVar &lt;&lt; std::endl;  <span class="comment">// 可以访问公有成员</span></span><br><span class="line">        std::cout &lt;&lt; protectedVar &lt;&lt; std::endl;  <span class="comment">// 可以访问保护成员</span></span><br><span class="line">        <span class="comment">// std::cout &lt;&lt; privateVar &lt;&lt; std::endl;  // 错误，不能访问私有成员</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Derived d;</span><br><span class="line">    <span class="comment">// std::cout &lt;&lt; d.publicVar &lt;&lt; std::endl;  // 错误，不能访问基类的保护成员（基类的public到派生类变为protected）</span></span><br><span class="line">    <span class="comment">// std::cout &lt;&lt; d.protectedVar &lt;&lt; std::endl;  // 错误，不能访问保护成员</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">private</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; publicVar &lt;&lt; std::endl;  <span class="comment">// 可以访问公有成员</span></span><br><span class="line">        std::cout &lt;&lt; protectedVar &lt;&lt; std::endl;  <span class="comment">// 可以访问保护成员</span></span><br><span class="line">        <span class="comment">// std::cout &lt;&lt; privateVar &lt;&lt; std::endl;  // 错误，不能访问私有成员</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Derived d;</span><br><span class="line">    <span class="comment">// std::cout &lt;&lt; d.publicVar &lt;&lt; std::endl;  // 错误，不能访问基类的私有成员（基类的public到派生类变为private）</span></span><br><span class="line">    <span class="comment">// std::cout &lt;&lt; d.protectedVar &lt;&lt; std::endl;  // 错误，不能访问保护成员</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="派生类友元对基类成员的可访问性">派生类友元对基类成员的可访问性</h3><p>派生类的友元只能访问基类的<code>public</code>成员，无法访问<code>protected</code>和<code>private</code>的成员</p><p>示例</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="type">int</span> prot = <span class="number">1</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>: <span class="keyword">public</span> A &#123;</span><br><span class="line">    <span class="function"><span class="keyword">friend</span> <span class="type">void</span> <span class="title">func</span><span class="params">(B&amp; b)</span></span>; <span class="comment">// 可以访问B::prot</span></span><br><span class="line">    <span class="function"><span class="keyword">friend</span> <span class="type">void</span> <span class="title">func</span><span class="params">(A&amp; a)</span></span>; <span class="comment">// 不可访问A::prot</span></span><br><span class="line">    <span class="comment">// 事实上派生类的友元对基类的访问权限和一般的外部函数是一样的，取决于基类中成员的访问权限，即外部只能访问public成员</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(A&amp; a)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 不能访问A的私有和保护成员，只能访问公有成员</span></span><br><span class="line">    <span class="comment">// cout &lt;&lt; a.prot; 编译错误，外部不能访问受保护的成员prot</span></span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">(B&amp; b)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 该func可以访问B的所有成员变量，包括私有、保护、公有</span></span><br><span class="line">    cout &lt;&lt; b.prot; <span class="comment">// 可以访问</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="派生类对象的初始化">派生类对象的初始化</h3><ul><li>基类和派生类共同完成</li></ul><h3 id="构造函数的执行次序">构造函数的执行次序</h3><ul><li>基类构造函数</li><li>派生类对象成员类（成员变量是对象）的构造函数</li><li>派生类自己的构造函数</li></ul><h3 id="析构函数执行次序">析构函数执行次序</h3><ul><li>与构造函数相反</li></ul><h3 id="基类构造函数的调用">基类构造函数的调用</h3><ul><li>在派生类的构造函数中，若缺省对基类构造函数的调用则执行基类默认（无参）构造函数，若基类并未提供无参构造函数则会<strong>编译错误</strong></li><li>如果指定执行非默认构造函数，则必须在派生类的成员初始化表中显式指出，此时不在调用基类默认构造函数</li></ul><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line">  <span class="type">int</span> x;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">A</span>() &#123;x = <span class="number">0</span>;&#125;</span><br><span class="line">    <span class="built_in">A</span>(<span class="type">int</span> i): <span class="built_in">x</span>(i) &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>: <span class="keyword">public</span> A &#123;</span><br><span class="line">    <span class="type">int</span> y;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">B</span>() &#123;y = <span class="number">0</span>;&#125; <span class="comment">// 默认会先执行A()</span></span><br><span class="line">    <span class="built_in">B</span>(<span class="type">int</span> i): <span class="built_in">y</span>(i) &#123;&#125; <span class="comment">// 同理</span></span><br><span class="line">    <span class="built_in">B</span>(<span class="type">int</span> x_i, <span class="type">int</span> y_i): <span class="built_in">A</span>(x_i), <span class="built_in">y</span>(y_i) &#123;&#125; <span class="comment">// 显式调用基类A的带参构造函数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">B b1; <span class="comment">// A::A() =&gt; B::B() </span></span><br><span class="line"><span class="function">B <span class="title">b2</span><span class="params">(<span class="number">1</span>)</span></span>; <span class="comment">// A::A() =&gt; B::B(int)</span></span><br><span class="line"><span class="function">B <span class="title">b3</span><span class="params">(<span class="number">0</span>, <span class="number">1</span>)</span></span>; <span class="comment">// A::A(int) =&gt; B::B(int, int)</span></span><br></pre></td></tr></table></figure><h2 id="虚函数">虚函数</h2><p>在正式介绍虚函数前，先介绍一些C++中的重要概念，对后面正确理解静态与动态、判断多态很有帮助</p><h3 id="相容">相容</h3><p>相容分为两种：类型相容和赋值相容</p><h4 id="类型相容">类型相容</h4><p>类型相容是指两个类型之间是否能够在某些操作（如比较、函数调用、运算等）中互相使用。<strong>不意味着赋值操作能成功执行</strong>，仅指两个类型在语义上是兼容的，可以进行某些转换</p><ul><li>如果类型完全相同，自然是兼容的，比如<code>int</code>和<code>int</code></li><li>两个类型不同，但能够通过显式或隐式的转换来兼容，比如<code>int</code>和<code>double</code>；如果必须通过显式的类型转换才能兼容将会触发上面的特殊情况即赋值操作不能成功进行，示例：</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// B继承A</span></span><br><span class="line">A* a;</span><br><span class="line">a = <span class="keyword">new</span> B; <span class="comment">// 合法</span></span><br><span class="line">B* b;</span><br><span class="line"><span class="comment">// b = a; // 编译错误，不能隐式地从A* -&gt; B*，因此即使两者类型兼容，赋值操作并未成功</span></span><br><span class="line">b = <span class="built_in">static_cast</span>&lt;B*&gt;(a); <span class="comment">// 显式类型转换成功赋值</span></span><br></pre></td></tr></table></figure><h4 id="赋值相容">赋值相容</h4><p>赋值相容则是指是否可以把一个类型的值赋给另一个类型的变量。赋值相容关系通常取决于类型是否可以通过赋值语句进行转换。</p><ul><li>如果类型完全相同自然是相容的</li><li>类型不同，但可以通过显式或隐式的类型转换使得一个类型的变量可以赋给另一个类型，还是可以参考上例</li></ul><p>思考：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">A a;</span><br><span class="line">B b;</span><br><span class="line">a = b;</span><br></pre></td></tr></table></figure><p>什么时候上面的<code>a = b</code>是合法的？—— <strong>B是A的派生类，反过来不合法</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// b = a; // 编译错误</span></span><br></pre></td></tr></table></figure><p>尽管当B是A的派生类时赋值是合法的，但需要注意：</p><ul><li>对象的身份已经发生变化，即b赋值给a后，a和b并不等价（思考为什么，下面会放一张图解释）</li><li>对上一步的文字解释：属于派生类的属性不存在了（这是为什么？见图解）</li></ul><p>这种对象赋值操作称作<strong>切片</strong>，指a只保留了b的一部分属性，而这部分属性是基类和派生类共有的。</p><p>图解：</p><img src="/ymhui.github.io/2024/12/20/C-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6I/%E5%AF%B9%E8%B1%A1%E5%88%87%E7%89%87%E8%A7%A3%E9%87%8A.png" class="" title="对象切片解释"><p>考虑指针类型/引用类型的变量：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// B仍是A的派生类</span></span><br><span class="line">A* pa;</span><br><span class="line">B* pb = <span class="keyword">new</span> B;</span><br><span class="line">pa = pb; <span class="comment">// 不会发生切片，对象身份没有改变，pb指向的空间的对象还是那个对象没有变化，只不过多了个pa也指向它</span></span><br><span class="line"></span><br><span class="line">B b;</span><br><span class="line">A &amp;a = b; <span class="comment">// 取引用也是同理，对象身份没有变化，b还是那个b没有改变，只不过多了个引用a指向对象b，a是基类型的引用，不影响</span></span><br></pre></td></tr></table></figure><p>此外，总是可以把派生类直接赋给基类而不用做显式的类型转换，但是<strong>基类不能直接赋给派生类</strong>，必须做显示类型转换（可能出错，因此需要程序运行过程中进行适当的检查来保证）；形象地说就是：狗都是动物，但动物不全是狗。</p><h3 id="override何时发生？">override何时发生？</h3><p><strong>构造函数中无法使用多态</strong>，原因是当一个对象的构造函数被调用时，类的继承关系尚未完全建立，因此无法通过调用虚函数实现多态</p><p>看一个例子</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">A</span>() &#123;</span><br><span class="line">        <span class="built_in">call</span>();  <span class="comment">// 不会调用派生类的call()方法，而是调用A的call()方法</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">call</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;A&#x27;s call\n&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span> : <span class="keyword">public</span> A &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">call</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;B&#x27;s call\n&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    B b;  <span class="comment">// 输出 &quot;A&#x27;s call&quot;，而不是 &quot;B&#x27;s call&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A&#x27;s call</span><br></pre></td></tr></table></figure><p>该例可以解释为派生类构造时会先调用基类的构造函数，因此此时派生类尚未构造完成，内存找还找不到<code>B::call</code>的定义，而目前处于<code>A</code>的命空间内。只能找到<code>A::call</code>，因此调用的是<code>A::call</code></p><p>再看另一个例子</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line">    <span class="type">int</span> x, y;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>: <span class="keyword">public</span> A &#123;</span><br><span class="line"><span class="type">int</span> z;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">g</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func1</span><span class="params">(A&amp; a)</span> </span>&#123;</span><br><span class="line">    a.<span class="built_in">f</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">func2</span><span class="params">(A* pa)</span> </span>&#123;</span><br><span class="line">    pa-&gt;<span class="built_in">f</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    B b;</span><br><span class="line">    A &amp;a = b;</span><br><span class="line">    A* pa = &amp;b; <span class="comment">// A是B的基类，合法的</span></span><br><span class="line">    <span class="built_in">func1</span>(a); <span class="comment">// 调用的是A::f</span></span><br><span class="line">    <span class="built_in">func2</span>(pa); <span class="comment">// 调用的是A::f</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>为什么上例调用的都是<code>A::f</code>，明明传入的实例是<code>B</code>？ —— f并没有用<code>virtual</code>关键字修饰，编译器未把它解释成<strong>虚函数</strong>，因此执行<strong>静态绑定</strong>，<strong>编译时刻</strong>即与<code>A::f</code>绑定，因此不论后面传入A的哪个派生类，总是调用<code>A::f</code>。</p><p>下面开始正式介绍虚函数来解答上面的问题。</p><h3 id="两种绑定">两种绑定</h3><h4 id="前期（静态）绑定q">前期（静态）绑定q</h4><ul><li>编译时刻确定</li><li>依据对象的静态类型（即声明的类型）</li><li>效率高但灵活性差</li></ul><h4 id="动态绑定">动态绑定</h4><ul><li>运行时刻</li><li>依据对象的动态类型（即实际的类型）</li><li>灵活性高但效率低</li></ul><p>为了注重效率，C++默认使用前期绑定，若使用动态绑定需显式指出(<code>virtual</code>)</p><h3 id="定义虚函数">定义虚函数</h3><p><code>virtual</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>若<strong>基类</strong>中的成员函数被定义为<strong>虚函数</strong>，则派生类中所有<strong>override的该函数都是虚函数</strong>，从而可以继续被派生类重写。</p><h4 id="限制">限制</h4><ul><li>类的成员函数才可以是虚函数</li><li>静态成员函数<strong>不能</strong>是虚函数</li><li>内联成员函数<strong>不能</strong>是虚函数</li><li><strong>构造函数不能是虚函数</strong></li><li>析构函数可以是虚函数<strong>且往往是虚函数</strong></li></ul><h3 id="如何实现后期（动态）绑定？">如何实现后期（动态）绑定？</h3><h4 id="原理分析">原理分析</h4><p>对象的内存空间中存在一个指针，指向其对应的虚函数表，这个虚函数表指针位于<strong>对象首地址的前4字节中</strong>。</p><p>具体到一个对象指针p，p指向空间的前4字节中存放着指向对应虚函数表的指针，示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line">    <span class="type">int</span> x, y;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">g</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>: <span class="keyword">public</span> A &#123;</span><br><span class="line"><span class="type">int</span> z;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span> <span class="keyword">override</span></span>; <span class="comment">// override关键字不是必须的，但加上可以让编译器帮助检查是否真的重写了虚函数，更安全</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">g</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">A a;</span><br><span class="line">    B b;</span><br><span class="line">    <span class="comment">// case 1</span></span><br><span class="line">    A* p = &amp;a;</span><br><span class="line">    p-&gt;<span class="built_in">f</span>();</span><br><span class="line">    <span class="comment">// case 2</span></span><br><span class="line">    p = &amp;b;</span><br><span class="line">    p-&gt;<span class="built_in">f</span>();</span><br><span class="line">&#125;d</span><br></pre></td></tr></table></figure><p><code>p - 4</code>即取得虚函数表指针，这个虚函数表里记录两个运行时刻确定的虚函数的地址（动态绑定），对应实际指向的对象的虚函数，因为<code>f</code>在<code>g</code>前面定义所以虚函数表中f在g前面，虚函数表的首地址即指向<code>f</code></p><p>执行<code>p-&gt;f()</code>时，实际执行的是<code>(**((char*)p - 4))(p)</code>, 当<code>p</code>指向a时，该虚函数表指针就指向A的虚函数(<code>A::f, A::g</code>)，当p指向b时，该虚函数表指针就指向B的虚函数(<code>B::f, B::g</code>)。<strong>因此如果拿到对象的地址就可以根据地址向前偏移4B找到对应的虚函数表去调用具体的虚函数，找到的是哪一个虚函数完全由运行时刻赋予的具体的实际对象的地址情况决定，编译时刻不会绑定对象（也无法绑定因为编译时不知道地址），因此实现动态绑定</strong>。</p><h4 id="更多例子">更多例子</h4><p>再看另一个例子：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">A</span>() &#123;<span class="built_in">f</span>();&#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">g</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">h</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="built_in">f</span>();</span><br><span class="line">        <span class="built_in">g</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>: <span class="keyword">public</span> A &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">g</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    B b; <span class="comment">// A::A() =&gt; A::f() =&gt; B::B(), 因为执行A的构造函数中的f()时B的构造函数尚未执行完成(都没执行到)，因此只能找到A::f(), 还找不到B::f()</span></span><br><span class="line">    A* p = &amp;b;</span><br><span class="line">    p-&gt;<span class="built_in">f</span>(); <span class="comment">// B::f(), 动态绑定</span></span><br><span class="line">    p-&gt;<span class="built_in">g</span>(); <span class="comment">// A::g(), 没有virtual修饰，静态绑定</span></span><br><span class="line">    p-&gt;<span class="built_in">h</span>(); <span class="comment">// A::h() =&gt; B::f() =&gt; A::g(), 因为h执行的实际是this-&gt;f(); this-&gt;g(); this即p，是b的版本，其中f动态绑定，g静态绑定</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另一个例子:</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">g</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>: <span class="keyword">public</span> A &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span> </span>&#123; <span class="comment">// f()看似无参实际上有一个默认参数（隐藏）B* const this, p-&gt;f()调用时实际上是p-&gt;f(p)</span></span><br><span class="line">        <span class="built_in">g</span>(); <span class="comment">// 实际上是this-&gt;g(); 隐去了this</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">g</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    B b;</span><br><span class="line">    A* p = &amp;b;</span><br><span class="line">    p-&gt;<span class="built_in">f</span>(); <span class="comment">// B::f() =&gt; B::g(), 因为B::f的定义中实际上是this-&gt;g(); 省略了this，而this是指向对象b的指针，因此调用的是B版本的g, 即B::g</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="final与override">final与override</h3><p><code>final</code>关键字告诉编译器这个函数已经是“最终版本”了，不允许被派生类重写；尝试重写<code>final</code>修饰的函数会触发<strong>编译错误</strong></p><p><code>override</code>关键字在任何时候都不是必须的，但它告诉编译器这个函数的定义意图重写基类的虚函数，那么编译器会帮助做一些检查来保证重写是正确的（即函数签名完全一致），如果函数签名并不满足重写的要求，会触发<strong>编译错误</strong>。</p><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">B</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f1</span><span class="params">(<span class="type">int</span>)</span> <span class="type">const</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f2</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f3</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f5</span><span class="params">(<span class="type">int</span>)</span> <span class="keyword">final</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">D</span>: <span class="keyword">public</span> B &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f1</span><span class="params">(<span class="type">int</span>)</span> <span class="type">const</span> <span class="keyword">override</span></span>; <span class="comment">// 正确的重写，函数签名与基类虚函数完全一致</span></span><br><span class="line">    <span class="comment">// void f2(int) override; // 错误的重写，基类的虚函数f2没有带一个int参数的重载版本</span></span><br><span class="line">    <span class="comment">// void f3() override; // 错误的重写，基类f3不是虚函数</span></span><br><span class="line">    <span class="comment">// void f4() override; // 错误的重写，基类没有虚函数f4</span></span><br><span class="line">    <span class="comment">// void f5(int) override; // 错误的重写，尝试重写final函数</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="纯虚函数">纯虚函数</h3><ul><li>声明时在虚函数原型后面加上<code>= 0</code>, 意味着定义不在这里</li><li>往往只给出函数声明，不给定义（实现）</li></ul><h3 id="抽象类">抽象类</h3><ul><li>至少包含一个<strong>纯虚函数</strong></li><li>不能用于创建对象（类似<code>Java</code>抽象类<code>abstract class</code>），因此没有栈上对象，只有指针</li><li>为派生类提供框架，<strong>派生类必须为抽象基类的所有纯虚成员函数提供实现</strong></li></ul><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AbstractClass</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f</span><span class="params">()</span> </span>= <span class="number">0</span>; <span class="comment">// 纯虚函数</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DicretClass</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="comment">// ... 为抽象基类的纯虚函数提供实现</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>提供框架的意义：抽象类总结出出派生类的特征，提供共有的行为，设为纯虚，派生类按照各自特性<strong>对同一行为表现出不同动作</strong>，即<strong>为纯虚函数提供不同的具体实现</strong>。</p><p>利于一些设计模式的实现，比如抽象工厂模式、策略模式等</p><h3 id="虚析构函数">虚析构函数</h3><p><strong>析构函数往往是虚函数</strong>，只要在堆上申请了新资源。</p><p>因为如果基类析构函数不是虚函数，那么析构的时候就不会调用派生类的析构函数，如果<strong>派生类申请了超过基类持有的资源</strong>（指针），就会发生<strong>内存泄漏</strong>。</p><h3 id="好的继承习惯">好的继承习惯</h3><ul><li>确定public继承是真正意义上的&quot;<code>is a</code>&quot;关系</li><li>不要定义一个成员函数，它和继承而来的<strong>非虚</strong>成员函数同名。因为非虚函数是静态绑定的，同名没有意义，除非静态类型是派生类否则还是会调用基类的版本，并不会重写</li></ul><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Rectangle</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/*virtual*/</span> <span class="function"><span class="type">void</span> <span class="title">setHeight</span><span class="params">(<span class="type">int</span>)</span></span>; <span class="comment">// 加上virtual也不能改变仍可从外部访问派生类set方法的事实（继承而来）</span></span><br><span class="line">    <span class="comment">/*virtual*/</span> <span class="function"><span class="type">void</span> <span class="title">setWidth</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">height</span><span class="params">()</span> <span class="type">const</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">width</span><span class="params">()</span> <span class="type">const</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Square</span>: <span class="keyword">public</span> Rectangle &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setLength</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line"><span class="keyword">private</span>: <span class="comment">// 这样写是没有意义的，因为会从基类继承过来一个仍可被外部访问的set方法</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setHeight</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setWidth</span><span class="params">(<span class="type">int</span>)</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 除非声明类型是明确是Square类型，否则只要带上基类型Rectangle。就能在外部访问到set方法</span></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">B</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">mf</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">D</span>: <span class="keyword">public</span> B &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">mf</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    B b;</span><br><span class="line">    D d;</span><br><span class="line">    B* pd = &amp;d;</span><br><span class="line">    D* pb = &amp;b;</span><br><span class="line">    pd-&gt;<span class="built_in">mf</span>(); <span class="comment">// B::mf() </span></span><br><span class="line">    pb-&gt;<span class="built_in">mf</span>(); <span class="comment">// D::mf()</span></span><br><span class="line">    <span class="comment">// 没有virtual不是虚函数，都是静态绑定的，派生类与基类的非虚函数重名是没有意义的，并不会重写</span></span><br><span class="line">    <span class="comment">// 调用非虚函数的什么版本完全取决于编译时刻的静态绑定</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>明智地运用private继承，private继承后自己的派生类就继承不了自己基类中的方法了</li></ul><h3 id="函数种类汇总">函数种类汇总</h3><ul><li>纯虚函数<ul><li>只有函数接口会被继承</li><li>派生类必须继承函数接口</li><li>派生类必须提供实现代码</li></ul></li><li>一般虚函数<ul><li>函数的接口及缺省的实现代码都会被继承</li><li>子类必须继承函数接口</li><li>可以缺省实现代码</li></ul></li><li>非虚函数<ul><li>函数的代码及其实现代码都会被继承</li><li>必须同时继承接口和实现代码，无法重写以提供自己的实现</li></ul></li></ul><h3 id="切记">切记</h3><p>绝对不要重新定义继承而来的缺省参数值！考虑到：</p><ul><li>缺省参数值是静态绑定的</li><li>效率问题</li></ul><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">int</span> x=<span class="number">0</span>)</span> </span>= <span class="number">0</span>; <span class="comment">// 缺省参数</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>: <span class="keyword">public</span> A &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">int</span> x = <span class="number">1</span>)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">C</span>: <span class="keyword">public</span> A &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    A* p_a, *p_a1;</span><br><span class="line">    B b;</span><br><span class="line">    C c;</span><br><span class="line">    p_a = &amp;b;</span><br><span class="line">    p_a1 = &amp;c;</span><br><span class="line">    p_a-&gt;<span class="built_in">f</span>(); <span class="comment">// 0</span></span><br><span class="line">    p_a1-&gt;<span class="built_in">f</span>(); <span class="comment">// 0</span></span><br><span class="line">    <span class="comment">// 均输出0说明缺省参数值是静态绑定的，因为编译时刻编译器并不知道派生类型，因此缺省参数都是静态绑定，编译器在编译时刻只知道x = 0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="多继承">多继承</h2><p>一个常见的问题：名冲突</p><h3 id="语法">语法</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> &lt;派生类名&gt;: [&lt;继承方式&gt;]&lt;基类名<span class="number">1</span>&gt;, [&lt;继承方式&gt;]&lt;基类名<span class="number">2</span>&gt;, ... &#123;</span><br><span class="line">    &lt;成员表&gt;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>示例：</p><p><code>case1</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Furniture</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> weight;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setWeight</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bed</span>: <span class="keyword">public</span> Furniture &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">sleep</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Sofa</span>: <span class="keyword">public</span> Furniture &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">watchTV</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SleepSofa</span>: <span class="keyword">public</span> Bed, <span class="keyword">public</span> Sofa &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> Bed::weight;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">Bed::setWeight</span><span class="params">(<span class="type">int</span>)</span></span>; <span class="comment">// 不加Bed::会编译错误</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">foldOut</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">// 有一个问题：派生类SleepSofa有来自基类的两个版本的weight和setWeight, 如果不指明namespace的话会触发编译错误</span></span><br><span class="line">    <span class="comment">// 两个版本的weight: Bed::weigh, Sofa::weight</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>那么一个更好的实践是什么呢？—— 虚继承</p><h3 id="虚继承">虚继承</h3><p>在声明继承时使用<code>virtual</code>关键字，表示派生类继承的并不是一个“单独新”的基类，而是所有虚继承的派生类“共享”一个基类，使得继承树形成一个**“格”**的结构。</p><p>示例：</p><p><code>case2</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Furniture</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> weight;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setWeight</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bed</span>: <span class="keyword">virtual</span> <span class="keyword">public</span> Furniture &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">sleep</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Sofa</span>: <span class="keyword">public</span> <span class="keyword">virtual</span> Furniture &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">watchTV</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SleepSofa</span>: <span class="keyword">public</span> Bed, <span class="keyword">public</span> Sofa &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="comment">// 因为Bed和Sofa都是虚继承的Furniture，所以派生类SleepSofa只有一个版本的weight和setWeight</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">foldOut</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="本质">本质</h4><p>虚继承不会带来名冲突的本质：<strong>并没有真的继承基类的成员，而是生成了一个指向基类的指针</strong>，因此不会有名冲突问题（基类实际上只有一份，被所有虚继承的派生类共用）</p><h3 id="多继承总结">多继承总结</h3><h4 id="基类的声明次序将会决定：">基类的声明次序将会决定：</h4><ol><li>对基类构造函数/析构函数的调用次序，先声明的构造函数先调用，析构函数则相反</li><li>对基类数据成员的存储安排</li></ol><h4 id="名冲突问题">名冲突问题</h4><p>如果不使用虚继承可能会引入名冲突问题，即不同的命名域存在重名变量，编译器找不到该用哪个从而引起编译错误，解决方法是显式标明命名域，即<code>&lt;基类名&gt;::&lt;基类成员名&gt;</code>；或<strong>使用虚继承</strong></p><h4 id="非虚基类">非虚基类</h4><p>如果<strong>多继承</strong>派生类的<strong>直接基类</strong>有公共的基类，则该公共基类中的成员变量在多继承的派生类中有多个副本（直接基类对公共基类不使用虚继承，见上例<code>case1</code>）, 不标明namespace将引起上述的<strong>名冲突问题</strong></p><h4 id="虚继承-2">虚继承</h4><p>解决派生类中成员变量有多个基类副本的名冲突问题，且使得继承结构更优雅清晰</p><h4 id="虚基类">虚基类</h4><p><code>virtual</code>继承</p><ul><li>虚基类的构造函数由最新派生出的类（即继承树中最底层的类）的构造函数调用</li><li>虚基类的构造函数优先于非虚基类的构造函数执行</li></ul><p>(•‿•)</p>]]></content>
      
      
      <categories>
          
          <category> 软件学院 </category>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 面向对象 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++面向对象编程基础</title>
      <link href="/ymhui.github.io/2024/12/20/C-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"/>
      <url>/ymhui.github.io/2024/12/20/C-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1>C++面向对象编程基础知识</h1><p>参考南软2024秋C++高级程序设计课件 <em>CPP-2-1</em></p><h2 id="为什么使用面向对象">为什么使用面向对象</h2><h3 id="不使用面向对象的隐患">不使用面向对象的隐患</h3><ul><li>安全隐患：结构体的访问控制权是<strong>public</strong>的，这意味着其中所有变量外部可以自由访问，而某些状态应当只通过相关的函数调用更新</li><li>不符合数据类型定义：public权限带来的衍生问题，对外暴露出来的数据结构（实现）可能并不满足数据类型的定义/要求，如栈用数组实现，但数组对外暴露了，可在任意位置更新、操作而非栈顶、压栈、出栈。</li></ul><h3 id="使用面向对象">使用面向对象</h3><ul><li>对数据结构进行封装，各司其职，利于对象之间的协作和任务分解</li><li>信息隐藏，对外暴露必要的接口和功能，而非实现</li></ul><h2 id="面向对象的视角">面向对象的视角</h2><h3 id="结构化编程">结构化编程</h3><p>程序 = 算法 + 数据结构</p><h3 id="面向对象编程">面向对象编程</h3><p>程序 = <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∑</mo></mrow><annotation encoding="application/x-tex">\sum</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span></span></span></span>对象</p><p>对象 = 数据 + 行为（体现职责）</p><p>信息传递：函数调用</p><p>用类来管理</p><h2 id="面向对象体系下的项目开发流程">面向对象体系下的项目开发流程</h2><p>需求，架构，构建模式，代码，测试用例，项目组织</p><p>评价标准：开发效率</p><h2 id="封装">封装</h2><h3 id="类">类</h3><ul><li>成员变量</li><li>成员函数</li></ul><h4 id="默认提供的函数">默认提供的函数</h4><p>编译器会为类提供一些默认函数，只要你不显式地声明相关的函数</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Empty</span> &#123;</span><br><span class="line"><span class="comment">// Empty();</span></span><br><span class="line">    <span class="comment">// ~Empty();</span></span><br><span class="line">    <span class="comment">// Empty(const Empty&amp;);</span></span><br><span class="line">    <span class="comment">// Empty&amp; operator=(const Empty&amp;);</span></span><br><span class="line">    <span class="comment">// Empty* operator&amp;();</span></span><br><span class="line">    <span class="comment">// const Empty* operator&amp;() const;</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>共6个（除去常说的构造、析构、拷贝、赋值，还要注意会有两个版本的取地址操作符默认提供）</p><p>还要注意的是编译器只有在你<strong>用到相关函数的时候才会生成对应的默认版本</strong>（前提是没有显式声明），比如如果你不使用Empty去构造一个类就不会生成默认构造函数，其余函数同理</p><p>下面以<code>gcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04)，Target: x86_64-linux-gnu</code>的<code>g++</code>编译器为例，编译指令为</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ g++ tmp.cc -S -o tmp.s -O0</span><br></pre></td></tr></table></figure><p>比如不使用Empty构造类，则编译器没有生成对应的默认构造函数</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Empty</span>&#123;</span><br><span class="line">  <span class="type">int</span> a = <span class="number">1</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        .file   &quot;tmp.cc&quot;</span><br><span class="line">        .text</span><br><span class="line">        .globl  main</span><br><span class="line">        .type   main, @function</span><br><span class="line">main:</span><br><span class="line">.LFB0:</span><br><span class="line">        .cfi_startproc</span><br><span class="line">        endbr64</span><br><span class="line">        pushq   %rbp</span><br><span class="line">        .cfi_def_cfa_offset 16</span><br><span class="line">        .cfi_offset 6, -16</span><br><span class="line">        movq    %rsp, %rbp</span><br><span class="line">        .cfi_def_cfa_register 6</span><br><span class="line">        movl    $0, %eax</span><br><span class="line">        popq    %rbp</span><br><span class="line">        .cfi_def_cfa 7, 8</span><br><span class="line">        ret</span><br><span class="line">        .cfi_endproc</span><br><span class="line">.LFE0:</span><br><span class="line">        .size   main, .-main</span><br><span class="line">        .ident  &quot;GCC: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0&quot;</span><br><span class="line">        .section        .note.GNU-stack,&quot;&quot;,@progbits</span><br><span class="line">        .section        .note.gnu.property,&quot;a&quot;</span><br><span class="line">        .align 8</span><br><span class="line">        .long   1f - 0f</span><br><span class="line">        .long   4f - 1f</span><br><span class="line">        .long   5</span><br><span class="line">0:</span><br><span class="line">        .string &quot;GNU&quot;</span><br><span class="line">1:</span><br><span class="line">        .align 8</span><br><span class="line">        .long   0xc0000002</span><br><span class="line">        .long   3f - 2f</span><br><span class="line">2:</span><br><span class="line">        .long   0x3</span><br><span class="line">3:</span><br><span class="line">        .align 8</span><br><span class="line">4:</span><br></pre></td></tr></table></figure><p>反之生成了</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Empty</span>&#123;</span><br><span class="line">    <span class="type">int</span> a = <span class="number">1</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Empty* e = <span class="keyword">new</span> <span class="built_in">Empty</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.file   &quot;tmp.cc&quot;</span><br><span class="line">        .text</span><br><span class="line">        .section        .text._ZN5EmptyC2Ev,&quot;axG&quot;,@progbits,_ZN5EmptyC5Ev,comdat</span><br><span class="line">        .align 2</span><br><span class="line">        .weak   _ZN5EmptyC2Ev</span><br><span class="line">        .type   _ZN5EmptyC2Ev, @function</span><br><span class="line">_ZN5EmptyC2Ev:</span><br><span class="line">.LFB2:</span><br><span class="line">        .cfi_startproc</span><br><span class="line">        endbr64</span><br><span class="line">        pushq   %rbp</span><br><span class="line">        .cfi_def_cfa_offset 16</span><br><span class="line">        .cfi_offset 6, -16</span><br><span class="line">        movq    %rsp, %rbp</span><br><span class="line">        .cfi_def_cfa_register 6</span><br><span class="line">        movq    %rdi, -8(%rbp)</span><br><span class="line">        movq    -8(%rbp), %rax</span><br><span class="line">        movl    $1, (%rax)</span><br><span class="line">        nop</span><br><span class="line">        popq    %rbp</span><br><span class="line">        .cfi_def_cfa 7, 8</span><br><span class="line">        ret</span><br><span class="line">        .cfi_endproc</span><br><span class="line">.LFE2:</span><br><span class="line">        .size   _ZN5EmptyC2Ev, .-_ZN5EmptyC2Ev</span><br><span class="line">        .weak   _ZN5EmptyC1Ev</span><br><span class="line">        .set    _ZN5EmptyC1Ev,_ZN5EmptyC2Ev</span><br><span class="line">        .text</span><br><span class="line">        .globl  main</span><br><span class="line">        .type   main, @function</span><br><span class="line">main:</span><br><span class="line">.LFB0:</span><br><span class="line">        .cfi_startproc</span><br><span class="line">        endbr64</span><br><span class="line">        pushq   %rbp</span><br><span class="line">        .cfi_def_cfa_offset 16</span><br><span class="line">        .cfi_offset 6, -16</span><br><span class="line">        movq    %rsp, %rbp</span><br><span class="line">        .cfi_def_cfa_register 6</span><br><span class="line">        pushq   %rbx</span><br><span class="line">        subq    $24, %rsp</span><br><span class="line">        .cfi_offset 3, -24</span><br><span class="line">        movl    $4, %edi</span><br><span class="line">        call    _Znwm@PLT</span><br><span class="line">        movq    %rax, %rbx</span><br><span class="line">        movl    $0, (%rbx)</span><br><span class="line">        movq    %rbx, %rdi</span><br><span class="line">        call    _ZN5EmptyC1Ev</span><br><span class="line">        movq    %rbx, -24(%rbp)</span><br><span class="line">        movl    $0, %eax</span><br><span class="line">        movq    -8(%rbp), %rbx</span><br><span class="line">        leave</span><br><span class="line">        .cfi_def_cfa 7, 8</span><br><span class="line">        ret</span><br><span class="line">        .cfi_endproc</span><br><span class="line">.LFE0:</span><br><span class="line">        .size   main, .-main</span><br><span class="line">        .ident  &quot;GCC: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0&quot;</span><br><span class="line">        .section        .note.GNU-stack,&quot;&quot;,@progbits</span><br><span class="line">        .section        .note.gnu.property,&quot;a&quot;</span><br><span class="line">        .align 8</span><br><span class="line">        .long   1f - 0f</span><br><span class="line">        .long   4f - 1f</span><br><span class="line">        .long   5</span><br><span class="line">0:</span><br><span class="line">        .string &quot;GNU&quot;</span><br><span class="line">1:</span><br><span class="line">        .align 8</span><br><span class="line">        .long   0xc0000002</span><br><span class="line">        .long   3f - 2f</span><br><span class="line">2:</span><br><span class="line">        .long   0x3</span><br><span class="line">3:</span><br><span class="line">        .align 8</span><br><span class="line">4:</span><br></pre></td></tr></table></figure><h3 id="模块">模块</h3><p><code>foo.cppm</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">module</span> M; <span class="comment">// 导出为模块M</span></span><br><span class="line"><span class="keyword">import</span> K; <span class="comment">// 同时依赖模块K</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">export</span> <span class="type">int</span> <span class="title">func</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> + <span class="built_in">square</span>(x);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>foo2.cppm</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">module</span> K; <span class="comment">// 导出为模块K</span></span><br><span class="line"><span class="comment">// 不依赖任何其他模块</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">export</span> <span class="type">int</span> <span class="title">square</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x * x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>bar.cpp</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> M; <span class="comment">// 依赖模块M</span></span><br><span class="line"><span class="keyword">import</span> K; <span class="comment">// 依赖模块K</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">square</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="built_in">func</span>(<span class="number">5</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>clang</code>中的模块单元在 <code>.cppm</code></p><p>编译后输出</p><ul><li>可导入的<code>pcm</code></li><li>可链接的对象文件</li></ul><p>根据上述依赖关系，编译顺序为K =&gt; M =&gt; …</p><h3 id="构造函数">构造函数</h3><p>与<code>java</code>一致，默认构造函数无参，支持重载</p><p>且</p><ul><li>当类中不实现任何构造函数，编译器会提供默认构造函数（各种）</li><li>一旦定义任何一种构造函数，编译器不再提供默认（<strong>五三原则</strong>的背景），但可以手动<code>=default</code></li></ul><p>构造函数既可以是public的也可以是private的；private的构造函数旨在类自己接管对象创建（如单例模式）</p><h4 id="五三原则">五三原则</h4><p>事实上C++11为三原则(Rules of Three)，<strong>C++17引入了移动</strong>的语义，因此扩展到了<strong>五原则</strong>(Rules of Five)</p><p><strong>规定</strong>：（事实上是一种好的编程实践）若在类定义时重载了<strong>拷贝构造函数、拷贝赋值运算符、析构函数、移动构造函数、移动赋值运算符</strong>之一或之几，也要主动重载剩余的，即<strong>牵一发而动全身</strong>。</p><h4 id="调用">调用</h4><p>创建对象时自动调用</p><p>示例</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Obj</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Obj</span>();</span><br><span class="line">    <span class="built_in">Obj</span>(<span class="type">int</span> i);</span><br><span class="line">    <span class="built_in">Obj</span>(<span class="type">char</span>* p);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Obj o1 = <span class="built_in">Obj</span>(<span class="number">1</span>); <span class="comment">// &lt;=&gt; Obj o1(1); &lt;=&gt; Obj o1 = 1; </span></span><br><span class="line">    Obj o2 = <span class="built_in">Obj</span>(); <span class="comment">// 不能写成 Obj o2();</span></span><br><span class="line">    Obj o3 = <span class="built_in">Obj</span>(<span class="string">&quot;abcd&quot;</span>); <span class="comment">// &lt;=&gt; Obj o3(&quot;abcd&quot;); &lt;=&gt; Obj o3 = &quot;abcd&quot;;</span></span><br><span class="line">    Obj a[<span class="number">4</span>]; <span class="comment">// 从a[0] ~ a[3], 自动调用Obj()</span></span><br><span class="line">    Obj b[<span class="number">5</span>] = &#123;<span class="built_in">Obj</span>(), <span class="built_in">Obj</span>(<span class="number">1</span>), <span class="built_in">Obj</span>(<span class="string">&quot;abcd&quot;</span>), <span class="number">114</span>, <span class="string">&quot;514&quot;</span>&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>注</em>：形如<code>Obj o1 = 1; </code>这种写法仅限于构造函数没有被 <code>explicit</code>关键字修饰，因为这种写法实际上发生了<strong>隐式类型转换</strong>，自动触发了隐式构造函数调用，从整型1转成了对象。</p><h4 id="成员初始化表">成员初始化表</h4><ul><li>对构造函数的补充</li><li>会先于构造函数体被执行</li><li><strong>建议按类成员的声明顺序进行初始化</strong>，可以减轻编译器的负担（在较新版的CLion里不按顺序会警告）</li></ul><p>示例</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line">    <span class="type">int</span> x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> y;</span><br><span class="line">    <span class="type">int</span>&amp; z;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">A</span>(): <span class="built_in">x</span>(<span class="number">114</span>), <span class="built_in">y</span>(<span class="number">514</span>), <span class="built_in">z</span>(x) &#123; <span class="comment">// &lt;- 成员初始化表</span></span><br><span class="line">        x = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>如果类中持有其他对象，则在构造函数执行时会逐级调用成员对象的构造函数（若在构造函数中初始化成员对象）</p><p>当成员变量不多时，建议使用成员初始化表代替赋值操作</p><h5 id="使用成员初始化表的缘由">使用成员初始化表的缘由</h5><ul><li>const成员变量和引用成员变量<strong>必须</strong>在成员初始化表中初始化，而不能在构造函数体中进行赋值</li><li>派生类初始化基类也必须在成员初始化表中完成（<code>:</code>后面，<code>&#123;</code>前面）</li><li>效率高</li><li>例外：成员变量过多，再使用成员初始化表会降低可读性和可维护性</li></ul><h4 id="析构函数">析构函数</h4><p>析构函数会在对象消亡时自动调用，一般需要程序员在其中主动释放对象持有的非栈内存资源（如指针指向的<strong>堆内存</strong>）</p><p>和构造函数一样可以是public也可以是private</p><h5 id="私有析构函数">私有析构函数</h5><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">A</span>();</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">destroy</span><span class="params">()</span> </span>&#123;<span class="keyword">delete</span> <span class="keyword">this</span>;&#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    ~<span class="built_in">A</span>();</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>当析构函数被private修饰，则不能通过形如<code>A a;</code>的形式在栈上创建对象（私有构造函数应该也是同理），因为此时的析构函数为私有，外部无法访问到，编译器无法实现在对象生命周期结束时自动释放对象，因此会触发<strong>编译错误</strong>。因此只能通过<code>A* a = new A;</code>形式创建。</p><p>同理，直接<code>delete a</code>也是会触发<strong>编译错误</strong>的，同样是因为析构函数为私有，外部作用域访问非法。因此类内部必须对外实现一个public的接口，并在接口实现中主动调用<code>delete</code>（类内部作用域可访问私有成员），防止内存泄漏。</p><p>一个更好的实践：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="function"><span class="type">static</span> <span class="type">void</span> <span class="title">free</span><span class="params">(A* p)</span></span>;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    ~<span class="built_in">A</span>();</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">static</span> <span class="type">void</span> <span class="title">A::free</span><span class="params">(A* p)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">delete</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    A* a = <span class="keyword">new</span> <span class="built_in">A</span>();</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    A::<span class="built_in">free</span>(a);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="GC-vs-RAII">GC vs RAII</h5><p>众所周知Java使用一种叫做垃圾回收（Garbage Collection）的机制管理和释放资源，但这也导致了Java运行的效率瓶颈；而C++采用一种叫做RAII（Resource Acquisition Is Initialization）的机制让程序员自行实现资源的分配（构造函数中）与释放（析构函数中），再由编译器管理的构造与析构函数集中处理。</p><h4 id="拷贝构造函数">拷贝构造函数</h4><p>创建对象时，传入同类对象对其进行初始化，往往用来实现对象<strong>深拷贝</strong>操作。</p><p>也是自动调用的（创建对象时自动匹配重载版本）</p><h5 id="浅拷贝">浅拷贝</h5><p>不论变量的类型，只是对变量值进行一份拷贝，这对于值类型无可厚非，但对于指针变量，浅拷贝只会复制指针的值（即地址），而非其中的内容，这将导致多个指针指向同一块内存，一旦通过其中某个指针释放了内存（本意可能只是释放特定的指针），所有剩余的指针都将悬挂，造成<strong>悬挂指针隐患</strong>。</p><h5 id="深拷贝">深拷贝</h5><p>对于值类型变量仍是拷贝一份值存储，而对于指针类型变量，则会额外<strong>开辟一块新内存</strong>，并将旧指针指向地址中的数据拷贝到新内存中。</p><h5 id="默认的拷贝构造函数">默认的拷贝构造函数</h5><ul><li>默认是浅拷贝的</li><li>逐个成员初始化</li><li>若存在成员对象，则递归地执行上述步骤，即对每个成员对象也调用其拷贝构造函数</li></ul><h5 id="什么时候需要实现拷贝构造函数">什么时候需要实现拷贝构造函数</h5><p>深拷贝防止浅拷贝引入的悬挂指针风险。</p><h4 id="移动构造函数（C-17）">移动构造函数（C++17）</h4><h5 id="右值的概念">右值的概念</h5><p>声明参数时使用<code>&amp;&amp;</code>告知编译器这里要传一个右值参数，与左值不同，右值只有本身的值，不能被赋值。</p><p>示例</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">string::<span class="built_in">string</span>(string &amp;&amp;s): <span class="built_in">p</span>(s.p) &#123;</span><br><span class="line">    s.p = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>*注：*上例中的<code>s</code>虽然在传参时是右值，但是传完参之后的<code>s</code>是一个<strong>左值</strong>，存储的是一个右值，如果需要把它作为右值传给其他需要右值的函数，必须使用<code>std::move()</code>获取右值。</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">process</span><span class="params">(string &amp;&amp;s)</span> </span>&#123;</span><br><span class="line"><span class="comment">//...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">string::<span class="built_in">string</span>(string &amp;&amp;s) &#123;</span><br><span class="line">    <span class="comment">// process(s); 会触发编译错误</span></span><br><span class="line">    <span class="built_in">process</span>(std::<span class="built_in">move</span>(s));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="为什么需要移动构造函数？">为什么需要移动构造函数？</h5><ul><li>性能提升：可以高效的转移资源，不同于深拷贝的逐字节复制，移动操作直接转移资源的所有权，以<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span>时间复杂度高效完成</li><li>内存管理：转移资源所有权避免了重复开辟新内存，避免了不必要的空间开销和内存浪费，简化资源的管理（不需要管理多份相同资源）</li><li>支持标准库容器的优化，比如<code>vector</code>的扩容</li><li>返回值优化——避免不必要的内存分配，当在函数中返回一个局部对象时，C++11引入的<strong>返回值优化</strong>（RVO）和<strong>移动语义</strong>（通过移动构造函数和移动赋值运算符）可以避免不必要的内存分配。</li></ul><p>返回值优化示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">MyString <span class="title">createString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">MyString <span class="title">str</span><span class="params">(<span class="string">&quot;Hello, world!&quot;</span>)</span></span>;</span><br><span class="line">    <span class="keyword">return</span> str; <span class="comment">// 返回时可能触发移动构造函数, 避免调用拷贝构造函数创建新的返回值对象，造成对str的拷贝和内存的重新分配(浪费)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>简化代码</li></ul><h4 id="动态内存">动态内存</h4><p>动态内存分配在堆上</p><p><strong>C:</strong></p><ul><li>malloc</li><li>free</li></ul><p><strong>C++:</strong></p><ul><li>new</li><li>delete</li></ul><h4 id="动态对象">动态对象</h4><h5 id="创建">创建</h5><p>在堆上通过<code>new</code>创建，并通过<code>delete</code>主动释放其内存。其中<code>new</code>绑定constructor，<code>delete</code>绑定destructor。</p><p>示例</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">A</span>();</span><br><span class="line">    <span class="built_in">A</span>(<span class="type">int</span> i);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    A* p, *q;</span><br><span class="line">    p = <span class="keyword">new</span> A; <span class="comment">// &lt;=&gt; p = new A();</span></span><br><span class="line">    q = <span class="keyword">new</span> <span class="built_in">A</span>(<span class="number">114</span>);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">delete</span> p; <span class="comment">// 调用p所指向对象的析构函数，释放对象空间</span></span><br><span class="line">    <span class="keyword">delete</span> q;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>C++支持对基础类型也是用new（完全替代C的<code>malloc</code>）来进行动态内存分配</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span>* ptr = (<span class="type">int</span>*) <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">int</span>));</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span>* ptr = <span class="keyword">new</span> <span class="type">int</span>; <span class="comment">// 两种写法对于基本类型是等价的</span></span><br></pre></td></tr></table></figure><p>*注：*无论存储的数据占多大的空间，指针本身的大小都是一样的(<code>4B</code>)</p><p><code>C</code>风格的<strong>malloc不会调用构造函数</strong>（<code>new</code>会），只是分配一块指定对象类型的空间，并不会进行初始化；同理<strong>free也不会调用析构函数</strong>（<code>delete</code>会），只是释放指定对象被分配的空间，不会释放其进一步占用的资源。</p><h5 id="删除">删除</h5><p><code>delete ptr;</code></p><p>需要注意的是，<code>delete</code>只是释放ptr指向的内存被分配出去的空间，即这块空间不再是被分配的状态，不再属于ptr（不再是合法空间），而ptr指针本身的值没有发生改变，即仍指向这块不再属于自己的空间，成为<strong>悬挂指针</strong>。因此，一个<strong>安全的编程习惯</strong>是：<code>delete</code>掉一个对象后把对应的指针置为<code>nullptr</code>。</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> ptr1;</span><br><span class="line">ptr1 = <span class="literal">nullptr</span>; <span class="comment">// 释放后置为nullptr</span></span><br><span class="line"><span class="keyword">delete</span> ptr2;</span><br><span class="line">ptr2 = <span class="literal">nullptr</span>;</span><br></pre></td></tr></table></figure><h5 id="动态对象数组">动态对象数组</h5><p>创建：<code>A *p = new A[100];</code></p><p>删除：<code>delete[] p; // []不可省</code></p><p><em>注意以下几点：</em></p><ul><li>p是一个指针（数组的首地址），p指向的数组空间的每个元素是具体的对象<code>A</code>, 不是指针</li><li>这种写法无法显式初始化，相应的类<code>A</code>必须提供默认（无参）构造函数，如果没有会触发<strong>编译错误</strong></li><li><code>delete</code>中的<code>[]</code>不能省</li></ul><h6 id="delete时-不能省的原因">delete时[]不能省的原因</h6><p><code>delete</code>期望释放分配出去的所有空间，但指针本身不记录数组的大小信息，编译器仅从<code>delete</code>和<code>ptr</code>无法知道应该释放多少内存。而数组空间在分配时会在指针指向的首地址前预留<code>4B</code>的内容，其中存了数组的大小，而<code>[]</code>会告诉编译器去找指针前4字节的内容，确认要释放多少内存。</p><p>二维数组分配时先分配行，再给每一行分配列；释放时先释放各行的每一列，再释放每一行（和分配反过来）</p><h4 id="const成员">const成员</h4><p>const成员属性的初始化在构造函数的成员初始化表中进行</p><p>注意const修饰的位置：</p><ul><li><code>const A*</code>是指针指向空间的值不能变</li><li><code>A* const</code>是指针本身的值不改变（常量指针）</li></ul><h5 id="const成员函数">const成员函数</h5><p>函数在声明时被const修饰，代表该函数中不会改变任何变量的值，只读</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="type">int</span> x, y;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="built_in">A</span>(<span class="type">int</span> a, <span class="type">int</span> b): <span class="built_in">x</span>(a), <span class="built_in">y</span>(b) &#123;&#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">show</span><span class="params">()</span> <span class="type">const</span></span>; <span class="comment">// show是const成员函数，里面不可修改成员</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">A::f</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    x = <span class="number">1</span>;</span><br><span class="line">    y = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">A::show</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">    std::cout &lt;&lt; x &lt;&lt; y &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于上述代码编译器如何检查被const修饰的成员函数的定义是否合法呢？编译器会对const修饰的函数中所有变量都带上const，这样若函数定义中尝试写一个变量，就会触发<strong>编译错误</strong></p><p>继续上述代码，若定义了一个常量对象<code>a</code></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">const</span> A <span class="title">a</span><span class="params">(<span class="number">0</span>, <span class="number">0</span>)</span></span>; <span class="comment">// a是常量对象，其中的所有成员变量都会被编译器自动带上const</span></span><br></pre></td></tr></table></figure><p>若对象在声明时被const修饰，则这个对象中所有成员变量都会自动加上const（编译器来做，同时做后续对const的检查），因此<code>a</code>将<strong>只能调用类中的const成员函数</strong>，因为编译器无法确定非const函数是否会”写“成员变量，但const成员函数一定不会，因此编译器只允许调用const函数确保const的变量不被写，而<strong>非const的调用</strong>会触发<strong>编译错误</strong></p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// a.f(); 编译错误</span></span><br><span class="line">a.<span class="built_in">show</span>(); <span class="comment">// 可以通过编译</span></span><br></pre></td></tr></table></figure><h4 id="静态成员">静态成员</h4><p>既可以通过对象使用，也可以直接通过类名作用域来使用</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">A a;</span><br><span class="line">a.<span class="built_in">f</span>();</span><br><span class="line">A::<span class="built_in">f</span>();</span><br></pre></td></tr></table></figure><p>C++支持观点：类也是对象</p><h5 id="静态成员变量">静态成员变量</h5><p>所有类的示例共享同一个静态成员变量。</p><p>为什么要引入静态成员——同一个类的不同对象如何共享变量？</p><ul><li>如果使用全局变量，缺乏数据保护</li><li>且会造成名污染</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line">    <span class="type">int</span> x, y;</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> shared; <span class="comment">// 内部声明</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> A::shared = <span class="number">0</span>; <span class="comment">// 外部定义来初始化</span></span><br></pre></td></tr></table></figure><ul><li>类对象共享</li><li>唯一拷贝</li><li>遵循类访问控制（public，protected，private）</li></ul><p>需要注意的是，静态成员的定义必须在外部不能在内部（<code>const</code>除外），因为静态变量在编译器眼中类似全局常量，全局常量不能重定义（内部定义静态变量会有重定义问题，比如反复include多次），因此静态成员变量在类内声明，类外定义；但如果是<code>static const</code>，还是要在内部定义（<code>const</code>要求）。</p><h5 id="静态成员函数">静态成员函数</h5><p><code>static</code>修饰函数</p><ul><li>只能存取静态成员变量，调用静态成员函数</li><li>遵循类访问控制</li></ul><h4 id="友元">友元</h4><p><code>friend</code>关键字，使外在友元的定义中也能访问类私有成员。不使用友元就只能通过public访问成员或对private成员提供getter/setter，降低了对private成员的访问效率。</p><h5 id="分类">分类</h5><ul><li>友元函数</li><li>友元类</li><li>友元类成员函数</li></ul><p>示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">func</span><span class="params">()</span></span>;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>; <span class="comment">// 前向声明，在此处只声明一个类B，告诉编译器有这么一个类，但是不定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">C</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="comment">//...</span></span><br><span class="line">    <span class="function"><span class="keyword">friend</span> <span class="type">void</span> <span class="title">func</span><span class="params">()</span></span>; <span class="comment">// 友元函数</span></span><br><span class="line">    <span class="keyword">friend</span> <span class="keyword">class</span> <span class="title class_">B</span>;<span class="comment">// 友元类</span></span><br><span class="line">    <span class="function"><span class="keyword">friend</span> <span class="type">void</span> <span class="title">C::f</span><span class="params">()</span></span>; <span class="comment">// 友元类成员函数</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>对上例的解释：</p><ol><li><code>B</code>的前向声明可以省略，因为<code>B</code>的作用域是全局，此处的<code>friend</code>可帮助在全局声明</li><li><code>friend</code>类似<code>extern</code>（但并不是），因此func可以不必在之前先声明（类似全局函数的效果，<code>friend</code>帮助其将作用域扩展到全局），因为<code>friend void func();</code>相当于声明了，可以在之后定义</li><li>但<code>C::f()</code>必须先在C中声明，不然编译器找不到<code>C::f</code>的声明会编译错误（因为<code>C::f</code>不是全局函数而是类的某个函数，<code>friend</code>做不到像<code>extern</code>那样扩展全局函数的声明范围）</li><li><code>func</code>，<code>B</code>，<code>C::f</code>均能访问A的私有成员</li></ol><p>友元不具有传递性，即A是B的友元，B是C的友元，但A不是C的友元</p><p>另一个示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vector</span>; <span class="comment">// 此处不加前向声明会触发编译错误，因为下面的代码存在循环依赖，谁在前面顶哟都会缺少对方的声明，因此前向声明打破循环依赖</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  <span class="function"><span class="keyword">friend</span> <span class="type">void</span> <span class="title">multiply</span><span class="params">(Matrix&amp; m, Vector&amp; v, Vector&amp; r)</span></span>; <span class="comment">// 如果这里不用引用Vector&amp;而是直接用具体的对象类型Vector，即使有前向声明也会编译错误，因为前向声明只是声明了Vector但没有定义，编译器不知道Vector多大，也就不知道要在编译时分配多大的空间（编译是静态的），自然会编译错误；而引用的大小固定，所以不会出错。</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Vector</span> &#123;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">    <span class="function"><span class="keyword">friend</span> <span class="type">void</span> <span class="title">multiply</span><span class="params">(Matrix&amp; m, Vector&amp; v, Vector&amp; r)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>(•‿•)</p>]]></content>
      
      
      <categories>
          
          <category> 软件学院 </category>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 软件学院 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++常用二分库函数</title>
      <link href="/ymhui.github.io/2024/12/19/C-%E5%B8%B8%E7%94%A8%E4%BA%8C%E5%88%86%E5%BA%93%E5%87%BD%E6%95%B0/"/>
      <url>/ymhui.github.io/2024/12/19/C-%E5%B8%B8%E7%94%A8%E4%BA%8C%E5%88%86%E5%BA%93%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="容器相关二分">容器相关二分</h2><h3 id="在有序容器中找第一个大于或大于等于指定元素的迭代器">在有序容器中找第一个大于或大于等于指定元素的迭代器</h3><p>第一个大于：<code>upper_bound</code></p><p>第一个大于等于：<code>lower_bound</code></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">1</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">20</span>, <span class="number">23</span>, <span class="number">55</span>, <span class="number">78</span>, <span class="number">98</span>, <span class="number">100</span>&#125;;</span><br><span class="line">    <span class="keyword">auto</span> it = <span class="built_in">upper_bound</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), <span class="number">8</span>);</span><br><span class="line">    cout &lt;&lt; <span class="built_in">distance</span>(arr.<span class="built_in">begin</span>(), it) &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; *it &lt;&lt; endl;</span><br><span class="line">    it = <span class="built_in">upper_bound</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), <span class="number">23</span>);</span><br><span class="line">    cout &lt;&lt; <span class="built_in">distance</span>(arr.<span class="built_in">begin</span>(), it) &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; *it &lt;&lt; endl;</span><br><span class="line">    it = <span class="built_in">lower_bound</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), <span class="number">23</span>);</span><br><span class="line">    cout &lt;&lt; <span class="built_in">distance</span>(arr.<span class="built_in">begin</span>(), it) &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; *it &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2: 9</span><br><span class="line">8: 55</span><br><span class="line">7: 23</span><br></pre></td></tr></table></figure><h3 id="同时找第一个大于和大于等于的元素">同时找第一个大于和大于等于的元素</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">1</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">20</span>, <span class="number">23</span>, <span class="number">55</span>, <span class="number">78</span>, <span class="number">98</span>, <span class="number">100</span>&#125;;</span><br><span class="line">    <span class="keyword">auto</span> p = <span class="built_in">equal_range</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), <span class="number">55</span>);</span><br><span class="line">    cout &lt;&lt; <span class="built_in">distance</span>(arr.<span class="built_in">begin</span>(), p.first) &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; <span class="built_in">distance</span>(arr.<span class="built_in">begin</span>(), p.second);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">8 9</span><br></pre></td></tr></table></figure><h3 id="在容器中找最大值和最小值">在容器中找最大值和最小值</h3><p>二分查找：<code>max_element</code>, <code>min_element</code>, 返回的是对应元素的指针</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">1</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">15</span>, <span class="number">10</span>, <span class="number">16</span>, <span class="number">120</span>, <span class="number">23</span>, <span class="number">89</span>, <span class="number">78</span>, <span class="number">98</span>, <span class="number">100</span>&#125;;</span><br><span class="line">    <span class="keyword">auto</span> mmax = *<span class="built_in">max_element</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>()), mmin = *<span class="built_in">min_element</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>());</span><br><span class="line">    cout &lt;&lt; mmax &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; mmin &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">100 1</span><br></pre></td></tr></table></figure><h3 id="在有序容器中插入新元素保证有序">在有序容器中插入新元素保证有序</h3><p>结合<code>upper_bound</code>和<code>vector</code>的<code>insert</code>方法</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">(<span class="type">const</span> vector&lt;<span class="type">int</span>&gt;&amp; vec)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span> n : vec) &#123;</span><br><span class="line">        cout &lt;&lt; n &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">1</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">20</span>, <span class="number">23</span>, <span class="number">55</span>, <span class="number">78</span>, <span class="number">98</span>, <span class="number">100</span>&#125;;</span><br><span class="line">    <span class="comment">// 插入50，原始数组升序，因此获得第一个大于50的元素位置，插在那里</span></span><br><span class="line">    <span class="keyword">auto</span> pos = <span class="built_in">upper_bound</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), <span class="number">50</span>);</span><br><span class="line">    arr.<span class="built_in">insert</span>(pos, <span class="number">50</span>);</span><br><span class="line">    <span class="built_in">print</span>(arr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1 6 9 10 15 16 20 23 50 55 78 98 100 </span><br></pre></td></tr></table></figure><h3 id="查询是否有指定元素">查询是否有指定元素</h3><p><code>binary_search</code></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; arr = &#123;<span class="number">1</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">20</span>, <span class="number">23</span>, <span class="number">55</span>, <span class="number">78</span>, <span class="number">98</span>, <span class="number">100</span>&#125;;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">binary_search</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), <span class="number">23</span>) &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">binary_search</span>(arr.<span class="built_in">begin</span>(), arr.<span class="built_in">end</span>(), <span class="number">21</span>) &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注：<code>binary_search</code>的复杂度是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\log(n))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">))</span></span></span></span>, 而通用的<code>find</code>是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>的，且<code>binary_search</code>要求容器必须是有序的</p><h2 id="二分库的自定义比较函数">二分库的自定义比较函数</h2><p>自定义比较函数用于告诉编译器该有序数组是按照何种<strong>规则</strong>有序的，默认就是<code>&lt;</code></p><p>比如若stl容器存储的非基本类型而是类，容器<strong>按照类的某个属性有序排列</strong>，那就要在比较函数中告诉编译器<strong>按照该属性的比较规则</strong>（<code>&lt;</code>升序，<code>&gt;</code>降序）</p>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++部分作业代码</title>
      <link href="/ymhui.github.io/2024/12/19/C-%E9%83%A8%E5%88%86%E4%BD%9C%E4%B8%9A%E4%BB%A3%E7%A0%81/"/>
      <url>/ymhui.github.io/2024/12/19/C-%E9%83%A8%E5%88%86%E4%BD%9C%E4%B8%9A%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h2 id="数据结构与算法部分">数据结构与算法部分</h2><p>题目见<a href="https://github.com/NJU-ymhui/NJUSE-2024Cpp-problems">https://github.com/NJU-ymhui/NJUSE-2024Cpp-problems</a></p><p>滑动窗口：<a href="###%E6%9C%80%E7%9F%AD%E5%AD%90%E6%95%B0%E7%BB%84">最短子数组</a></p><p>字符串处理: <a href="###%E5%AF%86%E9%92%A5%E6%A0%BC%E5%BC%8F%E5%8C%96">密钥格式化</a></p><p>哈希：<a href="###%E5%AD%97%E6%AF%8D%E9%A2%91%E7%8E%87%E7%BB%9F%E8%AE%A1">字母频率统计</a>, <a href="###%E7%94%B5%E5%AD%90%E8%A1%A8">电子表</a></p><p>二分：<a href="###%E6%96%B9%E7%A8%8B%E7%9A%84%E8%A7%A3">方程的解</a>, <a href="###%E5%8E%9F%E6%96%99%E9%87%87%E8%B4%AD">原料采购</a>, <a href="###%E5%8D%A1%E7%89%87%E6%B8%B8%E6%88%8F">卡片游戏</a></p><p>最短路径：<a href="###%E5%A4%96%E5%8D%96%E9%AA%91%E6%89%8B">外卖骑手</a></p><p>搜索：<a href="###%E5%A4%96%E5%8D%96%E9%AA%91%E6%89%8B">外卖骑手</a>, <a href="###%E6%B6%88%E7%81%AD%E6%98%9F%E6%98%9F">消灭星星</a></p><p>贪心：<a href="###%E9%87%87%E8%B4%AD%E6%97%A5%E5%BF%97">采购日志</a>, <a href="###%E6%A0%A1%E5%9B%AD%E7%A5%AD%E6%89%93%E5%8D%A1">校园祭打卡</a>, <a href="###%E4%B8%8D%E9%87%8D%E5%8F%A0%E6%97%B6%E9%97%B4%E6%AE%B5">不重叠时间段</a></p><p>技巧：<a href="###%E4%B8%8D%E9%87%8D%E5%8F%A0%E6%97%B6%E9%97%B4%E6%AE%B5">不重叠时间段</a>, <a href="###%E5%8D%A1%E7%89%87%E6%B8%B8%E6%88%8F">卡片游戏</a></p><p>C++库：<a href="###%E7%94%B5%E5%AD%90%E8%A1%A8">电子表</a></p><p>暴力(可解不超时)：<a href="###%E5%8D%A1%E7%89%87%E6%B8%B8%E6%88%8F">卡片游戏</a></p><p>快速幂：<a href="###%E7%9F%A9%E9%98%B5%E5%BF%AB%E9%80%9F%E5%B9%82">矩阵快速幂</a></p><h3 id="最短子数组">最短子数组</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n, target = <span class="number">0</span>;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; arr;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    arr.<span class="built_in">resize</span>(n, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        cin &gt;&gt; arr[i];</span><br><span class="line">    &#125;</span><br><span class="line">    cin &gt;&gt; target;</span><br><span class="line">    <span class="type">int</span> length = n; <span class="comment">// 子数组长度</span></span><br><span class="line">    <span class="type">int</span> left = <span class="number">0</span>, right = <span class="number">0</span>; <span class="comment">// 滑动窗口的左右边界</span></span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>, idx = <span class="number">2</span> * n; <span class="comment">// 子数组和, 下标和</span></span><br><span class="line">    <span class="type">bool</span> found = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">for</span> (right = <span class="number">0</span>; right &lt; n; right++) &#123; <span class="comment">// 向右扩张窗口</span></span><br><span class="line">        sum += arr[right];</span><br><span class="line">        <span class="keyword">while</span> (sum &gt;= target) &#123;</span><br><span class="line">            found = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span> (length &gt; right - left + <span class="number">1</span>) &#123;</span><br><span class="line">                idx = left + right;</span><br><span class="line">                length = right - left + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (length == right - left + <span class="number">1</span>) &#123;</span><br><span class="line">                idx = <span class="built_in">min</span>(idx, left + right);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (left == right) &#123;</span><br><span class="line">                cout &lt;&lt; left; <span class="comment">// 都重合了肯定满足长度最小，第一次出现也是下标和最小的</span></span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            sum -= arr[left++]; <span class="comment">// 收缩窗口</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (found) &#123;</span><br><span class="line">        cout &lt;&lt; idx;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="密钥格式化">密钥格式化</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> int long long</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    string raw_key = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="type">int</span> k = <span class="number">0</span>;</span><br><span class="line">    cin &gt;&gt; raw_key &gt;&gt; k;</span><br><span class="line">    string key = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="type">int</span> n = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> c : raw_key) &#123;</span><br><span class="line">        <span class="keyword">if</span> (c == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">            n++;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">isalpha</span>(c)) &#123;</span><br><span class="line">            c = <span class="built_in">toupper</span>(c); <span class="comment">// 统一转大写</span></span><br><span class="line">        &#125;</span><br><span class="line">        key += c;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> ptr = <span class="number">0</span>; <span class="comment">// 指向旧密钥</span></span><br><span class="line">    string format_key = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="comment">// 格式化之后不一定还是n + 1组</span></span><br><span class="line">    <span class="type">int</span> group = <span class="number">1</span>, sz = key.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">while</span> (ptr &lt; sz) &#123;</span><br><span class="line">        <span class="keyword">if</span> (group != <span class="number">1</span>) &#123;</span><br><span class="line">            format_key += <span class="string">&quot;-&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (group == k) &#123;</span><br><span class="line">            <span class="type">int</span> left = sz - ptr; <span class="comment">// left 保证在这里ptr不会越界</span></span><br><span class="line">            <span class="keyword">if</span> (left == <span class="number">0</span>) &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;INVALID&quot;</span> &lt;&lt; endl;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">int</span> k_th = left % k;</span><br><span class="line">            <span class="keyword">if</span> (k_th == <span class="number">0</span>) &#123;</span><br><span class="line">                k_th = k;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span> (k_th) &#123;</span><br><span class="line">                <span class="type">char</span> c = key[ptr];</span><br><span class="line">                format_key += c;</span><br><span class="line">                ptr++; <span class="comment">// 上面保证了ptr不会越界因此此处不需要检验</span></span><br><span class="line">                k_th--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">int</span> sze = k;</span><br><span class="line">            <span class="type">bool</span> digit = <span class="literal">false</span>, alpha = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">while</span> (sze &amp;&amp; ptr &lt; sz) &#123;</span><br><span class="line">                <span class="type">char</span> c = key[ptr];</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">isalpha</span>(c)) &#123;</span><br><span class="line">                    alpha = <span class="literal">true</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    digit = <span class="literal">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                format_key += c;</span><br><span class="line">                sze--;</span><br><span class="line">                ptr++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (sze || !digit || !alpha) &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;INVALID&quot;</span> &lt;&lt; endl;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        group++;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; format_key;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字母频率统计">字母频率统计</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    map&lt;<span class="type">char</span>, <span class="type">int</span>&gt; table; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">char</span> a = <span class="string">&#x27;a&#x27;</span>; a &lt;= <span class="string">&#x27;z&#x27;</span>; a++) &#123;</span><br><span class="line">        table[a] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> q;</span><br><span class="line">    cin &gt;&gt; q;</span><br><span class="line">    <span class="type">bool</span> empty = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; q; i++) &#123;</span><br><span class="line">        string cmd;</span><br><span class="line">        cin &gt;&gt; cmd;</span><br><span class="line">        <span class="keyword">if</span> (cmd == <span class="string">&quot;add&quot;</span>) &#123;</span><br><span class="line">            empty = <span class="literal">false</span>;</span><br><span class="line">            string word;</span><br><span class="line">            cin &gt;&gt; word;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span> c : word) &#123;</span><br><span class="line">                table[c]++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (empty) &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;&quot;</span> &lt;&lt; endl;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">int</span> mm = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span> p : table) &#123;</span><br><span class="line">                <span class="keyword">if</span> (mm &lt; p.second) &#123;</span><br><span class="line">                    mm = p.second;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            string ans = <span class="string">&quot;&quot;</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span> p : table) &#123;</span><br><span class="line">                <span class="keyword">if</span> (mm == p.second) &#123;</span><br><span class="line">                    ans += p.first;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">sort</span>(ans.<span class="built_in">begin</span>(), ans.<span class="built_in">end</span>());</span><br><span class="line">            cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="方程的解">方程的解</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">double</span> a, b;</span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">f</span><span class="params">(<span class="type">double</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">exp</span>(x) - a * x - b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    cin &gt;&gt; a &gt;&gt; b;</span><br><span class="line">    <span class="type">double</span> epsilon = <span class="number">1e-7</span>;</span><br><span class="line">    <span class="type">double</span> left = epsilon, right = <span class="number">10</span> - epsilon;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">fabs</span>(right - left) &gt; epsilon) &#123;</span><br><span class="line">        <span class="type">double</span> mid = (right + left) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">f</span>(left) * <span class="built_in">f</span>(mid) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            left = mid;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">f</span>(left) * <span class="built_in">f</span>(mid) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            right = mid;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%.6f&quot;</span>, mid);</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%.6f&quot;</span>, (right + left) / <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="电子表">电子表</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">get_month</span><span class="params">(string month)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (month == <span class="string">&quot;January&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;February&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;March&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;April&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;May&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">4</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;June&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;July&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">6</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;August&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">7</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;September&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">8</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;October&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">9</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;November&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">10</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (month == <span class="string">&quot;December&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">11</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    string beijing = <span class="string">&quot;2024-09-16T00:00:00&quot;</span>, e_clock = <span class="string">&quot;2024-09-01T22:20:00&quot;</span>;</span><br><span class="line">    <span class="function">istringstream <span class="title">m1</span><span class="params">(beijing)</span>, <span class="title">m2</span><span class="params">(e_clock)</span></span>;</span><br><span class="line">    tm tm1 = &#123;&#125;, tm2 = &#123;&#125;;</span><br><span class="line">    m1 &gt;&gt; <span class="built_in">get_time</span>(&amp;tm1, <span class="string">&quot;%Y-%m-%dT%H:%M:%S&quot;</span>);</span><br><span class="line">    m2 &gt;&gt; <span class="built_in">get_time</span>(&amp;tm2, <span class="string">&quot;%Y-%m-%dT%H:%M:%S&quot;</span>);</span><br><span class="line">    <span class="type">time_t</span> beijing_std = <span class="built_in">mktime</span>(&amp;tm1), beijing_clock = <span class="built_in">mktime</span>(&amp;tm2);</span><br><span class="line">    string clock_time;</span><br><span class="line">    <span class="type">int</span> year, day, hour, minute, sec;</span><br><span class="line">    string month;</span><br><span class="line">    cin &gt;&gt; year &gt;&gt; month &gt;&gt; day &gt;&gt; hour &gt;&gt; minute &gt;&gt; sec;</span><br><span class="line">    tm tm_clock = &#123;&#125;;</span><br><span class="line">    tm_clock.tm_year = year - <span class="number">1900</span>;</span><br><span class="line">    tm_clock.tm_mon = <span class="built_in">get_month</span>(month);</span><br><span class="line">    tm_clock.tm_mday = day;</span><br><span class="line">    tm_clock.tm_hour = hour;</span><br><span class="line">    tm_clock.tm_min = minute;</span><br><span class="line">    tm_clock.tm_sec = sec;</span><br><span class="line">    <span class="type">time_t</span> clock_unix = <span class="built_in">mktime</span>(&amp;tm_clock);</span><br><span class="line">    <span class="type">time_t</span> gap = clock_unix - beijing_clock;</span><br><span class="line">    <span class="keyword">if</span> (gap &gt; <span class="number">0</span>)</span><br><span class="line">        gap = gap * <span class="number">60</span> / <span class="number">59</span>;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (gap * <span class="number">60</span> / <span class="number">59</span> * <span class="number">59</span> == gap * <span class="number">60</span>) &#123;</span><br><span class="line">            gap = gap * <span class="number">60</span> / <span class="number">59</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            gap = gap * <span class="number">60</span> / <span class="number">59</span> - <span class="number">1</span>; <span class="comment">// 实际上不足1秒，但是数值上向0截断了，还回去</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">time_t</span> current_unix = beijing_std + gap;</span><br><span class="line">    <span class="comment">// 准备从unix时间戳转化为标准时间</span></span><br><span class="line">    tm* current = <span class="built_in">localtime</span>(&amp;current_unix); <span class="comment">// 这里用localtime还是gmtime是一样的</span></span><br><span class="line">    cout &lt;&lt; <span class="built_in">put_time</span>(current, <span class="string">&quot;%Y-%m-%dT%H:%M:%S&quot;</span>) &lt;&lt; endl; <span class="comment">// 输出格式化的标准时间</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="外卖骑手">外卖骑手</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAX (INT_MAX - (1 &lt;&lt; 16))</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> GCC optimize (<span class="string">&quot;Ofast&quot;</span>)</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> ans = MAX;</span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line">vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; dist;</span><br><span class="line">vector&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; positions;</span><br><span class="line">vector&lt;<span class="type">bool</span>&gt; flags; <span class="comment">// 标记是否可以送餐</span></span><br><span class="line">vector&lt;<span class="type">bool</span>&gt; finish;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> current, <span class="type">int</span> cost)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (cost &gt;= ans) &#123;</span><br><span class="line">        <span class="keyword">return</span>; <span class="comment">// 剪枝</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">bool</span> all = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!finish[i]) &#123;</span><br><span class="line">            all = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (all) &#123;</span><br><span class="line">        <span class="keyword">if</span> (ans &gt; cost) &#123;</span><br><span class="line">            ans = cost;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (finish[i]) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!flags[i]) &#123;</span><br><span class="line">            <span class="comment">// 没有取餐，尝试取餐</span></span><br><span class="line">            flags[i] = <span class="literal">true</span>;</span><br><span class="line">            <span class="built_in">dfs</span>(positions[i].first, cost + dist[current][positions[i].first]);</span><br><span class="line">            flags[i] = <span class="literal">false</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (flags[i]) &#123;</span><br><span class="line">            <span class="comment">// 送餐</span></span><br><span class="line">            finish[i] = <span class="literal">true</span>;</span><br><span class="line">            <span class="built_in">dfs</span>(positions[i].second, cost + dist[current][positions[i].second]);</span><br><span class="line">            finish[i] = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    <span class="comment">// cout &lt;&lt; MAX &lt;&lt; endl;</span></span><br><span class="line">    dist.<span class="built_in">resize</span>(m, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(m, <span class="number">0</span>));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; m; j++) &#123;</span><br><span class="line">            cin &gt;&gt; dist[i][j];</span><br><span class="line">            <span class="keyword">if</span> (dist[i][j] == <span class="number">-1</span>) &#123;</span><br><span class="line">                dist[i][j] = MAX;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    positions.<span class="built_in">resize</span>(n, <span class="built_in">pair</span>(<span class="number">0</span>, <span class="number">0</span>));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        cin &gt;&gt; positions[i].first &gt;&gt; positions[i].second;</span><br><span class="line">        positions[i].first--;</span><br><span class="line">        positions[i].second--;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// use floyed</span></span><br><span class="line">    <span class="comment">// floyed算法别记错了，最外层是循环迭代的次数，不是最内层，写反了迭代顺序会出问题导致最短路不一定对</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; m; k++) &#123;</span><br><span class="line">        <span class="comment">// 迭代轮数</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; m; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (dist[i][k] != MAX &amp;&amp; dist[k][j] != MAX) &#123;</span><br><span class="line">                    dist[i][j] = <span class="built_in">min</span>(dist[i][j], dist[i][k] + dist[k][j]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    flags.<span class="built_in">resize</span>(n, <span class="literal">false</span>);</span><br><span class="line">    finish.<span class="built_in">resize</span>(n, <span class="literal">false</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">dfs</span>(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="采购日志">采购日志</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> GCC optimize (<span class="string">&quot;Ofast&quot;</span>)</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> mmax[<span class="number">1000005</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> logs_id[<span class="number">1000005</span>], logs_prc[<span class="number">1000005</span>];</span><br><span class="line">string logs_cmd[<span class="number">1000005</span>];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n, m; <span class="comment">// n种商品，m条日志</span></span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    string line;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        string token;</span><br><span class="line">        <span class="type">int</span> id, prc;</span><br><span class="line">        cin &gt;&gt; token &gt;&gt; id &gt;&gt; prc;</span><br><span class="line">        logs_cmd[i] = token, logs_id[i] = id, logs_prc[i] = prc;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 倒序遍历日志，期望获得最大值, 注意前面买后面卖，动态规划 + 贪心</span></span><br><span class="line">    <span class="comment">// 遍历时记录价格更新最大值，到购买的时候直接判断取0和大 - 成本中的较大值来获取这次购买可以获得的利润</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = m - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="type">int</span> id = logs_id[i], prc = logs_prc[i];</span><br><span class="line">        <span class="comment">// 维护更新最大价格</span></span><br><span class="line">        <span class="keyword">if</span> (mmax[id - <span class="number">1</span>] &lt; prc) &#123;</span><br><span class="line">            mmax[id - <span class="number">1</span>] = prc;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (logs_cmd[i] == <span class="string">&quot;buy&quot;</span>) &#123;</span><br><span class="line">            <span class="comment">// 购买时直接可以计算出利润，因为记录了这个时间点之后价格的最高峰</span></span><br><span class="line">            ans += mmax[id - <span class="number">1</span>] - prc;</span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="消灭星星">消灭星星</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> game[<span class="number">2005</span>][<span class="number">2005</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="type">bool</span> visited[<span class="number">2005</span>][<span class="number">2005</span>] = &#123;<span class="literal">false</span>&#125;;</span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> cur_x, <span class="type">int</span> cur_y, <span class="type">int</span> key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (cur_x &gt;= n || cur_x &lt; <span class="number">0</span> || cur_y &gt;= m || cur_y &lt; <span class="number">0</span> || visited[cur_x][cur_y]) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    visited[cur_x][cur_y] = <span class="literal">true</span>; <span class="comment">// 不加这个会导致StackOverflow然后运行异常</span></span><br><span class="line">    <span class="keyword">if</span> (game[cur_x][cur_y] == key) &#123;</span><br><span class="line">        game[cur_x][cur_y] = <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span>; <span class="comment">// 不是这个符号消除就终止了，不能再递归搜索了</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">dfs</span>(cur_x - <span class="number">1</span>, cur_y, key);</span><br><span class="line">    <span class="built_in">dfs</span>(cur_x + <span class="number">1</span>, cur_y, key);</span><br><span class="line">    <span class="built_in">dfs</span>(cur_x, cur_y - <span class="number">1</span>, key);</span><br><span class="line">    <span class="built_in">dfs</span>(cur_x, cur_y + <span class="number">1</span>, key);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; m; ++j) &#123;</span><br><span class="line">            cout &lt;&lt; game[i][j] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; m; ++j) &#123;</span><br><span class="line">            cin &gt;&gt; game[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> x, y;</span><br><span class="line">    cin &gt;&gt; x &gt;&gt; y;</span><br><span class="line">    x--, y--;</span><br><span class="line">    <span class="built_in">dfs</span>(x, y, game[x][y]);</span><br><span class="line">    <span class="comment">// 按照之前做双指针题目的思路，维护两个指针，一个指向当前填充位置，一个从下往上遍历每一列</span></span><br><span class="line">    <span class="comment">// 只有遇到非0数才在填充指针位置处填入该非0数，并更新填充指针；最后剩余位置填0</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        <span class="type">int</span> fill = n - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = n - <span class="number">1</span>; j &gt;= <span class="number">0</span>; --j) &#123;</span><br><span class="line">            <span class="keyword">if</span> (game[j][i] != <span class="number">0</span>) &#123;</span><br><span class="line">                game[fill--][i] = game[j][i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (fill &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            game[fill--][i] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="校园祭打卡">校园祭打卡</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    vector&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; positions;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; input;</span><br><span class="line">        cin &gt;&gt; input.first &gt;&gt; input.second; <span class="comment">// first起始位置，end终止位置</span></span><br><span class="line">        positions.<span class="built_in">push_back</span>(input);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 可能按结束位置排序更好做</span></span><br><span class="line">    <span class="built_in">sort</span>(positions.<span class="built_in">begin</span>(), positions.<span class="built_in">end</span>(), [](<span class="keyword">auto</span> a, <span class="keyword">auto</span> b) &#123;</span><br><span class="line">       <span class="keyword">return</span> a.first &lt; b.first; </span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="comment">// 处于前一节点的结束位置之前的一定不会超过后者的结束位置，且若后者的起始位置在前者结束之前</span></span><br><span class="line">    <span class="comment">// 则自己位置一定也在后者之中</span></span><br><span class="line">    <span class="comment">// 所以尽可能向前走</span></span><br><span class="line">    <span class="type">int</span> pos = <span class="number">0</span>, i = <span class="number">0</span>, ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (i &lt; n) &#123;</span><br><span class="line">        <span class="keyword">if</span> (pos + <span class="number">27</span> &lt; positions[i].first) &#123;</span><br><span class="line">            pos += <span class="number">27</span>;</span><br><span class="line">            ans++;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">int</span> emax = <span class="built_in">min</span>(positions[i].second, pos + <span class="number">27</span>); <span class="comment">// 最远可达位置</span></span><br><span class="line">            <span class="keyword">while</span> (i &lt; n &amp;&amp; positions[i].first &lt;= emax) &#123;</span><br><span class="line">                <span class="keyword">if</span> (emax &gt; positions[i].second) &#123;</span><br><span class="line">                    <span class="comment">// 为了一次性打更多卡且避免回头（贪心），停下来时必须把之前经过的摊位都打了，因此如果某个摊位更靠前（小于emax），则更新</span></span><br><span class="line">                    emax = positions[i].second;</span><br><span class="line">                &#125;</span><br><span class="line">                i++;</span><br><span class="line">                pos = emax; <span class="comment">// 记得更新位置</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 不能再走了，停下来打卡</span></span><br><span class="line">            ans++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; <span class="string">&quot;T&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="原料采购">原料采购</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">can_buy</span><span class="params">(<span class="type">const</span> vector&lt;<span class="type">int</span>&gt;&amp; fund, <span class="type">int</span> price, <span class="type">double</span> target, <span class="type">double</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="type">double</span> have = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> left = <span class="number">0</span>; <span class="comment">// 剩余资金 </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> money : fund) &#123;</span><br><span class="line">        left += money % price;</span><br><span class="line">        have += money / price;</span><br><span class="line">    &#125;</span><br><span class="line">    have += left / (price - k);</span><br><span class="line">    <span class="keyword">return</span> have &gt;= target;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    <span class="type">double</span> target, k;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; target &gt;&gt; k; <span class="comment">// 0 &lt; k &lt;= 1</span></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">fund</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        cin &gt;&gt; fund[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> left = <span class="number">2</span>, right = <span class="number">10000000</span>, mid = right;</span><br><span class="line">    <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">can_buy</span>(fund, mid, target, k)) &#123;</span><br><span class="line">            <span class="comment">// 可以买得下</span></span><br><span class="line">            left = mid + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">can_buy</span>(fund, mid, target, k)) &#123;</span><br><span class="line">        mid--; <span class="comment">// 防止迭代过程中多加了1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (mid &lt; <span class="number">2</span>) &#123;</span><br><span class="line">        cout &lt;&lt; <span class="number">-1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        cout &lt;&lt; mid;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="卡片游戏">卡片游戏</h3><p><strong>暴力</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n, k;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; k;</span><br><span class="line">    vector&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; <span class="built_in">cards</span>(n); </span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">copy</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        cin &gt;&gt; cards[i].second;</span><br><span class="line">        cards[i].first = i;</span><br><span class="line">        copy[i] = cards[i].second;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(cards.<span class="built_in">begin</span>(), cards.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span> a, <span class="type">const</span> <span class="keyword">auto</span> b) &#123;</span><br><span class="line">        <span class="keyword">return</span> a.second &lt; b.second;</span><br><span class="line">    &#125;); <span class="comment">// 对其中一组进行排序，方便按顺序从小到大获得所有能获取的合法分数</span></span><br><span class="line">    <span class="type">int</span> take = <span class="number">0</span>; <span class="comment">// 取得的卡片数</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="comment">// O(n^2)</span></span><br><span class="line">        <span class="type">int</span> score = cards[i].second; <span class="comment">// 当前准备获取的分数</span></span><br><span class="line">        <span class="comment">// 下面再遍历一遍卡片组，尽可能取得所有比score小的卡片，且一定拿score</span></span><br><span class="line">        <span class="comment">// 由于本题n最大才1000，不算大，所以O(n^2)遍历两遍的暴力做法应该不会超时</span></span><br><span class="line">        <span class="comment">// 不能连续取，记录上一次拿的下标，为了能拿第一个(0), 初始化为-2</span></span><br><span class="line">        <span class="type">int</span> last = <span class="number">-2</span>;</span><br><span class="line">        take = <span class="number">1</span>; <span class="comment">// 必拿score</span></span><br><span class="line">        <span class="type">int</span> score_idx = cards[i].first;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (</span><br><span class="line">                copy[j] &lt;= score &amp;&amp; j - <span class="number">1</span> != last &amp;&amp; j != score_idx &amp;&amp; j - <span class="number">1</span> != score_idx &amp;&amp; j + <span class="number">1</span> != score_idx <span class="comment">// 保证score前后的都不取</span></span><br><span class="line">            ) &#123;</span><br><span class="line">                take++;</span><br><span class="line">                last = j;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (take &gt;= k) &#123;</span><br><span class="line">            cout &lt;&lt; score &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>二分优化</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n\log(n))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">))</span></span></span></span></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">judge</span><span class="params">(vector&lt;<span class="type">int</span>&gt; cards, <span class="type">int</span> score, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 遍历一遍，O(n)</span></span><br><span class="line">    <span class="comment">// 尽可能多拿，但不拿相邻的，为了可能拿第一个，初始化上一次拿的为-2</span></span><br><span class="line">    <span class="type">int</span> last = <span class="number">-2</span>, cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; cards.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (cards[i] &lt;= score &amp;&amp; i - <span class="number">1</span> != last) &#123;</span><br><span class="line">            last = i;</span><br><span class="line">            cnt++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (cnt &gt;= k) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> cnt &gt;= k;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n, k;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; k;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">cards</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        cin &gt;&gt; cards[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> left = *<span class="built_in">min_element</span>(cards.<span class="built_in">begin</span>(), cards.<span class="built_in">end</span>()), right = *<span class="built_in">max_element</span>(cards.<span class="built_in">begin</span>(), cards.<span class="built_in">end</span>());</span><br><span class="line">    <span class="type">int</span> res = right;</span><br><span class="line">    <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">        <span class="comment">// 二分循环O(logn)，judge O(n) 共O(nlogn)</span></span><br><span class="line">        <span class="type">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">judge</span>(cards, mid, k)) &#123;</span><br><span class="line">            <span class="comment">// mid分数的卡片可以在k个卡片的条件下满足，尝试更小的分数（最小得分）</span></span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">            res = mid;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            left = mid + <span class="number">1</span>; <span class="comment">// 分数太小了，无法满足，大一些</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="不重叠时间段">不重叠时间段</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta"># <span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    vector&lt;string&gt; times;</span><br><span class="line">    string input;</span><br><span class="line">    <span class="function">pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; <span class="title">start</span><span class="params">(<span class="number">0</span>, <span class="number">5</span>)</span>, <span class="title">end</span><span class="params">(<span class="number">6</span>, <span class="number">11</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        cin &gt;&gt; input;</span><br><span class="line">        times.<span class="built_in">push_back</span>(input);</span><br><span class="line">    &#125;</span><br><span class="line">    function&lt;<span class="type">bool</span>(string, string)&gt; cmp = [end](<span class="keyword">auto</span> a, <span class="keyword">auto</span> b) -&gt; <span class="type">bool</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> a.<span class="built_in">substr</span>(end.first, end.second) &lt; b.<span class="built_in">substr</span>(end.first, end.second);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">sort</span>(times.<span class="built_in">begin</span>(), times.<span class="built_in">end</span>(), cmp);</span><br><span class="line">    string current = times[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="comment">// 如果开始时间早于当前时间的结束时间，则移除，否则更新其为当前时间</span></span><br><span class="line">        <span class="keyword">if</span> (times[i].<span class="built_in">substr</span>(start.first, start.second) &lt; current.<span class="built_in">substr</span>(end.first, end.second)) &#123;</span><br><span class="line">            <span class="comment">// remove</span></span><br><span class="line">            ans++;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 不重叠</span></span><br><span class="line">            current = times[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="模拟部分">模拟部分</h2><h3 id="BF解释器">BF解释器</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    string cmd;</span><br><span class="line">    <span class="built_in">getline</span>(cin, cmd);</span><br><span class="line">    <span class="type">uint8_t</span> buffer[<span class="number">1005</span>] = &#123;<span class="number">0</span>&#125;; <span class="comment">// 记得初始化！</span></span><br><span class="line">    <span class="type">int</span> ptr = <span class="number">0</span>;</span><br><span class="line">    unordered_map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; l2r, r2l;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; s;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; cmd.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="type">char</span> c = cmd[i];</span><br><span class="line">        <span class="keyword">if</span> (c == <span class="string">&#x27;[&#x27;</span>) &#123;</span><br><span class="line">            s.<span class="built_in">push_back</span>(i);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27;]&#x27;</span>) &#123;</span><br><span class="line">            <span class="type">int</span> bk = s.<span class="built_in">back</span>();</span><br><span class="line">            l2r[bk] = i;</span><br><span class="line">            r2l[i] = bk;</span><br><span class="line">            s.<span class="built_in">pop_back</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; cmd.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="type">char</span> c = cmd[i];</span><br><span class="line">        <span class="keyword">if</span> (c == <span class="string">&#x27;&gt;&#x27;</span>) &#123;</span><br><span class="line">            ptr++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27;&lt;&#x27;</span> &amp;&amp; ptr &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            ptr--;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27;+&#x27;</span>) &#123;</span><br><span class="line">            buffer[ptr]++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">            buffer[ptr]--;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27;,&#x27;</span>) &#123;</span><br><span class="line">            <span class="type">char</span> c;</span><br><span class="line">            c = <span class="built_in">getchar</span>();</span><br><span class="line">            <span class="keyword">if</span> (c == EOF) &#123;</span><br><span class="line">                buffer[ptr] = <span class="number">0</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                buffer[ptr] = c;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27;.&#x27;</span>) &#123;</span><br><span class="line">            cout &lt;&lt; buffer[ptr];</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27;[&#x27;</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (buffer[ptr] == <span class="number">0</span>) &#123;</span><br><span class="line">                i = l2r[i]; <span class="comment">// 之后会自动i++</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">&#x27;]&#x27;</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (buffer[ptr] != <span class="number">0</span>) &#123;</span><br><span class="line">                i = r2l[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="沙威玛">沙威玛</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta"># <span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">unordered_map&lt;string, <span class="type">int</span>&gt; inventory;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    inventory[<span class="string">&quot;Chicken&quot;</span>] = <span class="number">0</span>;</span><br><span class="line">    inventory[<span class="string">&quot;Lettuce&quot;</span>] = <span class="number">0</span>;</span><br><span class="line">    inventory[<span class="string">&quot;Tomato&quot;</span>] = <span class="number">0</span>;</span><br><span class="line">    inventory[<span class="string">&quot;Cheese&quot;</span>] = <span class="number">0</span>;</span><br><span class="line">    inventory[<span class="string">&quot;Onion&quot;</span>] = <span class="number">0</span>;</span><br><span class="line">    inventory[<span class="string">&quot;Sauce&quot;</span>] = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        string cmd, material;</span><br><span class="line">        cin &gt;&gt; cmd;</span><br><span class="line">        <span class="keyword">if</span> (cmd == <span class="string">&quot;Stock&quot;</span>) &#123;</span><br><span class="line">            cin &gt;&gt; material;</span><br><span class="line">            <span class="type">int</span> quantity;</span><br><span class="line">            cin &gt;&gt; quantity;</span><br><span class="line">            inventory[material] += quantity;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Stocked &quot;</span> &lt;&lt; material &lt;&lt; <span class="string">&quot; with &quot;</span> &lt;&lt; quantity &lt;&lt; <span class="string">&quot; units&quot;</span> &lt;&lt; endl;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmd == <span class="string">&quot;Inventory&quot;</span>) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;=== Inventory ===&quot;</span> &lt;&lt; endl;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Chicken: &quot;</span> &lt;&lt; inventory[<span class="string">&quot;Chicken&quot;</span>] &lt;&lt; endl;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Lettuce: &quot;</span> &lt;&lt; inventory[<span class="string">&quot;Lettuce&quot;</span>] &lt;&lt; endl;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Tomato: &quot;</span> &lt;&lt; inventory[<span class="string">&quot;Tomato&quot;</span>] &lt;&lt; endl;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Cheese: &quot;</span> &lt;&lt; inventory[<span class="string">&quot;Cheese&quot;</span>] &lt;&lt; endl;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Onion: &quot;</span> &lt;&lt; inventory[<span class="string">&quot;Onion&quot;</span>] &lt;&lt; endl;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Sauce: &quot;</span> &lt;&lt; inventory[<span class="string">&quot;Sauce&quot;</span>] &lt;&lt; endl;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;=================&quot;</span> &lt;&lt; endl;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// Order</span></span><br><span class="line">            string id;</span><br><span class="line">            cin &gt;&gt; id;</span><br><span class="line">            string line;</span><br><span class="line">            <span class="built_in">getline</span>(cin, line);</span><br><span class="line">            <span class="function">istringstream <span class="title">stream</span><span class="params">(line)</span></span>;</span><br><span class="line">            <span class="type">int</span> quantity;</span><br><span class="line">            map&lt;string, <span class="type">int</span>&gt; tmp; <span class="comment">// 存放需要的材料</span></span><br><span class="line">            <span class="type">bool</span> enough = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">while</span> (stream &gt;&gt; material) &#123;</span><br><span class="line">                stream &gt;&gt; quantity;</span><br><span class="line">                <span class="keyword">if</span> (quantity &gt; inventory[material]) &#123;</span><br><span class="line">                    enough = <span class="literal">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                tmp[material] = quantity;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (enough) &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;Order &quot;</span> &lt;&lt; id &lt;&lt; <span class="string">&quot; completed&quot;</span> &lt;&lt; endl;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">auto</span> p : tmp) &#123;</span><br><span class="line">                    inventory[p.first] -= p.second;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;Order &quot;</span> &lt;&lt; id &lt;&lt; <span class="string">&quot; failed: missing &quot;</span>;</span><br><span class="line">                <span class="type">bool</span> first = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">auto</span> p : tmp) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (inventory[p.first] &lt; p.second) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (!first) &#123;</span><br><span class="line">                            cout &lt;&lt; <span class="string">&quot;, &quot;</span>;</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            first = <span class="literal">false</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                        cout &lt;&lt; p.first &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; p.second - inventory[p.first];</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                cout &lt;&lt; endl;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="角色管理">角色管理</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta"># <span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta"># <span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta"># <span class="keyword">include</span> <span class="string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Role</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">double</span> base_strength = <span class="number">0</span>;</span><br><span class="line">    <span class="type">double</span> base_mana = <span class="number">0</span>;</span><br><span class="line">    <span class="type">double</span> base_agility = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> id;</span><br><span class="line">    <span class="type">int</span> level = <span class="number">0</span>;</span><br><span class="line">    <span class="type">bool</span> equip = <span class="literal">false</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">Role</span>(<span class="type">int</span> i) &#123;</span><br><span class="line">        id = i;</span><br><span class="line">        base_agility = <span class="number">0</span>;</span><br><span class="line">        base_mana = <span class="number">0</span>;</span><br><span class="line">        base_strength = <span class="number">0</span>;</span><br><span class="line">        level = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">double</span> <span class="title">getPower</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">upgrade</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">equipWith</span><span class="params">(string tool, <span class="type">double</span> p)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">double</span> <span class="title">getStrength</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> base_strength;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">double</span> <span class="title">getMana</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> base_mana;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">double</span> <span class="title">getAgility</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> base_agility;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Warrior</span>: <span class="keyword">public</span> Role &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">double</span> weapon_strength = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">Warrior</span>(<span class="type">int</span> id): <span class="built_in">Role</span>(id) &#123;</span><br><span class="line">       weapon_strength = <span class="number">0</span>; </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">getStrength</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> base_strength + weapon_strength;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">getPower</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (base_strength + weapon_strength) * <span class="number">1.5</span> + base_mana * <span class="number">0.5</span> + base_agility * <span class="number">0.8</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">upgrade</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (level &gt;= <span class="number">5</span>)&#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Character C&quot;</span> &lt;&lt; id &lt;&lt; <span class="string">&quot; is already at max level&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        level++;</span><br><span class="line">        base_strength *= <span class="number">1.1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">equipWith</span><span class="params">(string tool, <span class="type">double</span> p)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (tool != <span class="string">&quot;weapon&quot;</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        equip = <span class="literal">true</span>;</span><br><span class="line">        weapon_strength = p;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mage</span>: <span class="keyword">public</span> Role &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">double</span> staff_power = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">Mage</span>(<span class="type">int</span> id): <span class="built_in">Role</span>(id) &#123;</span><br><span class="line">        staff_power = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">getMana</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> base_mana + <span class="number">2</span> * staff_power;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">getPower</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (base_mana + <span class="number">2</span> * staff_power) * <span class="number">2</span> + base_strength * <span class="number">0.6</span> + base_agility * <span class="number">0.7</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">upgrade</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (level &gt;= <span class="number">5</span>)&#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Character C&quot;</span> &lt;&lt; id &lt;&lt; <span class="string">&quot; is already at max level&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        level++;</span><br><span class="line">        base_mana *= <span class="number">1.2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">equipWith</span><span class="params">(string tool, <span class="type">double</span> p)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (tool != <span class="string">&quot;staff&quot;</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        equip = <span class="literal">true</span>;</span><br><span class="line">        staff_power = p;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Rogue</span>: <span class="keyword">public</span> Role &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">double</span> stealth_bonus = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">Rogue</span>(<span class="type">int</span> id): <span class="built_in">Role</span>(id) &#123;</span><br><span class="line">        stealth_bonus = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">getAgility</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> base_agility + stealth_bonus;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">getPower</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (base_agility + stealth_bonus) * <span class="number">1.8</span> + base_strength * <span class="number">0.6</span> + base_mana * <span class="number">0.4</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">upgrade</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (level &gt;= <span class="number">5</span>)&#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Character C&quot;</span> &lt;&lt; id &lt;&lt; <span class="string">&quot; is already at max level&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        level++;</span><br><span class="line">        base_agility *= <span class="number">1.15</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">equipWith</span><span class="params">(string tool, <span class="type">double</span> p)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (tool != <span class="string">&quot;cloak&quot;</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        equip = <span class="literal">true</span>;</span><br><span class="line">        stealth_bonus = p;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Team</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> id;</span><br><span class="line">    Team* manager;</span><br><span class="line">    vector&lt;Role*&gt; rolesById; <span class="comment">// Id升序</span></span><br><span class="line">    vector&lt;Role*&gt; rolesByPower; <span class="comment">// 战力降序</span></span><br><span class="line">    <span class="built_in">Team</span>(<span class="type">int</span> i) &#123;</span><br><span class="line">        id = i;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(Role* role)</span> </span>&#123;</span><br><span class="line">        rolesById.<span class="built_in">push_back</span>(role);</span><br><span class="line">        <span class="type">int</span> i = rolesById.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &gt; <span class="number">0</span> &amp;&amp; rolesById[i]-&gt;id &lt; rolesById[i - <span class="number">1</span>]-&gt;id) &#123;</span><br><span class="line">            Role* tmp = rolesById[i - <span class="number">1</span>];</span><br><span class="line">            rolesById[i - <span class="number">1</span>] = rolesById[i];</span><br><span class="line">            rolesById[i] = tmp;</span><br><span class="line">            i--;</span><br><span class="line">        &#125;</span><br><span class="line">        rolesByPower.<span class="built_in">push_back</span>(role);</span><br><span class="line">        i = rolesByPower.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &gt; <span class="number">0</span> &amp;&amp; (rolesByPower[i]-&gt;<span class="built_in">getPower</span>() &gt; rolesByPower[i - <span class="number">1</span>]-&gt;<span class="built_in">getPower</span>() || (rolesByPower[i]-&gt;<span class="built_in">getPower</span>() == rolesByPower[i - <span class="number">1</span>]-&gt;<span class="built_in">getPower</span>() &amp;&amp; rolesByPower[i]-&gt;id &lt; rolesByPower[i - <span class="number">1</span>]-&gt;id))) &#123;</span><br><span class="line">            Role* tmp = rolesByPower[i - <span class="number">1</span>];</span><br><span class="line">            rolesByPower[i - <span class="number">1</span>] = rolesByPower[i];</span><br><span class="line">            rolesByPower[i] = tmp;</span><br><span class="line">            i--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">printById</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Role* role : rolesById) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;C&quot;</span> &lt;&lt; role-&gt;id &lt;&lt; <span class="string">&quot; strength &quot;</span> &lt;&lt; role-&gt;<span class="built_in">getStrength</span>() &lt;&lt; <span class="string">&quot; mana &quot;</span> &lt;&lt; role-&gt;<span class="built_in">getMana</span>() &lt;&lt; <span class="string">&quot; agility &quot;</span> &lt;&lt; role-&gt;<span class="built_in">getAgility</span>() &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">printByPower</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// sort </span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; rolesByPower.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; rolesByPower.<span class="built_in">size</span>() - i - <span class="number">1</span>; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (rolesByPower[j]-&gt;<span class="built_in">getPower</span>() &lt; rolesByPower[j + <span class="number">1</span>]-&gt;<span class="built_in">getPower</span>() || (rolesByPower[j]-&gt;<span class="built_in">getPower</span>() == rolesByPower[j + <span class="number">1</span>]-&gt;<span class="built_in">getPower</span>() &amp;&amp; rolesByPower[j]-&gt;id &gt; rolesByPower[j + <span class="number">1</span>]-&gt;id)) &#123;</span><br><span class="line">                    Role* tmp = rolesByPower[j];</span><br><span class="line">                    rolesByPower[j] = rolesByPower[j + <span class="number">1</span>];</span><br><span class="line">                    rolesByPower[j + <span class="number">1</span>] = tmp;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (Role* role : rolesByPower) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;C&quot;</span> &lt;&lt; role-&gt;id &lt;&lt; <span class="string">&quot; strength &quot;</span> &lt;&lt; role-&gt;<span class="built_in">getStrength</span>() &lt;&lt; <span class="string">&quot; mana &quot;</span> &lt;&lt; role-&gt;<span class="built_in">getMana</span>() &lt;&lt; <span class="string">&quot; agility &quot;</span> &lt;&lt; role-&gt;<span class="built_in">getAgility</span>() &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">showPower</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 总是按id升序</span></span><br><span class="line">        <span class="keyword">for</span> (Role* role : rolesById) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;C&quot;</span> &lt;&lt; role-&gt;id &lt;&lt; <span class="string">&quot; power: &quot;</span> &lt;&lt; role-&gt;<span class="built_in">getPower</span>() &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">Role* <span class="title">findById</span><span class="params">(<span class="type">int</span> id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Role* role : rolesById) &#123;</span><br><span class="line">            <span class="keyword">if</span> (role-&gt;id == id) &#123;</span><br><span class="line">                <span class="keyword">return</span> role;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">Role* <span class="title">findById</span><span class="params">(string c)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> id = <span class="built_in">stoi</span>(c.<span class="built_in">substr</span>(<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">findById</span>(id);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">modify</span><span class="params">(string attri, <span class="type">double</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (attri == <span class="string">&quot;base_strength&quot;</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Role* role : rolesById) &#123;</span><br><span class="line">                role-&gt;base_strength = val;</span><br><span class="line">                <span class="comment">// manager-&gt;modify(role);</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (attri == <span class="string">&quot;base_mana&quot;</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Role* role : rolesById) &#123;</span><br><span class="line">                role-&gt;base_mana = val;</span><br><span class="line">                <span class="comment">// manager-&gt;modify(role);</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (Role* role : rolesById) &#123;</span><br><span class="line">                role-&gt;base_agility = val;</span><br><span class="line">                <span class="comment">// manager-&gt;modify(role);</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">modify</span><span class="params">(Role* role)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> index = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; rolesByPower.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (role-&gt;id == rolesByPower[i]-&gt;id) &#123;</span><br><span class="line">                index = i;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (index &gt; <span class="number">0</span> &amp;&amp; (rolesByPower[index]-&gt;<span class="built_in">getPower</span>() &gt; rolesByPower[index - <span class="number">1</span>]-&gt;<span class="built_in">getPower</span>() || (rolesByPower[index]-&gt;<span class="built_in">getPower</span>() == rolesByPower[index - <span class="number">1</span>]-&gt;<span class="built_in">getPower</span>() &amp;&amp; rolesByPower[index]-&gt;id &lt; rolesByPower[index - <span class="number">1</span>]-&gt;id))) &#123;</span><br><span class="line">            Role* tmp = rolesByPower[index];</span><br><span class="line">            rolesByPower[index] = rolesByPower[index - <span class="number">1</span>];</span><br><span class="line">            rolesByPower[index - <span class="number">1</span>] = tmp;</span><br><span class="line">            index--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (index &lt; rolesByPower.<span class="built_in">size</span>() - <span class="number">1</span> &amp;&amp; (rolesByPower[index]-&gt;<span class="built_in">getPower</span>() &lt; rolesByPower[index + <span class="number">1</span>]-&gt;<span class="built_in">getPower</span>() || (rolesByPower[index]-&gt;<span class="built_in">getPower</span>() == rolesByPower[index + <span class="number">1</span>]-&gt;<span class="built_in">getPower</span>() &amp;&amp; rolesByPower[index]-&gt;id &gt; rolesByPower[index + <span class="number">1</span>]-&gt;id))) &#123;</span><br><span class="line">            Role* tmp = rolesByPower[index];</span><br><span class="line">            rolesByPower[index] = rolesByPower[index + <span class="number">1</span>];</span><br><span class="line">            rolesByPower[index + <span class="number">1</span>] = tmp;</span><br><span class="line">            index++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Team* manager = <span class="keyword">new</span> <span class="built_in">Team</span>(<span class="number">-1</span>); <span class="comment">// 管理全局的角色</span></span><br><span class="line">vector&lt;Team*&gt; teams;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">parseLine</span><span class="params">(string line)</span> </span>&#123;</span><br><span class="line">    <span class="function">istringstream <span class="title">stream</span><span class="params">(line)</span></span>;</span><br><span class="line">    string word;</span><br><span class="line">    string tool;</span><br><span class="line">    stream &gt;&gt; word;</span><br><span class="line">    <span class="type">int</span> id;</span><br><span class="line">    <span class="type">double</span> val;</span><br><span class="line">    string num = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    Role* role, *r1, *r2;</span><br><span class="line">    Team* team;</span><br><span class="line">    string c1, c2;</span><br><span class="line">    <span class="keyword">switch</span>(word[<span class="number">0</span>]) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;A&#x27;</span>:</span><br><span class="line">            <span class="comment">// Add</span></span><br><span class="line">            stream &gt;&gt; word; <span class="comment">// 职业</span></span><br><span class="line">            <span class="keyword">if</span> (word == <span class="string">&quot;Warrior&quot;</span>) &#123;</span><br><span class="line">                stream &gt;&gt; word;</span><br><span class="line">                id = <span class="built_in">stoi</span>(word.<span class="built_in">substr</span>(<span class="number">1</span>));</span><br><span class="line">                role = <span class="keyword">new</span> <span class="built_in">Warrior</span>(id);</span><br><span class="line">                manager-&gt;<span class="built_in">add</span>(role);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (word == <span class="string">&quot;Mage&quot;</span>) &#123;</span><br><span class="line">                stream &gt;&gt; word;</span><br><span class="line">                id = <span class="built_in">stoi</span>(word.<span class="built_in">substr</span>(<span class="number">1</span>));</span><br><span class="line">                role = <span class="keyword">new</span> <span class="built_in">Mage</span>(id);</span><br><span class="line">                manager-&gt;<span class="built_in">add</span>(role);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                stream &gt;&gt; word;</span><br><span class="line">                id = <span class="built_in">stoi</span>(word.<span class="built_in">substr</span>(<span class="number">1</span>));</span><br><span class="line">                role = <span class="keyword">new</span> <span class="built_in">Rogue</span>(id);</span><br><span class="line">                manager-&gt;<span class="built_in">add</span>(role);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;S&#x27;</span>:</span><br><span class="line">            <span class="comment">// Set</span></span><br><span class="line">            stream &gt;&gt; word;</span><br><span class="line">            id = <span class="built_in">stoi</span>(word.<span class="built_in">substr</span>(<span class="number">1</span>));</span><br><span class="line">            role = manager-&gt;<span class="built_in">findById</span>(id);</span><br><span class="line">            stream &gt;&gt; word;</span><br><span class="line">            stream &gt;&gt; num;</span><br><span class="line">            val = <span class="built_in">stof</span>(num);</span><br><span class="line">            <span class="keyword">if</span> (word == <span class="string">&quot;base_strength&quot;</span>) &#123;</span><br><span class="line">                role-&gt;base_strength = val;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (word == <span class="string">&quot;base_agility&quot;</span>) &#123;</span><br><span class="line">                role-&gt;base_agility = val;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                role-&gt;base_mana = val;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// manager-&gt;modify(role);</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;E&#x27;</span>:</span><br><span class="line">            <span class="comment">// equip</span></span><br><span class="line">            stream &gt;&gt; word; <span class="comment">// C_id</span></span><br><span class="line">            id = <span class="built_in">stoi</span>(word.<span class="built_in">substr</span>(<span class="number">1</span>));</span><br><span class="line">            role = manager-&gt;<span class="built_in">findById</span>(id);</span><br><span class="line">            <span class="keyword">if</span> (role-&gt;equip) &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;Character &quot;</span> &lt;&lt; word &lt;&lt;  <span class="string">&quot; already has equipment&quot;</span> &lt;&lt; endl;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            stream &gt;&gt; word;</span><br><span class="line">            tool = word;</span><br><span class="line">            stream &gt;&gt; word;</span><br><span class="line">            val = <span class="built_in">stof</span>(word);</span><br><span class="line">            role-&gt;<span class="built_in">equipWith</span>(tool, val);</span><br><span class="line">            <span class="comment">// manager-&gt;modify(role);</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;L&#x27;</span>:</span><br><span class="line">            <span class="comment">// List</span></span><br><span class="line">            stream &gt;&gt; word;</span><br><span class="line">            <span class="keyword">if</span> (word == <span class="string">&quot;Normal&quot;</span>) &#123;</span><br><span class="line">                manager-&gt;<span class="built_in">printById</span>();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                manager-&gt;<span class="built_in">printByPower</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;U&#x27;</span>:</span><br><span class="line">            stream &gt;&gt; word;</span><br><span class="line">            id = <span class="built_in">stoi</span>(word.<span class="built_in">substr</span>(<span class="number">1</span>));</span><br><span class="line">            role = manager-&gt;<span class="built_in">findById</span>(id);</span><br><span class="line">            role-&gt;<span class="built_in">upgrade</span>();</span><br><span class="line">            <span class="comment">// manager-&gt;modify(role);</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;P&#x27;</span>:</span><br><span class="line">            manager-&gt;<span class="built_in">showPower</span>();</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;B&#x27;</span>:</span><br><span class="line">            <span class="comment">// battle</span></span><br><span class="line">            stream &gt;&gt; c1;</span><br><span class="line">            stream &gt;&gt; c2;</span><br><span class="line">            r1 = manager-&gt;<span class="built_in">findById</span>(c1), r2 = manager-&gt;<span class="built_in">findById</span>(c2);</span><br><span class="line">            <span class="keyword">if</span> (r1-&gt;<span class="built_in">getPower</span>() &gt;= r2-&gt;<span class="built_in">getPower</span>() * <span class="number">1.1</span>) &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;C&quot;</span> &lt;&lt; r1-&gt;id &lt;&lt; <span class="string">&quot; wins&quot;</span> &lt;&lt; endl;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (r2-&gt;<span class="built_in">getPower</span>() &gt;= r1-&gt;<span class="built_in">getPower</span>() * <span class="number">1.1</span>) &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;C&quot;</span> &lt;&lt; r2-&gt;id &lt;&lt; <span class="string">&quot; wins&quot;</span> &lt;&lt; endl;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;Draw&quot;</span> &lt;&lt; endl;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;M&#x27;</span>:</span><br><span class="line">            stream &gt;&gt; word;</span><br><span class="line">            id = <span class="built_in">stoi</span>(word.<span class="built_in">substr</span>(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">for</span> (Team* t : teams) &#123;</span><br><span class="line">                <span class="keyword">if</span> (t-&gt;id == id) &#123;</span><br><span class="line">                    team = t;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            stream &gt;&gt; word;</span><br><span class="line">            stream &gt;&gt; num;</span><br><span class="line">            val = <span class="built_in">stof</span>(num);</span><br><span class="line">            team-&gt;<span class="built_in">modify</span>(word, val);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;T&#x27;</span>:</span><br><span class="line">            stream &gt;&gt; word;</span><br><span class="line">            id = <span class="built_in">stoi</span>(word.<span class="built_in">substr</span>(<span class="number">1</span>));</span><br><span class="line">            Team* newT = <span class="keyword">new</span> <span class="built_in">Team</span>(id);</span><br><span class="line">            newT-&gt;manager = manager;</span><br><span class="line">            <span class="keyword">while</span> (stream &gt;&gt; word) &#123;</span><br><span class="line">                role = manager-&gt;<span class="built_in">findById</span>(word);</span><br><span class="line">                newT-&gt;<span class="built_in">add</span>(role);</span><br><span class="line">                teams.<span class="built_in">push_back</span>(newT);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    string line;</span><br><span class="line">    <span class="built_in">getline</span>(cin, line);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="built_in">getline</span>(cin, line);</span><br><span class="line">        <span class="built_in">parseLine</span>(line);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="矩阵快速幂">矩阵快速幂</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta"># <span class="keyword">define</span> ll long long </span></span><br><span class="line"><span class="meta"># <span class="keyword">define</span> VAL 1000000007ll</span></span><br><span class="line"><span class="meta"># <span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ll data[<span class="number">10</span>][<span class="number">10</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">int</span> size;</span><br><span class="line">    <span class="built_in">Matrix</span>(<span class="type">int</span> n) &#123;</span><br><span class="line">        size = n;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    Matrix <span class="keyword">operator</span>*(<span class="type">const</span> Matrix&amp; other) <span class="type">const</span> &#123;</span><br><span class="line">        <span class="function">Matrix <span class="title">m</span><span class="params">(size)</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; size; j++) &#123;</span><br><span class="line">                ll res = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; size; k++)&#123;</span><br><span class="line">                    res += ((data[i][k] % VAL) * (other.data[k][j] % VAL));</span><br><span class="line">                    res %= VAL;</span><br><span class="line">                &#125;</span><br><span class="line">                m.data[i][j] = res;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> m;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">eye</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; size; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (i == j) &#123;</span><br><span class="line">                    data[i][j] = <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    data[i][j] = <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n, p;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; p;</span><br><span class="line">    <span class="function">Matrix <span class="title">m</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j++) &#123;</span><br><span class="line">            cin &gt;&gt; m.data[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">Matrix <span class="title">ans</span><span class="params">(n)</span></span>;</span><br><span class="line">    ans.<span class="built_in">eye</span>();</span><br><span class="line">    <span class="keyword">while</span> (p) &#123;</span><br><span class="line">        <span class="keyword">if</span> (p &amp; <span class="number">1</span>) &#123;</span><br><span class="line">            ans = ans * m;</span><br><span class="line">        &#125;</span><br><span class="line">        m = m * m;</span><br><span class="line">        p &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j++) &#123;</span><br><span class="line">            cout &lt;&lt; ans.data[i][j] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="树上寻宝">树上寻宝</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta"># <span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta"># <span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">TreeNode</span> &#123;</span><br><span class="line">    <span class="type">int</span> val;</span><br><span class="line">    vector&lt;TreeNode*&gt; children;</span><br><span class="line">    <span class="built_in">TreeNode</span>() &#123;&#125;</span><br><span class="line">    <span class="built_in">TreeNode</span>(<span class="type">int</span> x) : <span class="built_in">val</span>(x) &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Strategy</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Strategy</span>() &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">int</span> <span class="title">excute</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">int</span> <span class="title">visit</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Wang</span>: <span class="keyword">public</span> Strategy &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> p = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Wang</span>(<span class="type">int</span> cond) &#123;</span><br><span class="line">        p = cond;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">excute</span><span class="params">(TreeNode* root)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root-&gt;val &lt;= p) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">visit</span>(root);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">visit</span><span class="params">(TreeNode* root)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="comment">// dfs</span></span><br><span class="line">        <span class="type">int</span> res = root-&gt;val;</span><br><span class="line">        <span class="keyword">for</span> (TreeNode* child : root-&gt;children) &#123;</span><br><span class="line">            <span class="keyword">if</span> (child-&gt;val &gt; p) &#123;</span><br><span class="line">                res += <span class="built_in">visit</span>(child);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Hu</span>: <span class="keyword">public</span> Strategy &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> q = <span class="number">0</span>, k = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Hu</span>(<span class="type">int</span> _q, <span class="type">int</span> _k) &#123;</span><br><span class="line">        q = _q;</span><br><span class="line">        k = _k;</span><br><span class="line">    &#125; </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">excute</span><span class="params">(TreeNode* root)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">visit</span>(root);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">visit</span><span class="params">(TreeNode* root)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> res = root-&gt;val;</span><br><span class="line">        <span class="keyword">for</span> (TreeNode* child : root-&gt;children) &#123;</span><br><span class="line">            <span class="keyword">if</span> (root-&gt;val + child-&gt;val &gt; k &amp;&amp; child-&gt;val &gt; q) &#123;</span><br><span class="line">                res += <span class="built_in">visit</span>(child);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Xie</span>: <span class="keyword">public</span> Strategy &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Xie</span>()&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">excute</span><span class="params">(TreeNode* root)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">visit</span>(root);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">visit</span><span class="params">(TreeNode* root)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> res = root-&gt;val;</span><br><span class="line">        <span class="keyword">for</span> (TreeNode* child : root-&gt;children) &#123;</span><br><span class="line">            <span class="keyword">if</span> (child-&gt;val % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">                res += <span class="built_in">visit</span>(child);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n, p, q, k;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; p &gt;&gt; q &gt;&gt; k;</span><br><span class="line">    vector&lt;TreeNode*&gt; trees;</span><br><span class="line">    trees.<span class="built_in">push_back</span>(<span class="literal">nullptr</span>); <span class="comment">// 占位</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        trees.<span class="built_in">push_back</span>(<span class="keyword">new</span> <span class="built_in">TreeNode</span>());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n - <span class="number">1</span>; i++) &#123;</span><br><span class="line">        <span class="type">int</span> father, son;</span><br><span class="line">        cin &gt;&gt; father &gt;&gt; son;</span><br><span class="line">        trees[father]-&gt;children.<span class="built_in">push_back</span>(trees[son]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">        <span class="type">int</span> val;</span><br><span class="line">        cin &gt;&gt; val;</span><br><span class="line">        trees[i]-&gt;val = val;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 1号根节点</span></span><br><span class="line">    Strategy* strategy = <span class="keyword">new</span> <span class="built_in">Wang</span>(p);</span><br><span class="line">    cout &lt;&lt; strategy-&gt;<span class="built_in">excute</span>(trees[<span class="number">1</span>]);</span><br><span class="line">    strategy = <span class="keyword">new</span> <span class="built_in">Hu</span>(q, k);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; strategy-&gt;<span class="built_in">excute</span>(trees[<span class="number">1</span>]);</span><br><span class="line">    strategy = <span class="keyword">new</span> <span class="built_in">Xie</span>();</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; strategy-&gt;<span class="built_in">excute</span>(trees[<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="沙威玛商店评价系统">沙威玛商店评价系统</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">History</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">double</span> food_rating;</span><br><span class="line">    <span class="type">double</span> svc_rating;</span><br><span class="line">    <span class="type">double</span> env_rating;</span><br><span class="line">    string date;</span><br><span class="line">    <span class="type">int</span> version;</span><br><span class="line">    History* next = <span class="literal">nullptr</span>;</span><br><span class="line">    History* prev = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="built_in">History</span>(<span class="type">double</span> f, <span class="type">double</span> s, <span class="type">double</span> e, string da, <span class="type">int</span> v):</span><br><span class="line">    <span class="built_in">food_rating</span>(f), <span class="built_in">svc_rating</span>(s), <span class="built_in">env_rating</span>(e), <span class="built_in">date</span>(da), <span class="built_in">version</span>(v) &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Custom</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Custom</span>(<span class="type">int</span> d): <span class="built_in">id</span>(d), <span class="built_in">head</span>(<span class="keyword">new</span> <span class="built_in">History</span>(<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="string">&quot;&quot;</span>, <span class="number">0</span>)), <span class="built_in">version</span>(<span class="number">1</span>), <span class="built_in">latest</span>(<span class="literal">nullptr</span>) &#123;&#125; </span><br><span class="line">    <span class="type">int</span> id;</span><br><span class="line">    <span class="type">int</span> version;</span><br><span class="line">    History* head, *latest;</span><br><span class="line">    </span><br><span class="line">    <span class="function">History* <span class="title">createNewHistory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (latest == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            latest = <span class="keyword">new</span> <span class="built_in">History</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="string">&quot;&quot;</span>, version);</span><br><span class="line">            version++;</span><br><span class="line">            head-&gt;next = latest;</span><br><span class="line">            latest-&gt;prev = head;</span><br><span class="line">            <span class="keyword">return</span> latest;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            latest-&gt;next = <span class="keyword">new</span> <span class="built_in">History</span>(latest-&gt;food_rating, latest-&gt;svc_rating, latest-&gt;env_rating, latest-&gt;date, version++);</span><br><span class="line">            latest-&gt;next-&gt;prev = latest;</span><br><span class="line">            <span class="keyword">return</span> latest-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 此函数调用完毕一定一定记得更新latest为返回值</span></span><br><span class="line">    &#125; </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Manager</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;Custom*&gt; customs;</span><br><span class="line">    </span><br><span class="line">    <span class="function">Custom* <span class="title">add</span><span class="params">(<span class="type">int</span> id)</span> </span>&#123;</span><br><span class="line">        Custom* cus = <span class="keyword">new</span> <span class="built_in">Custom</span>(id);</span><br><span class="line">        customs.<span class="built_in">push_back</span>(cus);</span><br><span class="line">        <span class="keyword">return</span> cus;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">insert</span><span class="params">(<span class="type">int</span> id, <span class="type">double</span> food, <span class="type">double</span> svc, <span class="type">double</span> env, string date)</span> </span>&#123;</span><br><span class="line">        Custom* cus = <span class="built_in">add</span>(id);</span><br><span class="line">        History* init = cus-&gt;<span class="built_in">createNewHistory</span>();</span><br><span class="line">        init-&gt;food_rating = food, init-&gt;svc_rating = svc, init-&gt;env_rating = env, init-&gt;date = date;</span><br><span class="line">        cus-&gt;latest = init;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">modify</span><span class="params">(<span class="type">int</span> id, <span class="type">double</span> food, <span class="type">double</span> svc, <span class="type">double</span> env, string date)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">auto</span> it = <span class="built_in">find_if</span>(customs.<span class="built_in">begin</span>(), customs.<span class="built_in">end</span>(), [&amp;id](<span class="type">const</span> Custom* tmp) &#123;</span><br><span class="line">           <span class="keyword">return</span> tmp-&gt;id == id;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">if</span> (it == customs.<span class="built_in">end</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        History* his = (*it)-&gt;<span class="built_in">createNewHistory</span>();</span><br><span class="line">        <span class="keyword">if</span> (food != <span class="number">-1</span>) &#123;</span><br><span class="line">            his-&gt;food_rating = food;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (svc != <span class="number">-1</span>) &#123;</span><br><span class="line">            his-&gt;svc_rating = svc;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (env != <span class="number">-1</span>) &#123;</span><br><span class="line">            his-&gt;env_rating = env;</span><br><span class="line">        &#125;</span><br><span class="line">        his-&gt;date = date;</span><br><span class="line">        (*it)-&gt;latest = his; <span class="comment">// 记得对应前面所说的更新</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">del</span><span class="params">(<span class="type">int</span> id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">auto</span> it = <span class="built_in">find_if</span>(customs.<span class="built_in">begin</span>(), customs.<span class="built_in">end</span>(), [&amp;id](<span class="type">const</span> Custom* tmp) &#123;</span><br><span class="line">           <span class="keyword">return</span> tmp-&gt;id == id;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">if</span> (it == customs.<span class="built_in">end</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        customs.<span class="built_in">erase</span>(<span class="built_in">remove</span>(customs.<span class="built_in">begin</span>(), customs.<span class="built_in">end</span>(), *it));</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">showHistory</span><span class="params">(<span class="type">int</span> id, string dim)</span> </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;History:&quot;</span> &lt;&lt; endl;</span><br><span class="line">        <span class="keyword">auto</span> it = <span class="built_in">find_if</span>(customs.<span class="built_in">begin</span>(), customs.<span class="built_in">end</span>(), [&amp;id](<span class="type">const</span> Custom* tmp) &#123;</span><br><span class="line">          <span class="keyword">return</span> tmp-&gt;id == id;</span><br><span class="line">        &#125;); <span class="comment">// 数据保证能找到</span></span><br><span class="line">        <span class="comment">// dim ?</span></span><br><span class="line">        History* ptr = (*it)-&gt;latest;</span><br><span class="line">        <span class="keyword">while</span> (ptr != (*it)-&gt;head) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Version &quot;</span> &lt;&lt; ptr-&gt;version &lt;&lt; <span class="string">&quot;: &quot;</span>;</span><br><span class="line">            <span class="keyword">if</span> (dim == <span class="string">&quot;&quot;</span>) &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;Food Rating &quot;</span> &lt;&lt; ptr-&gt;food_rating &lt;&lt; <span class="string">&quot; Service Rating &quot;</span> &lt;&lt; ptr-&gt;svc_rating &lt;&lt; <span class="string">&quot; Environment Rating &quot;</span> &lt;&lt; ptr-&gt;env_rating &lt;&lt; <span class="string">&quot; Timestamp &quot;</span> &lt;&lt; ptr-&gt;date &lt;&lt; endl;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (dim == <span class="string">&quot;food&quot;</span>) &#123;</span><br><span class="line">                    cout &lt;&lt; <span class="string">&quot;Food Rating &quot;</span> &lt;&lt; ptr-&gt;food_rating;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dim == <span class="string">&quot;service&quot;</span>) &#123;</span><br><span class="line">                    cout &lt;&lt; <span class="string">&quot;Service Rating &quot;</span> &lt;&lt; ptr-&gt;svc_rating;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dim == <span class="string">&quot;environment&quot;</span>) &#123;</span><br><span class="line">                    cout &lt;&lt; <span class="string">&quot;Environment Rating &quot;</span> &lt;&lt; ptr-&gt;env_rating;</span><br><span class="line">                &#125;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot; Timestamp &quot;</span> &lt;&lt; ptr-&gt;date &lt;&lt; endl;</span><br><span class="line">            &#125;</span><br><span class="line">            ptr = ptr-&gt;prev;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">display</span><span class="params">(string dim)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">sort</span>(customs.<span class="built_in">begin</span>(), customs.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span>&amp; a, <span class="type">const</span> <span class="keyword">auto</span>&amp; b) &#123;</span><br><span class="line">            <span class="keyword">return</span> a-&gt;latest-&gt;date &gt; b-&gt;latest-&gt;date; </span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">if</span> (dim != <span class="string">&quot;&quot;</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span> cus : customs) &#123;</span><br><span class="line">                History* ptr = cus-&gt;latest;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;Customer ID &quot;</span> &lt;&lt; cus-&gt;id;</span><br><span class="line">                <span class="keyword">if</span> (dim == <span class="string">&quot;food&quot;</span>) &#123;</span><br><span class="line">                    cout &lt;&lt; <span class="string">&quot; Food Rating &quot;</span> &lt;&lt; ptr-&gt;food_rating;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dim == <span class="string">&quot;service&quot;</span>) &#123;</span><br><span class="line">                    cout &lt;&lt; <span class="string">&quot; Service Rating &quot;</span> &lt;&lt; ptr-&gt;svc_rating;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dim == <span class="string">&quot;environment&quot;</span>) &#123;</span><br><span class="line">                    cout &lt;&lt; <span class="string">&quot; Environment Rating &quot;</span> &lt;&lt; ptr-&gt;env_rating;</span><br><span class="line">                &#125;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot; Timestamp &quot;</span> &lt;&lt; ptr-&gt;date &lt;&lt; endl;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span> cus : customs) &#123;</span><br><span class="line">                History* ptr = cus-&gt;latest;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;Customer ID &quot;</span> &lt;&lt; cus-&gt;id &lt;&lt; <span class="string">&quot; Food Rating &quot;</span> &lt;&lt; ptr-&gt;food_rating &lt;&lt; <span class="string">&quot; Service Rating &quot;</span> &lt;&lt; ptr-&gt;svc_rating &lt;&lt; <span class="string">&quot; Environment Rating &quot;</span> &lt;&lt; ptr-&gt;env_rating &lt;&lt; <span class="string">&quot; Timestamp &quot;</span> &lt;&lt; ptr-&gt;date &lt;&lt; endl;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">query</span><span class="params">(<span class="type">double</span> food_lower, <span class="type">double</span> food_upper, <span class="type">double</span> svc_lower, <span class="type">double</span> svc_upper, <span class="type">double</span> env_lower, <span class="type">double</span> env_upper)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 顾客id降序</span></span><br><span class="line">        <span class="built_in">sort</span>(customs.<span class="built_in">begin</span>(), customs.<span class="built_in">end</span>(), [](<span class="type">const</span> <span class="keyword">auto</span>&amp; a, <span class="type">const</span> <span class="keyword">auto</span>&amp; b) &#123;</span><br><span class="line">           <span class="keyword">return</span> a-&gt;id &gt; b-&gt;id; </span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; cus : customs) &#123;</span><br><span class="line">            History* cu = cus-&gt;latest;</span><br><span class="line">            <span class="keyword">if</span> (cu-&gt;food_rating &gt;= food_lower &amp;&amp; cu-&gt;food_rating &lt;= food_upper &amp;&amp; cu-&gt;svc_rating &gt;= svc_lower &amp;&amp; cu-&gt;svc_rating &lt;= svc_upper &amp;&amp; cu-&gt;env_rating &gt;= env_lower &amp;&amp; cu-&gt;env_rating &lt;= env_upper) &#123;</span><br><span class="line">                <span class="comment">// legal, output information which is latest</span></span><br><span class="line">                History* ptr = cus-&gt;latest;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;Customer ID &quot;</span> &lt;&lt; cus-&gt;id &lt;&lt; <span class="string">&quot; Food Rating &quot;</span> &lt;&lt; ptr-&gt;food_rating &lt;&lt; <span class="string">&quot; Service Rating &quot;</span> &lt;&lt; ptr-&gt;svc_rating &lt;&lt; <span class="string">&quot; Environment Rating &quot;</span> &lt;&lt; ptr-&gt;env_rating &lt;&lt; <span class="string">&quot; Timestamp &quot;</span> &lt;&lt; ptr-&gt;date &lt;&lt; endl;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">parse</span><span class="params">(Manager* manager, string line)</span> </span>&#123;</span><br><span class="line">    <span class="function">istringstream <span class="title">stream</span><span class="params">(line)</span></span>;</span><br><span class="line">    string token, date;</span><br><span class="line">    <span class="type">int</span> id;</span><br><span class="line">    stream &gt;&gt; token;</span><br><span class="line">    <span class="keyword">if</span> (token == <span class="string">&quot;insert&quot;</span>) &#123;</span><br><span class="line">        stream &gt;&gt; id;</span><br><span class="line">        <span class="type">double</span> food, svc, env;</span><br><span class="line">        stream &gt;&gt; food &gt;&gt; svc &gt;&gt; env &gt;&gt; date &gt;&gt; token;</span><br><span class="line">        date = date + <span class="string">&quot; &quot;</span> + token; <span class="comment">// 拼接出完整时间</span></span><br><span class="line">        manager-&gt;<span class="built_in">insert</span>(id, food, svc, env, date);</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Review inserted successfully&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (token == <span class="string">&quot;modify&quot;</span>) &#123;</span><br><span class="line">        stream &gt;&gt; id;</span><br><span class="line">        <span class="type">double</span> food = <span class="number">-1</span>, svc = <span class="number">-1</span>, env = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span> (stream &gt;&gt; token &amp;&amp; !(token[<span class="number">0</span>] &gt;= <span class="string">&#x27;0&#x27;</span> &amp;&amp; token[<span class="number">0</span>] &lt;= <span class="string">&#x27;9&#x27;</span>)) &#123;</span><br><span class="line">            <span class="keyword">if</span> (token == <span class="string">&quot;food&quot;</span>) &#123;</span><br><span class="line">                stream &gt;&gt; food;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (token == <span class="string">&quot;service&quot;</span>) &#123;</span><br><span class="line">                stream &gt;&gt; svc;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (token == <span class="string">&quot;environment&quot;</span>) &#123;</span><br><span class="line">                stream &gt;&gt; env;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stream &gt;&gt; date; <span class="comment">// date放的时分，token现在是年月日</span></span><br><span class="line">        date = token + <span class="string">&quot; &quot;</span> + date;</span><br><span class="line">        <span class="keyword">if</span> (manager-&gt;<span class="built_in">modify</span>(id, food, svc, env, date)) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Modification successful&quot;</span> &lt;&lt; endl;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Customer ID not found, modification failed&quot;</span> &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (token == <span class="string">&quot;delete&quot;</span>) &#123;</span><br><span class="line">        stream &gt;&gt; id;</span><br><span class="line">        <span class="keyword">if</span> (manager-&gt;<span class="built_in">del</span>(id)) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Deletion successful&quot;</span> &lt;&lt; endl;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;Customer ID not found, deletion failed&quot;</span> &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (token == <span class="string">&quot;history&quot;</span>) &#123;</span><br><span class="line">        string dim;</span><br><span class="line">        stream &gt;&gt; id;</span><br><span class="line">        <span class="keyword">if</span> (!(stream &gt;&gt; dim)) &#123;</span><br><span class="line">            dim = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        manager-&gt;<span class="built_in">showHistory</span>(id, dim);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (token == <span class="string">&quot;range_query&quot;</span>) &#123;</span><br><span class="line">        <span class="comment">// TODO</span></span><br><span class="line">        <span class="type">double</span> food_lower = <span class="number">1.0</span>, food_upper = <span class="number">5.0</span>, svc_lower = <span class="number">1.0</span>, svc_upper = <span class="number">5.0</span>, env_lower = <span class="number">1.0</span>, env_upper = <span class="number">5.0</span>;</span><br><span class="line">        <span class="type">double</span> tmp;</span><br><span class="line">        <span class="type">double</span> *addr[] = &#123;&amp;food_lower, &amp;food_upper, &amp;svc_lower, &amp;svc_upper, &amp;env_lower, &amp;env_upper&#125;;</span><br><span class="line">        <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (stream &gt;&gt; tmp) &#123;</span><br><span class="line">            *(addr[i++]) = tmp;</span><br><span class="line">        &#125;</span><br><span class="line">        manager-&gt;<span class="built_in">query</span>(food_lower, food_upper, svc_lower, svc_upper, env_lower, env_upper);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (token == <span class="string">&quot;display&quot;</span>) &#123;</span><br><span class="line">        string dim;</span><br><span class="line">        <span class="keyword">if</span> (!(stream &gt;&gt; dim)) &#123;</span><br><span class="line">            dim = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        manager-&gt;<span class="built_in">display</span>(dim);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">assert</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    Manager* manager = <span class="keyword">new</span> <span class="built_in">Manager</span>();</span><br><span class="line">    string line;</span><br><span class="line">    <span class="built_in">getline</span>(cin, line);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="built_in">getline</span>(cin, line);</span><br><span class="line">        <span class="built_in">parse</span>(manager, line);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="铺木地板">铺木地板</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">length</span><span class="params">(string fl)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> fl.<span class="built_in">size</span>() / <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FloorManager</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;string&gt; floors;</span><br><span class="line">    string res = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="type">int</span> mmax = INT_MIN;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(string hex_floor)</span> </span>&#123;</span><br><span class="line">        hex_floor = hex_floor.<span class="built_in">substr</span>(<span class="number">2</span>);</span><br><span class="line">        floors.<span class="built_in">push_back</span>(hex_floor);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">del</span><span class="params">(string hex_floor)</span> </span>&#123;</span><br><span class="line">        hex_floor = hex_floor.<span class="built_in">substr</span>(<span class="number">2</span>);</span><br><span class="line">        <span class="comment">// 从木板vector中移除特定的木板</span></span><br><span class="line">        floors.<span class="built_in">erase</span>(<span class="built_in">remove</span>(floors.<span class="built_in">begin</span>(), floors.<span class="built_in">end</span>(), hex_floor));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        res = <span class="string">&quot;&quot;</span>; <span class="comment">// 清空上一次的结果</span></span><br><span class="line">        <span class="comment">// 先找到木板当中的最大值，以此为标准进行排列</span></span><br><span class="line">        mmax = INT_MIN;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span> a : floors) &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">length</span>(a) &gt; mmax) &#123;</span><br><span class="line">                mmax = <span class="built_in">length</span>(a);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// res的每一个元素代表铺木地板的每一行</span></span><br><span class="line">        <span class="type">int</span> current_len = <span class="number">0</span>;</span><br><span class="line">        string current = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span> f : floors) &#123;</span><br><span class="line">            <span class="comment">// 开始铺木地板</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">length</span>(f) + current_len &lt;= mmax) &#123;</span><br><span class="line">                <span class="comment">// 还可以在一行里</span></span><br><span class="line">                current += f;</span><br><span class="line">                current_len += <span class="built_in">length</span>(f);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 不能在同一行里了</span></span><br><span class="line">                <span class="keyword">while</span> (current_len &lt; mmax) &#123;</span><br><span class="line">                    current += <span class="string">&quot;CC&quot;</span>; <span class="comment">// 使用普通木地板填充</span></span><br><span class="line">                    current_len++;</span><br><span class="line">                &#125;</span><br><span class="line">                res += current;</span><br><span class="line">                current = <span class="string">&quot;&quot;</span> + f, current_len = <span class="built_in">length</span>(f);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 对齐最后一次的结果</span></span><br><span class="line">        <span class="keyword">while</span> (current_len &lt; mmax) &#123;</span><br><span class="line">            current += <span class="string">&quot;CC&quot;</span>;</span><br><span class="line">            current_len++;</span><br><span class="line">        &#125;</span><br><span class="line">        res += current;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="built_in">execute</span>(); <span class="comment">// 铺木地板</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; res.<span class="built_in">size</span>(); i += <span class="number">2</span> * mmax) &#123;</span><br><span class="line">            string tmp = res.<span class="built_in">substr</span>(i, <span class="number">2</span> * mmax); <span class="comment">// mmax是概念上的长度，但实际长度是两倍</span></span><br><span class="line">            <span class="comment">// 注意C++的substr第二个参数不是结束位置，是子串长度</span></span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;0x&quot;</span> &lt;&lt; tmp &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">(<span class="type">int</span> start, <span class="type">int</span> len)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">execute</span>();</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;0x&quot;</span>;</span><br><span class="line">        <span class="keyword">if</span> (start &gt;= <span class="built_in">length</span>(res)) &#123;</span><br><span class="line">            <span class="keyword">while</span> (len--) &#123;</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;CC&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            cout &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        string ans = res.<span class="built_in">substr</span>(<span class="number">2</span> * start, <span class="number">2</span> * len); <span class="comment">// len也是概念上的长度，实际长度是两倍</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="built_in">length</span>(ans) &lt; len) &#123;</span><br><span class="line">            ans += <span class="string">&quot;CC&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">FloorManager* manager;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">parse_line</span><span class="params">(string line)</span> </span>&#123;</span><br><span class="line">    <span class="function">istringstream <span class="title">stream</span><span class="params">(line)</span></span>;</span><br><span class="line">    string token;</span><br><span class="line">    stream &gt;&gt; token;</span><br><span class="line">    <span class="keyword">if</span> (token == <span class="string">&quot;add&quot;</span>) &#123;</span><br><span class="line">        stream &gt;&gt; token;</span><br><span class="line">        manager-&gt;<span class="built_in">add</span>(token);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (token == <span class="string">&quot;del&quot;</span>) &#123;</span><br><span class="line">        stream &gt;&gt; token;</span><br><span class="line">        manager-&gt;<span class="built_in">del</span>(token);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (token == <span class="string">&quot;print&quot;</span>) &#123;</span><br><span class="line">        <span class="type">int</span> start;</span><br><span class="line">        <span class="keyword">if</span> (stream &gt;&gt; start) &#123;</span><br><span class="line">            <span class="type">int</span> len;</span><br><span class="line">            stream &gt;&gt; len;</span><br><span class="line">            manager-&gt;<span class="built_in">print</span>(start, len);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            manager-&gt;<span class="built_in">print</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    manager = <span class="keyword">new</span> <span class="built_in">FloorManager</span>();</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    string line;</span><br><span class="line">    <span class="built_in">getline</span>(cin, line);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="built_in">getline</span>(cin, line);</span><br><span class="line">        <span class="comment">// 分析输入的命令行</span></span><br><span class="line">        <span class="built_in">parse_line</span>(line);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="sort命令">sort命令</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta"># <span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Print</span><span class="params">(vector&lt;string&gt; ss)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (string s : ss) &#123;</span><br><span class="line">        cout &lt;&lt; s &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n, c;</span><br><span class="line">    string line;</span><br><span class="line">    vector&lt;string&gt; inputs;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="built_in">getline</span>(cin, line);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="built_in">getline</span>(cin, line);</span><br><span class="line">        inputs.<span class="built_in">push_back</span>(line);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 读入命令</span></span><br><span class="line">    cin &gt;&gt; c;</span><br><span class="line">    vector&lt;string&gt; cmds;</span><br><span class="line">    <span class="built_in">getline</span>(cin, line);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; c; i++) &#123;</span><br><span class="line">        <span class="built_in">getline</span>(cin, line);</span><br><span class="line">        <span class="keyword">if</span> (line[<span class="number">0</span>] == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">            <span class="built_in">sort</span>(inputs.<span class="built_in">begin</span>(), inputs.<span class="built_in">end</span>(), [](string a, string b) &#123;</span><br><span class="line">               <span class="keyword">return</span> a &lt; b; </span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="built_in">Print</span>(inputs);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (line[<span class="number">0</span>] == <span class="string">&#x27;n&#x27;</span>) &#123;</span><br><span class="line">            <span class="built_in">sort</span>(inputs.<span class="built_in">begin</span>(), inputs.<span class="built_in">end</span>(), [](string a, string b) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">stoi</span>(a) &lt; <span class="built_in">stoi</span>(b);</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="built_in">Print</span>(inputs);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (line[<span class="number">0</span>] == <span class="string">&#x27;r&#x27;</span>) &#123;</span><br><span class="line">            <span class="built_in">sort</span>(inputs.<span class="built_in">begin</span>(), inputs.<span class="built_in">end</span>(), [](string a, string b) &#123;</span><br><span class="line">               <span class="keyword">return</span> a &gt; b; </span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="built_in">Print</span>(inputs);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (line[<span class="number">0</span>] == <span class="string">&#x27;i&#x27;</span>) &#123;</span><br><span class="line">            <span class="built_in">sort</span>(inputs.<span class="built_in">begin</span>(), inputs.<span class="built_in">end</span>(), [](string a, string b) &#123;</span><br><span class="line">               <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; a.<span class="built_in">size</span>() &amp;&amp; k &lt; b.<span class="built_in">size</span>(); k++) &#123;</span><br><span class="line">                   <span class="keyword">if</span> (<span class="built_in">tolower</span>(a[k]) &lt; <span class="built_in">tolower</span>(b[k])) &#123;</span><br><span class="line">                       <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">tolower</span>(a[k] &gt; <span class="built_in">tolower</span>(b[k]))) &#123;</span><br><span class="line">                       <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="keyword">if</span> (a.<span class="built_in">size</span>() &gt; b.<span class="built_in">size</span>()) &#123;</span><br><span class="line">                   <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">               &#125; <span class="keyword">else</span> <span class="keyword">if</span> (a.<span class="built_in">size</span>() &lt; b.<span class="built_in">size</span>()) &#123;</span><br><span class="line">                   <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="keyword">return</span> a &lt; b;</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="built_in">Print</span>(inputs);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (line[<span class="number">0</span>] == <span class="string">&#x27;d&#x27;</span>) &#123;</span><br><span class="line">            <span class="built_in">sort</span>(inputs.<span class="built_in">begin</span>(), inputs.<span class="built_in">end</span>(), [](string a, string b) &#123;</span><br><span class="line">               <span class="type">int</span> l = <span class="number">0</span>, r = <span class="number">0</span>;</span><br><span class="line">               <span class="keyword">while</span> (l &lt; a.<span class="built_in">size</span>() &amp;&amp; r &lt; b.<span class="built_in">size</span>()) &#123;</span><br><span class="line">                   <span class="keyword">while</span> (l &lt; a.<span class="built_in">size</span>() &amp;&amp; <span class="built_in">ispunct</span>(a[l])) &#123;</span><br><span class="line">                       l++;</span><br><span class="line">                   &#125;</span><br><span class="line">                   <span class="keyword">while</span> (r &lt; b.<span class="built_in">size</span>() &amp;&amp; <span class="built_in">ispunct</span>(b[r])) &#123;</span><br><span class="line">                       r++;</span><br><span class="line">                   &#125;</span><br><span class="line">                   <span class="keyword">if</span> (a[l] &lt; b[r]) &#123;</span><br><span class="line">                       <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (a[l] &gt; b[r]) &#123;</span><br><span class="line">                       <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                   &#125;</span><br><span class="line">                   l++, r++;</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="keyword">if</span> (l &gt;= a.<span class="built_in">size</span>() &amp;&amp; r &gt;= b.<span class="built_in">size</span>()) &#123;</span><br><span class="line">                   <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="keyword">if</span> (l &lt; a.<span class="built_in">size</span>()) &#123;</span><br><span class="line">                   <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">               &#125; </span><br><span class="line">               <span class="keyword">if</span> (r &lt; a.<span class="built_in">size</span>()) &#123;</span><br><span class="line">                   <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">               &#125;</span><br><span class="line">              <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="built_in">Print</span>(inputs);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="植物大战僵尸">植物大战僵尸</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">User</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> hp = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> atk = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> speed = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> row = <span class="number">0</span>, col = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">User</span>(<span class="type">int</span> h): <span class="built_in">hp</span>(h) &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">int</span> <span class="title">alive</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> hp &gt; <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">is_plant</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">is_zombie</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Zombie</span>: <span class="keyword">public</span> User &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Zombie</span>(<span class="type">int</span> h, <span class="type">int</span> a, <span class="type">int</span> s, <span class="type">int</span> x, <span class="type">int</span> y): <span class="built_in">User</span>(h) &#123;</span><br><span class="line">      atk = a;</span><br><span class="line">      speed = s;</span><br><span class="line">      row = x;</span><br><span class="line">      col = y;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">is_zombie</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span> &amp;&amp; <span class="built_in">alive</span>();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Plant</span>: <span class="keyword">public</span> User &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Plant</span>(<span class="type">int</span> h): <span class="built_in">User</span>(h) &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">is_pea</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">is_nut</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">is_potato</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">is_plant</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span> &amp;&amp; <span class="built_in">alive</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Pea</span>: <span class="keyword">public</span> Plant &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Pea</span>(<span class="type">int</span> h, <span class="type">int</span> a): <span class="built_in">Plant</span>(h) &#123;</span><br><span class="line">        atk = a;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">is_pea</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span> &amp;&amp; <span class="built_in">alive</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Nut</span>: <span class="keyword">public</span> Plant &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Nut</span>(<span class="type">int</span> h): <span class="built_in">Plant</span>(h) &#123;&#125;</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">is_nut</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span> &amp;&amp; <span class="built_in">alive</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Potato</span>: <span class="keyword">public</span> Plant &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">bool</span> sleep = <span class="literal">true</span>;</span><br><span class="line">    <span class="built_in">Potato</span>(<span class="type">int</span> a): <span class="built_in">Plant</span>(<span class="number">10</span>) &#123;</span><br><span class="line">        atk = a;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">is_potato</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span> &amp;&amp; <span class="built_in">alive</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Game</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;vector&lt;User*&gt;&gt;&gt; plate;</span><br><span class="line">    vector&lt;Plant*&gt; plants;</span><br><span class="line">    vector&lt;Zombie*&gt; zombies;</span><br><span class="line">    <span class="type">int</span> plant_cnt = <span class="number">0</span>, zombie_cnt = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">Game</span>() &#123;</span><br><span class="line">        plate.<span class="built_in">resize</span>(<span class="number">5</span>, vector&lt;vector&lt;User*&gt;&gt;(<span class="number">10</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">add_plant</span><span class="params">(Plant* p, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">        plants.<span class="built_in">push_back</span>(p);</span><br><span class="line">        plate[row][col].<span class="built_in">push_back</span>(p);</span><br><span class="line">        plant_cnt++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">add_zombie</span><span class="params">(Zombie* z, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">        zombies.<span class="built_in">push_back</span>(z);</span><br><span class="line">        plate[row][col].<span class="built_in">push_back</span>(z);</span><br><span class="line">        zombie_cnt++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">finish</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (zombie_cnt == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>; <span class="comment">// 植物胜利</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span> z : zombies) &#123;</span><br><span class="line">                <span class="keyword">if</span> (z-&gt;col &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">0</span>; <span class="comment">// 僵尸胜利</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>; <span class="comment">// 继续</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">plant_attack</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">9</span>; j++) &#123;</span><br><span class="line">                vector&lt;User*&gt; ps = plate[i][j];</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">auto</span> a : ps) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (!a-&gt;<span class="built_in">alive</span>() || a-&gt;<span class="built_in">is_zombie</span>()) &#123;</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    Plant* plant = <span class="built_in">static_cast</span>&lt;Plant*&gt;(a);</span><br><span class="line">                    <span class="keyword">if</span> (plant-&gt;<span class="built_in">is_pea</span>()) &#123;</span><br><span class="line">                        <span class="comment">// 豌豆射手</span></span><br><span class="line">                        <span class="type">int</span> z_idx = <span class="number">-1</span>; <span class="comment">// 前方僵尸的位置</span></span><br><span class="line">                        <span class="keyword">for</span> (<span class="type">int</span> k = j; k &lt; <span class="number">10</span> &amp;&amp; z_idx == <span class="number">-1</span>; k++) &#123;</span><br><span class="line">                            vector&lt;User*&gt; us = plate[i][k];</span><br><span class="line">                            <span class="keyword">for</span> (<span class="keyword">auto</span> a : us) &#123;</span><br><span class="line">                                <span class="keyword">if</span> (a-&gt;<span class="built_in">alive</span>() &amp;&amp; a-&gt;<span class="built_in">is_zombie</span>()) &#123;</span><br><span class="line">                                    z_idx = k;</span><br><span class="line">                                    <span class="keyword">break</span>;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">if</span> (z_idx != <span class="number">-1</span>) &#123;</span><br><span class="line">                            <span class="comment">// 攻击</span></span><br><span class="line">                            vector&lt;User*&gt; us = plate[i][z_idx];</span><br><span class="line">                            <span class="keyword">for</span> (<span class="keyword">auto</span> a : us) &#123;</span><br><span class="line">                                <span class="keyword">if</span> (a-&gt;<span class="built_in">alive</span>() &amp;&amp; a-&gt;<span class="built_in">is_zombie</span>()) &#123;</span><br><span class="line">                                    a-&gt;hp -= plant-&gt;atk;</span><br><span class="line">                                    <span class="keyword">if</span> (!a-&gt;<span class="built_in">alive</span>()) &#123;</span><br><span class="line">                                        zombie_cnt--;</span><br><span class="line">                                    &#125;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (plant-&gt;<span class="built_in">is_potato</span>()) &#123;</span><br><span class="line">                        Potato* potato = <span class="built_in">static_cast</span>&lt;Potato*&gt;(plant);</span><br><span class="line">                        <span class="keyword">if</span> (!potato-&gt;sleep) &#123;</span><br><span class="line">                            <span class="keyword">for</span> (<span class="type">int</span> r = <span class="built_in">max</span>(<span class="number">0</span>, i - <span class="number">1</span>); r &lt;= <span class="built_in">min</span>(<span class="number">4</span>, i + <span class="number">1</span>); r++) &#123;</span><br><span class="line">                                <span class="keyword">for</span> (<span class="type">int</span> c = <span class="built_in">max</span>(<span class="number">0</span>, j - <span class="number">1</span>); c &lt;= <span class="built_in">min</span>(<span class="number">9</span>, j + <span class="number">1</span>); c++) &#123;</span><br><span class="line">                                    vector&lt;User*&gt; us = plate[r][c];</span><br><span class="line">                                    <span class="keyword">for</span> (<span class="keyword">auto</span> a : us) &#123;</span><br><span class="line">                                        <span class="keyword">if</span> (a-&gt;<span class="built_in">alive</span>() &amp;&amp; a-&gt;<span class="built_in">is_zombie</span>()) &#123;</span><br><span class="line">                                            a-&gt;hp -= plant-&gt;atk;</span><br><span class="line">                                            <span class="keyword">if</span> (!a-&gt;<span class="built_in">alive</span>()) &#123;</span><br><span class="line">                                                zombie_cnt--;</span><br><span class="line">                                            &#125;</span><br><span class="line">                                        &#125;</span><br><span class="line">                                    &#125;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                            plant_cnt--; <span class="comment">//地雷必死</span></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">zombie_move</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> z : zombies) &#123;</span><br><span class="line">            <span class="type">int</span> old_row = z-&gt;row, old_col = z-&gt;col;</span><br><span class="line">            <span class="keyword">if</span> (z-&gt;<span class="built_in">alive</span>() &amp;&amp; z-&gt;col &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="type">int</span> speed = z-&gt;speed;</span><br><span class="line">                <span class="type">bool</span> block = <span class="literal">false</span>;</span><br><span class="line">                <span class="keyword">while</span> (speed &amp;&amp; !block) &#123;</span><br><span class="line">                    vector&lt;User*&gt; tmp = plate[z-&gt;row][z-&gt;col];</span><br><span class="line">                    <span class="comment">// 判断是否有植物阻挡</span></span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">auto</span> p : tmp) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (p-&gt;<span class="built_in">alive</span>() &amp;&amp; p-&gt;<span class="built_in">is_plant</span>()) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (<span class="built_in">static_cast</span>&lt;Plant*&gt;(p)-&gt;<span class="built_in">is_potato</span>()) &#123;</span><br><span class="line">                                <span class="built_in">static_cast</span>&lt;Potato*&gt;(p)-&gt;sleep = <span class="literal">false</span>;</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                block = <span class="literal">true</span>;</span><br><span class="line">                                <span class="keyword">break</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (block) &#123;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    speed--;</span><br><span class="line">                    z-&gt;col--;</span><br><span class="line">                    <span class="keyword">if</span> (z-&gt;col &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 最后停下的位置可能触发土豆地雷</span></span><br><span class="line">                <span class="keyword">if</span> (z-&gt;col &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                    vector&lt;User*&gt; tmp = plate[z-&gt;row][z-&gt;col];</span><br><span class="line">                    <span class="comment">// 判断是否有植物阻挡</span></span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">auto</span> p : tmp) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (p-&gt;<span class="built_in">alive</span>() &amp;&amp; p-&gt;<span class="built_in">is_plant</span>()) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (<span class="built_in">static_cast</span>&lt;Plant*&gt;(p)-&gt;<span class="built_in">is_potato</span>()) &#123;</span><br><span class="line">                                <span class="built_in">static_cast</span>&lt;Potato*&gt;(p)-&gt;sleep = <span class="literal">false</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    plate[old_row][old_col].<span class="built_in">erase</span>(<span class="built_in">remove</span>(plate[old_row][old_col].<span class="built_in">begin</span>(), plate[old_row][old_col].<span class="built_in">end</span>(), z));</span><br><span class="line">                    plate[z-&gt;row][z-&gt;col].<span class="built_in">push_back</span>(z);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">zombie_attack</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> z : zombies) &#123;</span><br><span class="line">            <span class="keyword">if</span> (z-&gt;<span class="built_in">alive</span>() &amp;&amp; z-&gt;col &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                vector&lt;User*&gt; tmp = plate[z-&gt;row][z-&gt;col];</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">auto</span> p : tmp) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (p-&gt;<span class="built_in">alive</span>() &amp;&amp; p-&gt;<span class="built_in">is_plant</span>()) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (!<span class="built_in">static_cast</span>&lt;Plant*&gt;(p)-&gt;<span class="built_in">is_potato</span>()) &#123;</span><br><span class="line">                            p-&gt;hp -= z-&gt;atk; <span class="comment">// 攻击</span></span><br><span class="line">                            <span class="keyword">if</span> (!p-&gt;<span class="built_in">alive</span>()) &#123;</span><br><span class="line">                                plant_cnt--;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> turn = <span class="number">1</span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        cout &lt;&lt; turn++ &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; plant_cnt &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; zombie_cnt &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">play</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            <span class="built_in">plant_attack</span>();</span><br><span class="line">            <span class="built_in">zombie_move</span>();</span><br><span class="line">            <span class="built_in">zombie_attack</span>();</span><br><span class="line">            <span class="built_in">print</span>();</span><br><span class="line">        &#125; <span class="keyword">while</span> (<span class="built_in">finish</span>() == <span class="number">-1</span>);</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">finish</span>() == <span class="number">1</span>) &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;plants win&quot;</span> &lt;&lt; endl;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;zombies win&quot;</span> &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Game* game = <span class="keyword">new</span> <span class="built_in">Game</span>();</span><br><span class="line">    <span class="type">int</span> pcnt, zcnt;</span><br><span class="line">    cin &gt;&gt; pcnt &gt;&gt; zcnt;</span><br><span class="line">    <span class="keyword">while</span>(pcnt--) &#123;</span><br><span class="line">        string cmd;</span><br><span class="line">        cin &gt;&gt; cmd;</span><br><span class="line">        <span class="type">int</span> hp, atk, x, y;</span><br><span class="line">        <span class="keyword">if</span> (cmd == <span class="string">&quot;pea&quot;</span>) &#123;</span><br><span class="line">            cin &gt;&gt; hp &gt;&gt; atk &gt;&gt; x &gt;&gt; y;</span><br><span class="line">            Pea* pea = <span class="keyword">new</span> <span class="built_in">Pea</span>(hp, atk);</span><br><span class="line">            game-&gt;<span class="built_in">add_plant</span>(pea, x, y);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmd == <span class="string">&quot;nut&quot;</span>) &#123;</span><br><span class="line">            cin &gt;&gt; hp &gt;&gt; x &gt;&gt; y;</span><br><span class="line">            Nut* nut = <span class="keyword">new</span> <span class="built_in">Nut</span>(hp);</span><br><span class="line">            game-&gt;<span class="built_in">add_plant</span>(nut, x, y);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            cin &gt;&gt; atk &gt;&gt; x &gt;&gt; y;</span><br><span class="line">            Potato* potato = <span class="keyword">new</span> <span class="built_in">Potato</span>(atk);</span><br><span class="line">            game-&gt;<span class="built_in">add_plant</span>(potato, x, y);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (zcnt--) &#123;</span><br><span class="line">        <span class="type">int</span> hp, atk, speed, x;</span><br><span class="line">        <span class="type">int</span> y = <span class="number">9</span>;</span><br><span class="line">        cin &gt;&gt; hp &gt;&gt; atk &gt;&gt; speed &gt;&gt; x;</span><br><span class="line">        Zombie* zombie = <span class="keyword">new</span> <span class="built_in">Zombie</span>(hp, atk, speed, x, y);</span><br><span class="line">        game-&gt;<span class="built_in">add_zombie</span>(zombie, x, y);</span><br><span class="line">    &#125;</span><br><span class="line">    game-&gt;<span class="built_in">play</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 软件学院 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++时间处理</title>
      <link href="/ymhui.github.io/2024/12/18/C-%E6%97%B6%E9%97%B4%E5%A4%84%E7%90%86/"/>
      <url>/ymhui.github.io/2024/12/18/C-%E6%97%B6%E9%97%B4%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="从标准时间转化为Unix时间戳">从标准时间转化为Unix时间戳</h2><p>使用<code>&lt;ctime&gt;</code>库</p><p>核心数据结构: <code>tm, time_t</code></p><p>核心API:</p><p><code>time_t mktime(tm* __std_time__);</code></p><p>将特定字符串根据指定格式提取出时间格式数据并存入tm地址中: <code>std::istringstream &gt;&gt; get_time(tm* __addr__, string format);</code></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    tm time_struct = &#123;&#125;;</span><br><span class="line">    time_struct.tm_year = <span class="number">2024</span> - <span class="number">1900</span>; <span class="comment">// 始于1900</span></span><br><span class="line">    time_struct.tm_mon = <span class="number">12</span> - <span class="number">1</span>; <span class="comment">// 从0开始</span></span><br><span class="line">    time_struct.tm_mday = <span class="number">18</span>; <span class="comment">// 天</span></span><br><span class="line">    time_struct.tm_hour = <span class="number">8</span>; <span class="comment">// 时</span></span><br><span class="line">    time_struct.tm_min = <span class="number">52</span>; <span class="comment">// 分</span></span><br><span class="line">    time_struct.tm_sec = <span class="number">30</span>; <span class="comment">// 秒</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// std::mktime将标准时间转化为Unix时间戳</span></span><br><span class="line">    <span class="comment">// time_t mktime(tm* __std_time__);</span></span><br><span class="line">    <span class="type">time_t</span> timestamp = <span class="built_in">mktime</span>(&amp;time_struct);</span><br><span class="line">    <span class="keyword">if</span> (timestamp == <span class="number">-1</span>) &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;failed&quot;</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        cout &lt;&lt; timestamp &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用 std::istringstream &gt;&gt; get_time(tm* __addr, string format);</span></span><br><span class="line">    time_struct = &#123;&#125;;</span><br><span class="line">    <span class="function">istringstream <span class="title">time_stream</span><span class="params">(<span class="string">&quot;2024-12-18T08-52-30&quot;</span>)</span></span>;</span><br><span class="line">    time_stream &gt;&gt; <span class="built_in">get_time</span>(&amp;time_struct, <span class="string">&quot;%Y-%m-%dT%H-%M-%S&quot;</span>);</span><br><span class="line">    timestamp = <span class="built_in">mktime</span>(&amp;time_struct);</span><br><span class="line">    cout &lt;&lt; timestamp &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1734483150</span><br><span class="line">1734483150</span><br></pre></td></tr></table></figure><h2 id="从Unix时间戳转化为标准时间">从Unix时间戳转化为标准时间</h2><p>核心数据结构: <code>tm, time_t</code></p><p>核心API:</p><p>转化时间戳为<strong>tm结构体指针</strong>： <code>tm* localtime(time_t* __stamp__);</code> , <code>tm* gmtime(time_t* __stamp);</code></p><p>根据<strong>tm结构体指针</strong>格式化输出时间: <code>put_time(tm* __tms__, string __format__);</code></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">time_t</span> timestamp = <span class="number">1734483150</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// std::localtime 转化为本地时间(tm结构体指针)</span></span><br><span class="line">    <span class="comment">// tm* localtime(time_t* __stamp__);</span></span><br><span class="line">    tm* lc_time = <span class="built_in">localtime</span>(&amp;timestamp);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// std::put_time 格式化输出</span></span><br><span class="line">    <span class="comment">// put_time(tm* __tms__, string __format__);</span></span><br><span class="line">    <span class="keyword">auto</span> local_time = <span class="built_in">put_time</span>(lc_time, <span class="string">&quot;%Y-%m-%dT%H-%M-%S&quot;</span>);</span><br><span class="line">    cout &lt;&lt; local_time &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// std::gmtime 转化为UTC时间(tm结构体指针)</span></span><br><span class="line">    <span class="comment">// tm* gmtime(time_t* __stamp);</span></span><br><span class="line">    tm* utc_tm = <span class="built_in">gmtime</span>(&amp;timestamp);</span><br><span class="line">    <span class="keyword">auto</span> utc_time = <span class="built_in">put_time</span>(utc_tm, <span class="string">&quot;%Y-%m-%dT%H-%M-%S&quot;</span>);</span><br><span class="line">    cout &lt;&lt; utc_time &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2024-12-18T08-52-30</span><br><span class="line">2024-12-18T00-52-30</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python代码注释抽取并移动</title>
      <link href="/ymhui.github.io/2024/12/17/python%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A%E6%8A%BD%E5%8F%96%E5%B9%B6%E7%A7%BB%E5%8A%A8/"/>
      <url>/ymhui.github.io/2024/12/17/python%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A%E6%8A%BD%E5%8F%96%E5%B9%B6%E7%A7%BB%E5%8A%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="抽取策略">抽取策略</h2><p>准备源代码和对应的带注释版本，须注意LLM生成的带注释版本极有<strong>可能会吞掉注释之后具体实现的代码</strong>（LLM特色），因此我们才需要抽取注释并移动</p><h2 id="步骤">步骤</h2><ol><li>使用<code>tree-sitter</code>构造注释代码的语法树，并获取根节点</li><li>遍历注释版本的代码语法树，当遇到注释节点时<ol><li>向上不断找父节点，直到如下两种情况之一</li><li>如果父节点是<code>block</code>，则上一节点就是这个注释处于的层级位置<ol><li>比如可能是<code>class_definition</code>, <code>function_definition</code></li><li>还有可能是<code>for_statement</code>这种<code>statement</code>节点</li></ol></li><li>如果父节点是<code>module</code>，则说明这个注释写在代码主体之外</li></ol></li><li>对于停止父节点是<code>block</code>的，再获取其上的一级父节点，找到<code>block</code>在该节点中的子节点下标<code>index</code>，则在源代码中<strong>该注释应当插在注释代码中子节点下标为index - 2(index - 1可能是’:')的文本在源代码中对应文本的后面</strong></li><li>对于停止父节点是<code>module</code>的，直接插在源代码最上面</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据分析期末项目——构建数据库</title>
      <link href="/ymhui.github.io/2024/11/30/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%9C%9F%E6%9C%AB%E9%A1%B9%E7%9B%AE%E2%80%94%E2%80%94%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
      <url>/ymhui.github.io/2024/11/30/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%9C%9F%E6%9C%AB%E9%A1%B9%E7%9B%AE%E2%80%94%E2%80%94%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h1>如何构建期末项目中的数据库</h1><h2 id="步骤">步骤</h2><ol><li><p>获取文件<code>papers_with_predictions.csv</code></p><p>可见仓库<a href="https://github.com/NJU-ymhui/big-data-analysis-final-classifier%EF%BC%8C%E4%BD%8D%E4%BA%8Emain%E5%88%86%E6%94%AF%E4%B8%8B%E7%9A%84%60mlp/%60%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B">https://github.com/NJU-ymhui/big-data-analysis-final-classifier，位于main分支下的`mlp/`文件夹下</a></p></li><li><p>可以选择运行MySql手动在数据库中添加对应的数据表，也可以在配置文件<code>application.yml</code>中添加如下内容</p><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spring:</span>  </span><br><span class="line">  <span class="attr">jpa:</span></span><br><span class="line">    <span class="attr">hibernate:</span></span><br><span class="line">      <span class="attr">ddl-auto:</span> <span class="string">update</span></span><br><span class="line">    <span class="attr">database-platform:</span> <span class="string">org.hibernate.dialect.MySQL5Dialect</span></span><br></pre></td></tr></table></figure><p>然后启动spring后端，这将会在数据库中自动创建数据表</p><p>手动添加数据表的示例如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> paper (</span><br><span class="line">    id <span class="type">INT</span> AUTO_INCREMENT <span class="keyword">PRIMARY</span> KEY,</span><br><span class="line">    title <span class="type">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    abstract TEXT <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    category <span class="type">VARCHAR</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    <span class="keyword">year</span> <span class="type">INT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure></li><li><p>执行<code>mysql -u root -p</code>登录数据库，之后执行下面的指令</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">use database big_data_analysis; <span class="comment">-- 请使用你自己的数据库名</span></span><br><span class="line">LOAD DATA INFILE <span class="string">&#x27;path\\to\\your\\papers_with_predictions.csv&#x27;</span></span><br><span class="line"><span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8mb4</span><br><span class="line"><span class="keyword">INTO</span> <span class="keyword">TABLE</span> paper</span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">ENCLOSED <span class="keyword">BY</span> <span class="string">&#x27;&quot;&#x27;</span></span><br><span class="line">LINES TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">IGNORE <span class="number">1</span> <span class="keyword">ROWS</span></span><br><span class="line">(title, abstract, category, <span class="variable">@year</span>, id)</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="built_in">CAST</span>(<span class="variable">@year</span> <span class="keyword">AS</span> UNSIGNED);</span><br></pre></td></tr></table></figure></li></ol><h2 id="可能遇到的问题">可能遇到的问题</h2><ol><li><pre><code class="language-sql">ERROR 1074 (42000): Column length too big for column 'abstract' (max = 16383); use BLOB or TEXT instead<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">字符串过长，执行</span><br><span class="line"></span><br><span class="line">```sql</span><br><span class="line">ALTER TABLE paper MODIFY COLUMN abstract TEXT;</span><br></pre></td></tr></table></figure></code></pre></li><li><pre><code class="language-sql">ERROR 1406 (22001): Data too long for column 'abstract' at row 1<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">   解决方法同上</span><br><span class="line"></span><br><span class="line">3.  ```sql</span><br><span class="line">   No such file or directory</span><br></pre></td></tr></table></figure>确保路径中没有中文</code></pre></li><li><pre><code class="language-sql">ERROR 1290 (HY000): The MySQL server is running with the --secure-file-priv option so it cannot execute this statement<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">这是因为MySQL服务启用了`secure-file-priv`, 这将导致只能加载指定路径的文件</span><br><span class="line"></span><br><span class="line">使用`SHOW VARIABLES LIKE &#x27;secure_file_priv&#x27;;`查看输出，若输出</span><br><span class="line"></span><br><span class="line">```sql</span><br><span class="line">+------------------+-------+</span><br><span class="line">| Variable_name    | Value |</span><br><span class="line">+------------------+-------+</span><br><span class="line">| secure_file_priv | NULL  |</span><br></pre></td></tr></table></figure>则说明没有任何路径（NULL）的文件可以加载，请找到MySQL配置文件`my.ini`或`my.conf`，通常位于MySQL的安装位置(比如我的在`E:\MySQL\mysql-8.0.36-winx64\mysql-8.0.36-winx64`下)或者`C:\\AppData\\MySQL\\对应版本`或`C:\\Program Files\\MySQL\\对应版本`下，编辑文件，在`[mysqld]`下添加规则<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">secure-file-priv</span>=<span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure>以此禁用`secure-file-priv`之后重启MySQL使用`win+x`以管理员身份运行终端，执行<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">&gt; net stop mysql</span><br><span class="line">&gt; net <span class="built_in">start</span> mysql</span><br></pre></td></tr></table></figure>启动成功后，再次登录MySQL，重复**步骤3**</code></pre></li><li><pre><code class="language-sql">ERROR 1300 (HY000): Invalid utf8mb4 character string: '&quot;...&quot;'<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">字符集不匹配，同4修改配置文件中所有涉及编码的地方为utf8，重启MySQL</span><br><span class="line"></span><br><span class="line">之后登录时`mysql -u root -p --default-character-set=utf8`指定字符集</span><br><span class="line"></span><br><span class="line">*注：如果还不行可能还需要把已经建好的数据库和数据表的字符集也改掉，使用*</span><br><span class="line"></span><br><span class="line">```sql</span><br><span class="line">ALTER DATABASE big_data_analysis</span><br><span class="line">CHARACTER SET utf8</span><br><span class="line">COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line">ALTER TABLE paper</span><br><span class="line">CONVERT TO CHARACTER SET utf8</span><br><span class="line">COLLATE utf8_general_ci;</span><br></pre></td></tr></table></figure>如果还是不行（可能csv的数据被污染了），请使用https://github.com/NJU-ymhui/big-data-analysis-final-classifier下的`clean.py`清洗csv表格</code></pre></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 软件学院 </tag>
            
            <tag> 课程作业 </tag>
            
            <tag> 大数据分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络作业报告</title>
      <link href="/ymhui.github.io/2024/11/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/"/>
      <url>/ymhui.github.io/2024/11/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<h1>第三次作业报告</h1><p>为了完成对这101个类别的分类任务，使用<code>vgg</code>带块的卷积神经网络</p><h2 id="VGG">VGG</h2><p>网络结构的实现如下</p><p><code>vgg.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现一个VGG块</span></span><br><span class="line"><span class="string">    :param num_convs: 卷积层数量</span></span><br><span class="line"><span class="string">    :param in_channels: 输入通道数量</span></span><br><span class="line"><span class="string">    :param out_channels: 输出通道数量</span></span><br><span class="line"><span class="string">    :return: 由卷积层、激活函数和池化层组成的序列模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># vgg</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">3</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全连接层部分的输入维度依赖于最后一个卷积块的输出尺寸</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks,</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">102</span>)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>下面对这个网络结构进行分析：</p><ol><li><p>首先定义基本的vgg块：vgg块的初始化依赖外部传入的参数，即卷积层的数量、输入通道的数量、输出通道的数量，使用这三个参数初始化基本块（卷积层、输入通道和输出通道），返回由卷积层、激活函数和池化层组成的序列模型</p></li><li><p>然后使用这个vgg块定义vgg网络，因为彩色图像共有r, g, b三个输入通道，所有使用三个输入通道；之后通过一个循环遍历输入参数更新卷积层</p></li><li><p>vgg网络返回一个模型，该模型将卷积层输出的多维张量展平为一维向量，之后使用三个全连接层：</p><p><strong>第一个全连接层 (<code>nn.Linear(out_channels \* 7 \* 7, 4096)</code>)</strong>：</p><ul><li>输入维度是展平后的输出尺寸，即卷积层输出的通道数（<code>out_channels</code>）乘以空间尺寸（<code>7x7</code>），即 <code>out_channels * 7 * 7</code>。</li><li>输出维度是 4096，即该层有 4096 个神经元。</li></ul><p><strong><code>nn.ReLU()</code></strong>：对输出应用 ReLU 激活函数，增加非线性。</p><p><strong><code>nn.Dropout(0.5)</code></strong>：在训练过程中，50% 的神经元会被随机丢弃，以减少过拟合。</p><p><strong>第二个全连接层 (<code>nn.Linear(4096, 4096)</code>)</strong>：这是一个具有 4096 个神经元的全连接层，输入和输出大小均为 4096。</p><p><strong>第三个全连接层 (<code>nn.Linear(4096, 102)</code>)</strong>：</p><ul><li>输出的维度是 102，表示网络的最终输出是 102 个类别的概率分布（假设你处理的是 102 个类的分类问题）</li></ul></li></ol><h2 id="超参数与优化器">超参数与优化器</h2><p>训练时使用的优化器为随机梯度下降优化，损失函数使用交叉熵损失函数，学习率设置为0.01，训练轮数设置为20，设置批量大小batch_size为32。</p><h2 id="结果">结果</h2><p>模型训练与测试的结果如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">training on cuda</span><br><span class="line">epoch: 1, loss: 4.1256, accuracy: 11.84%</span><br><span class="line">epoch: 2, loss: 3.3503, accuracy: 29.57%</span><br><span class="line">epoch: 3, loss: 2.7933, accuracy: 39.18%</span><br><span class="line">epoch: 4, loss: 2.3512, accuracy: 46.23%</span><br><span class="line">epoch: 5, loss: 1.9799, accuracy: 53.01%</span><br><span class="line">epoch: 6, loss: 1.6415, accuracy: 60.11%</span><br><span class="line">epoch: 7, loss: 1.3198, accuracy: 66.47%</span><br><span class="line">epoch: 8, loss: 1.0045, accuracy: 73.82%</span><br><span class="line">epoch: 9, loss: 0.7526, accuracy: 80.01%</span><br><span class="line">epoch: 10, loss: 0.5546, accuracy: 85.41%</span><br><span class="line">epoch: 11, loss: 0.4127, accuracy: 88.86%</span><br><span class="line">epoch: 12, loss: 0.3165, accuracy: 91.59%</span><br><span class="line">epoch: 13, loss: 0.2522, accuracy: 92.89%</span><br><span class="line">epoch: 14, loss: 0.2186, accuracy: 94.30%</span><br><span class="line">epoch: 15, loss: 0.1374, accuracy: 96.43%</span><br><span class="line">epoch: 16, loss: 0.1213, accuracy: 97.03%</span><br><span class="line">epoch: 17, loss: 0.1369, accuracy: 96.51%</span><br><span class="line">epoch: 18, loss: 0.1459, accuracy: 96.16%</span><br><span class="line">epoch: 19, loss: 0.1010, accuracy: 97.10%</span><br><span class="line">epoch: 20, loss: 0.1033, accuracy: 97.48%</span><br></pre></td></tr></table></figure><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241129191242474.png" alt="image-20241129191242474"></p><h2 id="数据增广">数据增广</h2><p>数据增广的策略如下</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">        transforms.RandomHorizontalFlip(),  <span class="comment"># 随机水平翻转</span></span><br><span class="line">        transforms.RandomRotation(<span class="number">10</span>),  <span class="comment"># 随机旋转 +/- 10 度</span></span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>, scale=(<span class="number">0.8</span>, <span class="number">1.0</span>)),  <span class="comment"># 随机裁剪并调整大小</span></span><br><span class="line">        transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0.2</span>),  <span class="comment"># 颜色抖动</span></span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ])</span><br><span class="line">test_transform = transforms.Compose([</span><br><span class="line">transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>对于训练集：</p><ul><li>将所有图像调整到统一的大小，224x224像素，确保图像能够适应神经网络的输入尺寸。</li><li>随机水平翻转图像，这有助于模型学习到不依赖于物体方向的特征。</li><li>随机旋转图像 +/- 10度，帮助模型学习到旋转不变的特征。</li><li>随机裁剪图像到224x224像素，这有助于模型学习到不同尺度和位置的特征。</li><li>随机调整图像的亮度、对比度、饱和度和色调。这些调整模拟了不同光照和色彩条件下的图像，增强了模型对这些变化的鲁棒性。</li><li>将<code>ndarray</code> 转换为<code>torch.Tensor</code></li><li>对图像的每个通道进行标准化，使用的参数是ImageNet数据集上预训练模型的统计值，用于匹配预训练模型的期望输入分布。</li></ul><p>对于测试集：</p><ul><li>将图像调整到224x224像素</li><li>统一转换为<code>tensor</code>张量</li><li>对图像的每个通道进行标准化</li></ul><h2 id="辅助代码">辅助代码</h2><p><code>util.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU训练模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;training on&quot;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">2e-4</span>)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化记录列表</span></span><br><span class="line">    train_losses = []</span><br><span class="line">    train_accuracies = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练损失之和， 训练准确率之和， 样本数</span></span><br><span class="line">        net.train()</span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        epoch_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 梯度清空</span></span><br><span class="line">            X, y = X.to(device), y.to(device)  <span class="comment"># GPU加速</span></span><br><span class="line">            y_hat = net(X)  <span class="comment"># 前向计算</span></span><br><span class="line">            l = loss(y_hat, y)  <span class="comment"># 计算损失</span></span><br><span class="line">            l.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 参数更新</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 记录损失</span></span><br><span class="line">            epoch_loss += l.item()  <span class="comment"># 累加损失</span></span><br><span class="line">            <span class="comment"># 计算准确率</span></span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(y_hat, <span class="number">1</span>)</span><br><span class="line">            total += y.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == y).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个epoch的平均损失和准确率</span></span><br><span class="line">        avg_loss = epoch_loss / <span class="built_in">len</span>(train_iter)</span><br><span class="line">        accuracy = correct / total * <span class="number">100</span>  <span class="comment"># 百分比</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录到列表中</span></span><br><span class="line">        train_losses.append(avg_loss)</span><br><span class="line">        train_accuracies.append(accuracy)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, loss: <span class="subst">&#123;avg_loss:<span class="number">.4</span>f&#125;</span>, accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制训练损失和准确率曲线</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制损失曲线</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_losses, label=<span class="string">&#x27;Train Loss&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制准确率曲线</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_accuracies, label=<span class="string">&#x27;Train Accuracy&#x27;</span>, color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Accuracy (%)&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="主程序代码">主程序代码</h2><p><code>main.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> vgg <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, random_split</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> train</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line">    net = vgg(conv_arch)  <span class="comment"># vgg网络模型</span></span><br><span class="line"></span><br><span class="line">    dataset_path = <span class="string">&#x27;caltech-101\\caltech-101\\101_ObjectCategories\\101_ObjectCategories&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># transform进行数据增广</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据增广strategy1</span></span><br><span class="line">    <span class="comment"># train_transform = transforms.Compose([</span></span><br><span class="line">    <span class="comment">#     transforms.Resize((224, 224)),</span></span><br><span class="line">    <span class="comment">#     transforms.RandomHorizontalFlip(),</span></span><br><span class="line">    <span class="comment">#     transforms.ToTensor(),</span></span><br><span class="line">    <span class="comment">#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</span></span><br><span class="line">    <span class="comment"># ])</span></span><br><span class="line">    <span class="comment"># 最终准确率: 85.60%, loss = 0.5337</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据增广strategy2</span></span><br><span class="line">    train_transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">        transforms.RandomHorizontalFlip(),  <span class="comment"># 随机水平翻转</span></span><br><span class="line">        transforms.RandomRotation(<span class="number">10</span>),  <span class="comment"># 随机旋转 +/- 10 度</span></span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>, scale=(<span class="number">0.8</span>, <span class="number">1.0</span>)),  <span class="comment"># 随机裁剪并调整大小</span></span><br><span class="line">        transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0.2</span>),  <span class="comment"># 颜色抖动</span></span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ])</span><br><span class="line">    test_transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ])</span><br><span class="line">    <span class="comment"># 最终准确率: 84.76%, loss = 0.5694</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据集</span></span><br><span class="line">    dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据集</span></span><br><span class="line">    train_size = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(dataset))</span><br><span class="line">    test_size = <span class="built_in">len</span>(dataset) - train_size</span><br><span class="line">    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用transform</span></span><br><span class="line">    test_dataset.dataset.transform = test_transform</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得数据加载器</span></span><br><span class="line">    batch_size, lr, epochs = <span class="number">32</span>, <span class="number">0.01</span>, <span class="number">20</span></span><br><span class="line">    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义优化器、损失函数、策略</span></span><br><span class="line">    <span class="comment"># optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=2e-4)</span></span><br><span class="line">    <span class="comment"># criterion = nn.CrossEntropyLoss()  # 交叉熵损失</span></span><br><span class="line">    <span class="comment"># scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)</span></span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    train(net, train_loader, test_loader, epochs, lr, device)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 软件学院 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 课程作业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVM作业报告</title>
      <link href="/ymhui.github.io/2024/11/29/SVM%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/"/>
      <url>/ymhui.github.io/2024/11/29/SVM%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<h1>SVM作业报告</h1><h2 id="数据准备">数据准备</h2><p>这一步为加载Iris数据集，同时对数据进行标准化，划分训练集和测试集。使用train_test_split和StandardScaler标准化api</p><h2 id="模型训练">模型训练</h2><p>这一步为了使用数据训练SVM模型。分别声明使用线性核和RBF核的SVM，并使用上一步得到的训练集训练模型；训练完成后使用测试集数据进行预测，与标准结果进行比较计算准确率。</p><h2 id="决策边界绘制">决策边界绘制</h2><p>这一步为可视化SVM的训练效果，直观地看到好坏。使用plot_decision_boundary并传入训练好的模型与原始数据进行绘制，这一步修改了该函数的内部实现，增加了对网格化后的数据的标准化。</p><h2 id="超参数调优">超参数调优</h2><p>这一步为了寻找相对最优的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 。使用网格搜索GridSearchCV对最优超参数进行挑选，并使用三折交叉验证。</p><p>两个超参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> :</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>: 正则化参数</p><ul><li>用于调节错误分类的样本，调节正则化惩罚，控制模型对训练数据的拟合程度</li><li>可以平衡<strong>决策边界</strong>的平滑性和训练误差</li><li>大<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>值强调分类准确，而小<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>值可以使决策边界更平滑</li></ul><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span>：RBF核参数</p><ul><li><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 控制 RBF 核函数的影响范围，决定一个训练样本能“作用”到多远的特征空间</p></li><li><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 决定了样本点的高斯分布宽度</p></li><li><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 越大，每个支持向量的影响范围越小</p></li></ul><h2 id="模型评估">模型评估</h2><p>这一步为评估最佳模型的效果。对上一步得到的最佳模型进行训练，并对测试集数据进行预测；根据预测结果进行评分、计算准确率等，并绘制决策边界和混淆矩阵。</p><h2 id="效果图">效果图</h2><h3 id="GridSearchCV-搜索过程">GridSearchCV 搜索过程</h3><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241108130853230.png" alt="image-20241108130853230"></p><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241108130918162.png" alt="image-20241108130918162"></p><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241108131141094.png" alt="image-20241108131141094"></p><h3 id="模型性能">模型性能</h3><p>普通线性核和RBF核</p><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241108130937850.png" alt="image-20241108130937850"></p><p>最佳RBF核</p><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241108131127165.png" alt="image-20241108131127165"></p><h3 id="决策边界图">决策边界图</h3><p>线性核</p><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241108131229637.png" alt="image-20241108131229637"></p><p>RBF核</p><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241108131239414.png" alt="image-20241108131239414"></p><p>最佳RBF核</p><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241108131244254.png" alt="image-20241108131244254"></p><h3 id="混淆矩阵">混淆矩阵</h3><p>最佳RBF核</p><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241108131250422.png" alt="image-20241108131250422"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 软件学院 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 课程作业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TSDetect</title>
      <link href="/ymhui.github.io/2024/10/21/TSDetect/"/>
      <url>/ymhui.github.io/2024/10/21/TSDetect/</url>
      
        <content type="html"><![CDATA[<h1>TSDetect</h1><h2 id="Introduction">Introduction</h2><p>A useful tool for unit test smell detection.</p><h2 id="Git-Repository">Git Repository</h2><p><a href="https://github.com/NJU-ymhui/TSDetect">https://github.com/NJU-ymhui/TSDetect</a></p><h2 id="Have-a-try">Have a try</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/NJU-ymhui/TSDetect.git</span><br></pre></td></tr></table></figure><h2 id="References">References</h2><ol><li><a href="https://doi.org/10.1145/3368089.3417921">tsDetect: an open source test smells detection tool</a></li><li><a href="https://github.com/TestSmells/TSDetect">https://github.com/TestSmells/TSDetect</a></li></ol><h2 id="Contact">Contact</h2><p>The author of this blog, email: <a href="mailto:221870120@smail.nju.edu.cn">221870120@smail.nju.edu.cn</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 软件测试 </tag>
            
            <tag> test smell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自然语言处理</title>
      <link href="/ymhui.github.io/2024/10/01/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
      <url>/ymhui.github.io/2024/10/01/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1>写在前面</h1><blockquote><p>参考书籍</p></blockquote><p><a href="http://www.d2l.ai">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><h1>自然语言处理</h1><p><strong>source code</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui/DeepLearning: Deep Learning with pytorch (github.com)</a></p><p><strong>use git to clone</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p><p><code>/NLP</code></p><blockquote><p>word2vec_dataset.pyword2vec_pretraining.py<a href="http://fasttext.py">fasttext.py</a>similarity_compare.py<a href="http://BERT.py">BERT.py</a>BERT_pretraining_dataset.py</p></blockquote><h2 id="词嵌入">词嵌入</h2><p>在自然语言系统中，<strong>词</strong>是意义的基本单位。下面介绍一个新的概念：<strong>词向量</strong>，这是用于表示单词意义的向量。那么现在还需要<strong>将每个词映射到对应的词向量</strong>，这项技术就是<strong>词嵌入</strong>。</p><p>下面将介绍一些将词映射到向量的相关技术。</p><h3 id="为什么不再使用独热向量">为什么不再使用独热向量</h3><p>在之前做机器翻译时曾尝试用过独热编码的向量，但现在来看，这并不是一个好的选择。一个重要原因是独热向量不能表达不同词之间的相似度，比如经常使用的余弦相似度，它对两个向量<strong>x, y</strong>使用余弦表示两个向量之间的相似度</p><p><img src="%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/hot.png" alt="image-20241001114419714"></p><p>然而根据独热向量的定义可以得知，任意两个不同词的独热向量的余弦为0，即独热向量不能编码词之间的相似性。</p><h3 id="自监督的word2vec">自监督的word2vec</h3><ul><li>跳元模型</li><li>连续词袋模型</li></ul><h4 id="跳元模型Skip-Gram">跳元模型Skip-Gram</h4><p>详见<a href="https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#the-skip-gram-model">15.1. Skip-Gram — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h4 id="连续词袋CBOW">连续词袋CBOW</h4><p>详见<a href="https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#the-continuous-bag-of-words-cbow-model">15.1. CBOW — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="近似训练">近似训练</h2><h3 id="负采样">负采样</h3><p>详见<a href="https://d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html#negative-sampling">15.2. Negative-Sampling — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h3 id="层序Softmax">层序Softmax</h3><p>层序softmax是另一种近似训练的方法，使用二叉树，其中树的每个叶节点表示词表<em>V</em>中的一个词。</p><p><img src="%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/softmax.png" alt="image-20241001122958558"></p><p>其原理，详见<a href="https://d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html#hierarchical-softmax">15.2. Softmax — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="预训练词嵌入的数据集">预训练词嵌入的数据集</h2><p>直接上代码。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_ptb</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将ptb数据集加载到文本行的列表中&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;ptb&#x27;</span>)</span><br><span class="line">    <span class="comment"># 读取训练集</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;ptb.train.txt&#x27;</span>)) <span class="keyword">as</span> f:</span><br><span class="line">        raw_text = f.read()</span><br><span class="line">    <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> raw_text.split(<span class="string">&#x27;\n&#x27;</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">subsample</span>(<span class="params">sentences, vocab</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下采样高频词&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 排除未知词元&lt;unk&gt;</span></span><br><span class="line">    sentences = [[token <span class="keyword">for</span> token <span class="keyword">in</span> line <span class="keyword">if</span> vocab[token] != vocab.unk] <span class="keyword">for</span> line <span class="keyword">in</span> sentences]</span><br><span class="line">    counter = d2l.count_corpus(sentences)</span><br><span class="line">    num_tokens = <span class="built_in">sum</span>(counter.values())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果在下采样期间保留词元，返回True</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">keep</span>(<span class="params">token</span>):</span><br><span class="line">        <span class="keyword">return</span> random.uniform(<span class="number">0</span>, <span class="number">1</span>) &lt; math.sqrt(<span class="number">1e-4</span> / counter[token] * num_tokens)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [[token <span class="keyword">for</span> token <span class="keyword">in</span> line <span class="keyword">if</span> keep(token)] <span class="keyword">for</span> line <span class="keyword">in</span> sentences], counter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compare_counts</span>(<span class="params">token, sentences, subsampled</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;比较下采样前后某个token的频次&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="string">f&#x27;&quot;<span class="subst">&#123;token&#125;</span>&quot; count before subsample: <span class="subst">&#123;<span class="built_in">sum</span>(l.count(token) <span class="keyword">for</span> l <span class="keyword">in</span> sentences)&#125;</span>\n&#x27;</span></span><br><span class="line">            <span class="string">f&#x27;after subsample: <span class="subst">&#123;<span class="built_in">sum</span>(l.count(token) <span class="keyword">for</span> l <span class="keyword">in</span> subsampled)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 中心词和上下文词的提取</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_centers_and_contexts</span>(<span class="params">corpus, max_window_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回跳元模型中的中心词和上下文词&quot;&quot;&quot;</span></span><br><span class="line">    centers, contexts = [], []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> corpus:</span><br><span class="line">        <span class="comment"># 要形成”中心词-上下文词“对，每个句子至少需要有两个词</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(line) &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        centers += line</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(line)):</span><br><span class="line">            window_size = random.randint(<span class="number">1</span>, max_window_size)</span><br><span class="line">            indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">max</span>(<span class="number">0</span>, i - window_size), <span class="built_in">min</span>(<span class="built_in">len</span>(line), i + <span class="number">1</span> + window_size)))</span><br><span class="line">            <span class="comment"># 从上下文词中排除中心词</span></span><br><span class="line">            indices.remove(i)</span><br><span class="line">            contexts.append([line[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> indices])</span><br><span class="line">    <span class="keyword">return</span> centers, contexts</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了根据预定义的分布对噪声词进行采样，我们定义如下RandomGenerator类</span></span><br><span class="line"><span class="comment"># 其中采用分布通过变量sampling_weights传递</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomGenerator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据n个采样权重在&#123;1, ..., n&#125;中随机抽取&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sampling_weights</span>):</span><br><span class="line">        <span class="comment"># Exclude</span></span><br><span class="line">        <span class="variable language_">self</span>.population = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(sampling_weights) + <span class="number">1</span>))</span><br><span class="line">        <span class="variable language_">self</span>.sampling_weights = sampling_weights</span><br><span class="line">        <span class="variable language_">self</span>.candidates = []</span><br><span class="line">        <span class="variable language_">self</span>.i = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">draw</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.i == <span class="built_in">len</span>(<span class="variable language_">self</span>.candidates):</span><br><span class="line">            <span class="comment"># 缓存k个随机采样结果</span></span><br><span class="line">            <span class="variable language_">self</span>.candidates = random.choices(<span class="variable language_">self</span>.population, <span class="variable language_">self</span>.sampling_weights, k=<span class="number">10000</span>)</span><br><span class="line">            <span class="variable language_">self</span>.i = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.candidates[<span class="variable language_">self</span>.i - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于一对中心词和上下文词，我们随机抽取了K个（实验中为5个）噪声词。</span></span><br><span class="line"><span class="comment"># 根据word2vec论文中的建议，将噪声词w的采样概率P(w)设置为其在字典中的相对频率，其幂为0.75</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_negatives</span>(<span class="params">all_contexts, vocab, counter, K</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回负采样中的噪声词&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 索引为1、2、... (索引0是词表中排除的未知标记)</span></span><br><span class="line">    sampling_weights = [counter[vocab.to_tokens(i)] ** <span class="number">0.75</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(vocab))]</span><br><span class="line">    all_negatives, generator = [], RandomGenerator(sampling_weights)</span><br><span class="line">    <span class="keyword">for</span> contexts <span class="keyword">in</span> all_contexts:</span><br><span class="line">        negatives = []</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(negatives) &lt; <span class="built_in">len</span>(contexts) * K:</span><br><span class="line">            neg = generator.draw()</span><br><span class="line">            <span class="comment"># 噪声词不能是上下文词</span></span><br><span class="line">            <span class="keyword">if</span> neg <span class="keyword">not</span> <span class="keyword">in</span> contexts:  <span class="comment"># 通过这个条件保证</span></span><br><span class="line">                negatives.append(neg)</span><br><span class="line">        all_negatives.append(negatives)</span><br><span class="line">    <span class="keyword">return</span> all_negatives</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batchify</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回带有负采样的跳元模型的小批量样本&quot;&quot;&quot;</span></span><br><span class="line">    max_len = <span class="built_in">max</span>(<span class="built_in">len</span>(c) + <span class="built_in">len</span>(n) <span class="keyword">for</span> _, c, n <span class="keyword">in</span> data)</span><br><span class="line">    centres, contexts_negatives, masks, labels = [], [], [], []</span><br><span class="line">    <span class="keyword">for</span> center, context, negative <span class="keyword">in</span> data:</span><br><span class="line">        cur_len = <span class="built_in">len</span>(context) + <span class="built_in">len</span>(negative)</span><br><span class="line">        centres += [center]</span><br><span class="line">        contexts_negatives += [context + negative + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        masks += [[<span class="number">1</span>] * cur_len + [<span class="number">0</span>] * (max_len - cur_len)]</span><br><span class="line">        labels += [[<span class="number">1</span>] * <span class="built_in">len</span>(context) + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(context))]</span><br><span class="line">    <span class="keyword">return</span> (torch.tensor(centres).reshape((-<span class="number">1</span>, <span class="number">1</span>)), torch.tensor(contexts_negatives),</span><br><span class="line">            torch.tensor(masks), torch.tensor(labels))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 整合一下代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_ptb</span>(<span class="params">batch_size, max_window_size, num_noise_words</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载PTB数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    sentences = read_ptb()</span><br><span class="line">    vocab = d2l.Vocab(sentences, min_freq=<span class="number">10</span>)</span><br><span class="line">    subsampled, counter = subsample(sentences, vocab)</span><br><span class="line">    corpus = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> subsampled]</span><br><span class="line">    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)</span><br><span class="line">    all_negatives = get_negatives(all_contexts, vocab, counter, num_noise_words)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">PTBDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, centers, contexts, negatives</span>):</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(centers) == <span class="built_in">len</span>(contexts) == <span class="built_in">len</span>(negatives)</span><br><span class="line">            <span class="variable language_">self</span>.centers = centers</span><br><span class="line">            <span class="variable language_">self</span>.contexts = contexts</span><br><span class="line">            <span class="variable language_">self</span>.negatives = negatives</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.centers[index], <span class="variable language_">self</span>.contexts[index], <span class="variable language_">self</span>.negatives[index]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.centers)</span><br><span class="line"></span><br><span class="line">    dataset = PTBDataset(all_centers, all_contexts, all_negatives)</span><br><span class="line">    data_iter = torch.utils.data.DataLoader(dataset, batch_size,</span><br><span class="line">                                            shuffle=<span class="literal">True</span>, collate_fn=batchify, num_workers=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> data_iter, vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 读取数据集</span></span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;ptb&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;ptb.zip&#x27;</span>, <span class="string">&#x27;319d85e578af0cdc590547f26231e4e31cdf1e42&#x27;</span>)</span><br><span class="line">    sentences = read_ptb()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;length of sentence: <span class="subst">&#123;<span class="built_in">len</span>(sentences)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建词表</span></span><br><span class="line">    vocab = d2l.Vocab(sentences, min_freq=<span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;vocab size: <span class="subst">&#123;<span class="built_in">len</span>(vocab)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 下采样</span></span><br><span class="line">    subsampled, counter = subsample(sentences, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可视化下采样前后每句话词元数量的直方图</span></span><br><span class="line">    d2l.show_list_len_pair_hist([<span class="string">&#x27;origin&#x27;</span>, <span class="string">&#x27;subsampled&#x27;</span>], <span class="string">&#x27;# tokens per sentence&#x27;</span>, <span class="string">&#x27;count&#x27;</span>, sentences, subsampled)</span><br><span class="line">    d2l.plt.show()</span><br><span class="line">    <span class="comment"># 查看一下词元高频词 the 在下采样前后的频次</span></span><br><span class="line">    <span class="built_in">print</span>(compare_counts(<span class="string">&#x27;the&#x27;</span>, sentences, subsampled))  <span class="comment"># 可看出高频词the很多都被去除了</span></span><br><span class="line">    <span class="comment"># 查看一下词元低频词 join 在下采样前后的频次</span></span><br><span class="line">    <span class="built_in">print</span>(compare_counts(<span class="string">&#x27;join&#x27;</span>, subsampled, subsampled))  <span class="comment"># 可看出低频词join被完全保留了</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 下采样之后，将词元映射到它们在语料库中的索引</span></span><br><span class="line">    corpus = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> subsampled]</span><br><span class="line">    <span class="built_in">print</span>(corpus[:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 验证一下提取中心词和上下文词的函数</span></span><br><span class="line">    tiny_set = [<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">7</span>)), <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">7</span>, <span class="number">10</span>))]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;raw data: &quot;</span>, tiny_set)</span><br><span class="line">    <span class="keyword">for</span> center, context <span class="keyword">in</span> <span class="built_in">zip</span>(*get_centers_and_contexts(tiny_set, <span class="number">2</span>)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;center word &#x27;</span>, center, <span class="string">&#x27;, its&#x27;</span>, <span class="string">&#x27;context word:&#x27;</span>, context)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在PTB数据集上进行训练时，将最大上下文窗口大小设为5</span></span><br><span class="line">    all_centers, all_contexts = get_centers_and_contexts(corpus, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用负采样进行近似训练</span></span><br><span class="line">    generator = RandomGenerator([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">    <span class="built_in">print</span>([generator.draw() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 负采样</span></span><br><span class="line">    all_negatives = get_negatives(all_contexts, vocab, counter, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 小批量加载训练实例</span></span><br><span class="line">    <span class="comment"># 先测试一下batchify函数</span></span><br><span class="line">    x_1 = (<span class="number">1</span>, [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">    x_2 = (<span class="number">1</span>, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">    batch = batchify((x_1, x_2))</span><br><span class="line">    names = [<span class="string">&#x27;centers&#x27;</span>, <span class="string">&#x27;contexts_negatives&#x27;</span>, <span class="string">&#x27;masks&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> name, data <span class="keyword">in</span> <span class="built_in">zip</span>(names, batch):</span><br><span class="line">        <span class="built_in">print</span>(name, <span class="string">&#x27;=&#x27;</span>, data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检验load_data_ptb返回的数据迭代器</span></span><br><span class="line">    data_iter, vocab = load_data_ptb(<span class="number">512</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">for</span> name, data <span class="keyword">in</span> <span class="built_in">zip</span>(names, batch):</span><br><span class="line">            <span class="built_in">print</span>(name, <span class="string">&#x27;shape:&#x27;</span>, data.shape)</span><br><span class="line">        <span class="keyword">break</span>  <span class="comment"># 看一下第一个就够了</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">length of sentence: 42069</span><br><span class="line">vocab size: 6719</span><br></pre></td></tr></table></figure><p><img src="%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/word_freq_before_after.png" alt="image-20241001180806375"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;the&quot; count before subsample: 50770</span><br><span class="line">after subsample: 2043</span><br><span class="line">&quot;join&quot; count before subsample: 45</span><br><span class="line">after subsample: 45</span><br><span class="line">[[], [2115, 406], [22, 5277, 3054, 1580]]</span><br><span class="line">raw data:  [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]</span><br><span class="line">center word  0 , its context word: [1, 2]</span><br><span class="line">center word  1 , its context word: [0, 2]</span><br><span class="line">center word  2 , its context word: [0, 1, 3, 4]</span><br><span class="line">center word  3 , its context word: [2, 4]</span><br><span class="line">center word  4 , its context word: [3, 5]</span><br><span class="line">center word  5 , its context word: [4, 6]</span><br><span class="line">center word  6 , its context word: [4, 5]</span><br><span class="line">center word  7 , its context word: [8, 9]</span><br><span class="line">center word  8 , its context word: [7, 9]</span><br><span class="line">center word  9 , its context word: [7, 8]</span><br><span class="line">[2, 2, 2, 3, 2, 1, 2, 1, 3, 2]</span><br><span class="line">centers = tensor([[1],</span><br><span class="line">        [1]])</span><br><span class="line">contexts_negatives = tensor([[2, 2, 3, 3, 3, 3],</span><br><span class="line">        [2, 2, 2, 3, 3, 0]])</span><br><span class="line">masks = tensor([[1, 1, 1, 1, 1, 1],</span><br><span class="line">        [1, 1, 1, 1, 1, 0]])</span><br><span class="line">labels = tensor([[1, 1, 0, 0, 0, 0],</span><br><span class="line">        [1, 1, 1, 0, 0, 0]])</span><br><span class="line">centers shape: torch.Size([512, 1])</span><br><span class="line">contexts_negatives shape: torch.Size([512, 60])</span><br><span class="line">masks shape: torch.Size([512, 60])</span><br><span class="line">labels shape: torch.Size([512, 60])</span><br></pre></td></tr></table></figure><h2 id="预训练word2vec">预训练word2vec</h2><p>继续实现跳元语法模型，然后在PTB数据集上使用负采样预训练word2vec。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> word2vec_dataset <span class="keyword">import</span> load_data_ptb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">skip_gram</span>(<span class="params">center, contexts_and_negatives, embed_v, embed_u</span>):</span><br><span class="line">    v = embed_v(center)</span><br><span class="line">    u = embed_u(contexts_and_negatives)</span><br><span class="line">    pred = torch.bmm(v, u.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练负采样的跳元模型之前，先定义损失函数</span></span><br><span class="line"><span class="comment"># 二元交叉熵损失</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SigmoidBCELoss</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 带掩码的二元交叉熵损失</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, target, mask=<span class="literal">None</span></span>):</span><br><span class="line">        out = nn.functional.binary_cross_entropy_with_logits(inputs, target, weight=mask, reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> out.mean(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在二元交叉熵损失中使用sigmoid激活函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> -math.log(<span class="number">1</span> / (<span class="number">1</span> + math.exp(-x)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练阶段代码, 注意有填充的存在</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, data_iter, lr, num_epochs, device=d2l.try_gpu(<span class="params"></span>)</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Embedding:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 规范化的损失之和，规范化的损失数</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(data_iter)</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_iter):</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            center, context_negative, mask, label = [data.to(device) <span class="keyword">for</span> data <span class="keyword">in</span> batch]</span><br><span class="line">            pred = skip_gram(center, context_negative, net[<span class="number">0</span>], net[<span class="number">1</span>])</span><br><span class="line">            l = (loss(pred.reshape(label.shape).<span class="built_in">float</span>(), label.<span class="built_in">float</span>(), mask) / mask.<span class="built_in">sum</span>(axis=<span class="number">1</span>) * mask.shape[<span class="number">1</span>])</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>],))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss: <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop():<span class="number">.1</span>f&#125;</span> tokens / sec on <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用词嵌入</span></span><br><span class="line"><span class="comment"># 在训练好word2vec模型之后，我们可以使用训练好的模型中词向量的余弦相似度来从词表中找到与输入单词语义最相似的单词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_similar_tokens</span>(<span class="params">query_token, k, embed</span>):</span><br><span class="line">    W = embed.weight.data</span><br><span class="line">    x = W[vocab[query_token]]</span><br><span class="line">    <span class="comment"># 计算余弦相似性，增加1e-9以获得数值稳定性</span></span><br><span class="line">    cos = torch.mv(W, x) / torch.sqrt(torch.<span class="built_in">sum</span>(W * W, dim=<span class="number">1</span>) * torch.<span class="built_in">sum</span>(x * x + <span class="number">1e-9</span>))</span><br><span class="line">    topk = torch.topk(cos, k=k + <span class="number">1</span>)[<span class="number">1</span>].cpu().numpy().astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> topk[<span class="number">1</span>:]:</span><br><span class="line">        <span class="comment"># 删除输入词</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;cosine sim = <span class="subst">&#123;<span class="built_in">float</span>(cos[i]):<span class="number">.3</span>f&#125;</span>: <span class="subst">&#123;vocab.to_tokens(i)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, max_window_size, num_noise_words = <span class="number">512</span>, <span class="number">5</span>, <span class="number">5</span></span><br><span class="line">    <span class="comment"># 获得数据迭代器和词表</span></span><br><span class="line">    data_iter, vocab = load_data_ptb(batch_size, max_window_size, num_noise_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造跳元模型</span></span><br><span class="line">    <span class="comment"># 嵌入层</span></span><br><span class="line">    embed = nn.Embedding(num_embeddings=<span class="number">20</span>, embedding_dim=<span class="number">4</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Parameter embedding_weight: <span class="subst">&#123;embed.weight.shape&#125;</span>, dtype = <span class="subst">&#123;embed.weight.dtype&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 嵌入层的输入是词的索引，对于任何词元索引i，其向量表示可以从嵌入层中的权重矩阵的第i行获得</span></span><br><span class="line">    <span class="comment"># 由于embed的向量维度设为4，因此当小批量词元索引的形状为（2，3）时，嵌入层返回具有形状（2，3，4）的向量</span></span><br><span class="line">    x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;embed layer:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(embed(x))</span><br><span class="line">    <span class="comment"># 查看skip_gram函数的输出现状</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of skip_gram output:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(skip_gram(torch.ones((<span class="number">2</span>, <span class="number">1</span>), dtype=torch.long), torch.ones((<span class="number">2</span>, <span class="number">4</span>), dtype=torch.long)</span><br><span class="line">                    , embed, embed).shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    loss = SigmoidBCELoss()</span><br><span class="line">    pred = torch.tensor([[<span class="number">1.1</span>, -<span class="number">2.2</span>, <span class="number">3.3</span>, -<span class="number">4.4</span>]] * <span class="number">2</span>)</span><br><span class="line">    label = torch.tensor([[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]])</span><br><span class="line">    mask = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;loss:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(loss(pred, label, mask) * mask.shape[<span class="number">1</span>] / mask.<span class="built_in">sum</span>(axis=<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 初始化模型参数</span></span><br><span class="line">    embed_size, vocab_size = <span class="number">100</span>, <span class="built_in">len</span>(vocab)</span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size),</span><br><span class="line">        nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 使用负采样来训练跳元模型</span></span><br><span class="line">    lr, num_epochs = <span class="number">0.002</span>, <span class="number">5</span></span><br><span class="line">    train(net, data_iter, lr, num_epochs, d2l.try_gpu())</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用词嵌入</span></span><br><span class="line">    get_similar_tokens(<span class="string">&#x27;chip&#x27;</span>, <span class="number">3</span>, net[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Parameter embedding_weight: torch.Size([20, 4]), dtype = torch.float32</span><br><span class="line">embed layer:</span><br><span class="line">tensor([[[ 0.9359,  1.1477, -0.9412,  1.2183],</span><br><span class="line">         [-0.5608,  0.2200, -0.1888,  0.5098],</span><br><span class="line">         [ 0.1096,  0.2246, -1.7004, -0.9497]],</span><br><span class="line"></span><br><span class="line">        [[-0.1224,  0.2635,  0.1246, -0.4876],</span><br><span class="line">         [ 0.6482,  0.5379,  2.3570,  1.1432],</span><br><span class="line">         [ 0.0884, -0.8212, -1.0040, -0.7317]]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br><span class="line">shape of skip_gram output:</span><br><span class="line">torch.Size([2, 1, 4])</span><br><span class="line">loss:</span><br><span class="line">tensor([0.9352, 1.8462])</span><br><span class="line">loss: 0.410, 123025.4 tokens / sec on cpu</span><br></pre></td></tr></table></figure><p><img src="%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/word2vec_pretraining.png" alt="image-20241003155927780"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cosine sim = 0.722: microprocessor</span><br><span class="line">cosine sim = 0.703: laptop</span><br><span class="line">cosine sim = 0.691: intel</span><br></pre></td></tr></table></figure><h2 id="全局向量的词嵌入GloVe">全局向量的词嵌入GloVe</h2><p><strong>上下文窗口内的词共现可以携带丰富的语义信息</strong>。比如“固体”更可能与“冰”一同出现而不是“水”，反观“蒸汽”则更可能和“水”一起出现。此外，可以<strong>预先计算此类共现的全局语料库统计数据：这可以提高训练效率</strong>。为了利用整个语料库中的统计信息进行词嵌入，使用<strong>全局语料库统计</strong>。</p><p>具体原理见<a href="https://d2l.ai/chapter_natural-language-processing-pretraining/glove.html#skip-gram-with-global-corpus-statistics">15.5. Word Embedding with Global Vectors (GloVe) — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><p>模型见<a href="https://d2l.ai/chapter_natural-language-processing-pretraining/glove.html#the-glove-model">15.5. GloVe Model— Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="子词嵌入">子词嵌入</h2><p>有些单词可以被视作其他单词的变种，比如dog和dogs，help和helps、helping，又比如boy与boyfriend的关系和girl与girlfriend的关系一样。这种多个词之间潜在的联系有时会传达相当有用的信息，并在预测分析时提供关键的上下文；遗憾的是，<strong>word2vec和GloVe都没有对词的内部结构进行讨论</strong>。</p><h3 id="fastText模型">fastText模型</h3><p>原理见<a href="https://d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html#the-fasttext-model">15.6. fastText — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h4 id="字节对编码">字节对编码</h4><p>在fastText中，所有提取的子词都必须是指定的长度，例如3到6，因此词表大小不能预定义。为了在固定大小的词表中允许可变长度的子词，我们可以应用一种称为<strong>字节对编码</strong>（Byte Pair Encoding，BPE）的压缩算法来提取子词。</p><p><strong>字节对编码执行训练数据集的统计分析，以发现单词内的公共符号</strong>，诸如任意长度的连续字符。从长度为1的符号开始，字节对编码迭代地<strong>合并最频繁的连续符号对</strong>以<strong>产生新的更长的符号</strong>。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回词内最频繁的连续符号对</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_max_freq_pair</span>(<span class="params">token_freq</span>):</span><br><span class="line">    pairs = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freq.items():</span><br><span class="line">        symbols = token.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(symbols) - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># &quot;pairs&quot;的键是两个连续符号的元组</span></span><br><span class="line">            pairs[symbols[i], symbols[i + <span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(pairs, key=pairs.get)  <span class="comment"># 具有最大值的&quot;pairs&quot;键</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并最频繁的连续符号对以产生新符号</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">merge_symbols</span>(<span class="params">max_freq_pair, token_freq, symbols</span>):</span><br><span class="line">    <span class="comment"># 将最频繁的符号对合并为一个新的符号，并添加到symbols列表中</span></span><br><span class="line">    symbols.append(<span class="string">&#x27;&#x27;</span>.join(max_freq_pair))</span><br><span class="line">    <span class="comment"># 初始化新的词频字典，用于存储更新后的词频</span></span><br><span class="line">    new_token_freq = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="comment"># 遍历原始的词频字典</span></span><br><span class="line">    <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freq.items():</span><br><span class="line">        <span class="comment"># 将token中的max_freq_pair替换为新的符号，并保持其他部分不变</span></span><br><span class="line">        new_token = token.replace(<span class="string">&#x27; &#x27;</span>.join(max_freq_pair), <span class="string">&#x27;&#x27;</span>.join(max_freq_pair))</span><br><span class="line">        <span class="comment"># 更新新的词频字典，记录合并后的词频</span></span><br><span class="line">        new_token_freq[new_token] = token_freq[token]</span><br><span class="line">    <span class="keyword">return</span> new_token_freq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 尝试将单词从输入参数symbols分成可能最长的子词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">segment_BPE</span>(<span class="params">tokens, symbols</span>):</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">        start, end = <span class="number">0</span>, <span class="built_in">len</span>(token)</span><br><span class="line">        cur_output = []</span><br><span class="line">        <span class="comment"># 具有符号中可能最长子字的词元段</span></span><br><span class="line">        <span class="keyword">while</span> start &lt; <span class="built_in">len</span>(token) <span class="keyword">and</span> start &lt; end:</span><br><span class="line">            <span class="keyword">if</span> token[start:end] <span class="keyword">in</span> symbols:</span><br><span class="line">                cur_output.append(token[start:end])</span><br><span class="line">                start = end</span><br><span class="line">                end = <span class="built_in">len</span>(token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                end -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> start &lt; <span class="built_in">len</span>(token):</span><br><span class="line">            cur_output.append(<span class="string">&#x27;[UNK]&#x27;</span>)</span><br><span class="line">        outputs.append(<span class="string">&#x27; &#x27;</span>.join(cur_output))</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 初始化符号词表，内容为所有英文小写字符、特殊的词尾符号&#x27;_&#x27;, 和 未知符号&#x27;&lt;unk&gt;&#x27;</span></span><br><span class="line">    symbols = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;j&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;n&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;q&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>]</span><br><span class="line">    <span class="comment"># 定义一个原始token频率字典，用于记录不同token出现的频率</span></span><br><span class="line">    raw_token_freq = &#123;<span class="string">&#x27;fast_&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;faster_&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;tall_&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;taller_&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">    <span class="comment"># 初始化一个空字典，用于存储处理后的token频率</span></span><br><span class="line">    token_freq = &#123;&#125;</span><br><span class="line">    <span class="comment"># 遍历原始token频率字典，对每个token进行处理</span></span><br><span class="line">    <span class="keyword">for</span> token, freq <span class="keyword">in</span> raw_token_freq.items():</span><br><span class="line">        <span class="comment"># 将token按字符拆分，并用空格连接，然后将其作为新的键存储在token_freq中</span></span><br><span class="line">        token_freq[<span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(token))] = raw_token_freq[token]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;token_freq:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(token_freq)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对词典token_freq的键迭代地执行字节对编码算法</span></span><br><span class="line">    num_merges = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">        max_freq_pair = get_max_freq_pair(token_freq)</span><br><span class="line">        token_freq = merge_symbols(max_freq_pair, token_freq, symbols)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;merge #<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>:&#x27;</span>, max_freq_pair)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;symbols:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(symbols)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用列表symbols中的子词（从前面提到的数据集学习）来表示另一个数据集的tokens</span></span><br><span class="line">    tokens = [<span class="string">&#x27;tallest_&#x27;</span>, <span class="string">&#x27;fatter_&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;use token in symbols to represent another dataset tokens:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(segment_BPE(tokens, symbols))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">token_freq:</span><br><span class="line">&#123;&#x27;f a s t _&#x27;: 4, &#x27;f a s t e r _&#x27;: 3, &#x27;t a l l _&#x27;: 5, &#x27;t a l l e r _&#x27;: 4&#125;</span><br><span class="line">merge #1: (&#x27;t&#x27;, &#x27;a&#x27;)</span><br><span class="line">merge #2: (&#x27;ta&#x27;, &#x27;l&#x27;)</span><br><span class="line">merge #3: (&#x27;tal&#x27;, &#x27;l&#x27;)</span><br><span class="line">merge #4: (&#x27;f&#x27;, &#x27;a&#x27;)</span><br><span class="line">merge #5: (&#x27;fa&#x27;, &#x27;s&#x27;)</span><br><span class="line">merge #6: (&#x27;fas&#x27;, &#x27;t&#x27;)</span><br><span class="line">merge #7: (&#x27;e&#x27;, &#x27;r&#x27;)</span><br><span class="line">merge #8: (&#x27;er&#x27;, &#x27;_&#x27;)</span><br><span class="line">merge #9: (&#x27;tall&#x27;, &#x27;_&#x27;)</span><br><span class="line">merge #10: (&#x27;fast&#x27;, &#x27;_&#x27;)</span><br><span class="line">symbols:</span><br><span class="line">[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;j&#x27;, &#x27;k&#x27;, &#x27;l&#x27;, &#x27;m&#x27;, &#x27;n&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;q&#x27;, &#x27;r&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;u&#x27;, &#x27;v&#x27;, &#x27;w&#x27;, &#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;_&#x27;, &#x27;[UNK]&#x27;, &#x27;ta&#x27;, &#x27;tal&#x27;, &#x27;tall&#x27;, &#x27;fa&#x27;, &#x27;fas&#x27;, &#x27;fast&#x27;, &#x27;er&#x27;, &#x27;er_&#x27;, &#x27;tall_&#x27;, &#x27;fast_&#x27;]</span><br><span class="line">use token in symbols to represent another dataset tokens:</span><br><span class="line">[&#x27;tall e s t _&#x27;, &#x27;fa t t er_&#x27;]</span><br></pre></td></tr></table></figure><h2 id="词的相似性和类比任务">词的相似性和类比任务</h2><p>在<strong>大型语料库上预先训练的词向量可以应用于下游的自然语言处理</strong>任务。下面将展示大型语料库中预训练词向量的语义，即<strong>将预训练词向量应用到词的相似性和类比任务</strong>中。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了加载预训练的GloVe和fastText嵌入</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TokenEmbedding</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Glove嵌入&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_name</span>):</span><br><span class="line">        <span class="variable language_">self</span>.idx_to_token, <span class="variable language_">self</span>.idx_to_vec = <span class="variable language_">self</span>._load_embedding(embedding_name)</span><br><span class="line">        <span class="variable language_">self</span>.unknown_idx = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.token_to_idx = &#123;</span><br><span class="line">            token: idx <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.idx_to_token)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_load_embedding</span>(<span class="params">self, embedding_name</span>):</span><br><span class="line">        idx_to_token, idx_to_vec = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>], []</span><br><span class="line">        data_dir = d2l.download_extract(embedding_name)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;vec.txt&#x27;</span>), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                elems = line.rstrip().split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                token, elems = elems[<span class="number">0</span>], [<span class="built_in">float</span>(elem) <span class="keyword">for</span> elem <span class="keyword">in</span> elems[<span class="number">1</span>:]]</span><br><span class="line">                <span class="comment"># 跳过标题信息，比如fasttext首行</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(elems) &gt; <span class="number">1</span>:</span><br><span class="line">                    idx_to_token.append(token)</span><br><span class="line">                    idx_to_vec.append(elems)</span><br><span class="line">        idx_to_vec = [[<span class="number">0</span>] * <span class="built_in">len</span>(idx_to_vec[<span class="number">0</span>])] + idx_to_vec</span><br><span class="line">        <span class="keyword">return</span> idx_to_token, torch.tensor(idx_to_vec)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        indices = [<span class="variable language_">self</span>.token_to_idx.get(token, <span class="variable language_">self</span>.unknown_idx) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">        vecs = <span class="variable language_">self</span>.idx_to_vec[torch.tensor(indices)]</span><br><span class="line">        <span class="keyword">return</span> vecs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.idx_to_token)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用knn为词分类，以便根据词向量之间的余弦相似度为输入词查找语义相似的词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">knn</span>(<span class="params">W, x, k</span>):</span><br><span class="line">    <span class="comment"># 增加1e-9以获得数值稳定性</span></span><br><span class="line">    cos = torch.mv(W, x.reshape(-<span class="number">1</span>, )) / (torch.sqrt(torch.<span class="built_in">sum</span>(W * W, axis=<span class="number">1</span>) + <span class="number">1e-9</span>) * torch.sqrt((x * x).<span class="built_in">sum</span>()))</span><br><span class="line">    _, topk = torch.topk(cos, k=k)</span><br><span class="line">    <span class="keyword">return</span> topk, [cos[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> topk]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用TokenEmbedding的实例embed中预训练好的词向量来搜索相近的词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_similar_tokens</span>(<span class="params">query_token, k, embed</span>):</span><br><span class="line">    topks, cos = knn(embed.idx_to_vec, embed[[query_token]], k + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">zip</span>(topks[<span class="number">1</span>:], cos[<span class="number">1</span>:]):</span><br><span class="line">        <span class="comment"># 排除输入词</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;embed.idx_to_token[<span class="built_in">int</span>(i)]&#125;</span>: cosine similarity = <span class="subst">&#123;<span class="built_in">float</span>(c):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在词类比中，找到一个词，其向量值与vec(c) + vec(b) - vec(a)最接近</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_analogy</span>(<span class="params">token_a, token_b, token_c, embed</span>):</span><br><span class="line">    vecs = embed[[token_a, token_b, token_c]]</span><br><span class="line">    x = vecs[<span class="number">1</span>] - vecs[<span class="number">0</span>] + vecs[<span class="number">2</span>]  <span class="comment"># b - a + c</span></span><br><span class="line">    topks, cos = knn(embed.idx_to_vec, x, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> embed.idx_to_token[<span class="built_in">int</span>(topks[<span class="number">0</span>])]  <span class="comment"># 删除未知词</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 加载预训练向量</span></span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;glove.6b.50d&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;glove.6B.50d.zip&#x27;</span>,</span><br><span class="line">                                    <span class="string">&#x27;0b8703943ccdb6eb788e6f091b8946e82231bc4d&#x27;</span>)</span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;glove.6b.100d&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;glove.6B.100d.zip&#x27;</span>,</span><br><span class="line">                                     <span class="string">&#x27;cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a&#x27;</span>)</span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;glove.42b.300d&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;glove.42B.300d.zip&#x27;</span>,</span><br><span class="line">                                      <span class="string">&#x27;b5116e234e9eb9076672cfeabf5469f3eec904fa&#x27;</span>)</span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;wiki.en&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;wiki.en.zip&#x27;</span>,</span><br><span class="line">                               <span class="string">&#x27;c1816da3821ae9f43899be655002f6c723e91b88&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载50维GloVe嵌入，以创建TokenEmbedding实例</span></span><br><span class="line">    glove_6b50d = TokenEmbedding(<span class="string">&#x27;glove.6b.50d&#x27;</span>)</span><br><span class="line">    <span class="comment"># 看一下词表大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;size of vocab:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(glove_6b50d))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用预训练词向量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词相似度</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;token similarity:&quot;</span>)</span><br><span class="line">    <span class="comment"># glove_6b50d中预训练词向量的词表包含400000个词和一个特殊的未知词元。</span></span><br><span class="line">    <span class="comment"># 排除输入词和未知词元后，在词表中找到与“chip”一词语义最相似的三个词</span></span><br><span class="line">    <span class="built_in">print</span>(get_similar_tokens(<span class="string">&#x27;chip&#x27;</span>, <span class="number">3</span>, glove_6b50d))</span><br><span class="line">    <span class="comment"># 再看一下girl, beautiful</span></span><br><span class="line">    <span class="built_in">print</span>(get_similar_tokens(<span class="string">&#x27;girl&#x27;</span>, <span class="number">3</span>, glove_6b50d))</span><br><span class="line">    <span class="built_in">print</span>(get_similar_tokens(<span class="string">&#x27;beautiful&#x27;</span>, <span class="number">3</span>, glove_6b50d))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词类比</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;token compare:&quot;</span>)</span><br><span class="line">    <span class="comment"># 除了找到相似的词，还可以将词向量应用到词类比任务中，比如“man”:“woman”::“son”:“daughter”就是一个词的类比</span></span><br><span class="line">    <span class="comment"># “man”是对“woman”的类比，“son”是对“daughter”的类比</span></span><br><span class="line">    <span class="comment"># 对于单词类比a : b :: c : d，给出前三个词a、b和c，找到d。</span></span><br><span class="line">    <span class="comment"># 用vec(w)表示词w的向量，为了完成这个类比，将要找到一个词w，其向量与vec(c) + vec(b) − vec(a)的结果最相似、</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;man:woman::son:?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(get_analogy(<span class="string">&#x27;man&#x27;</span>, <span class="string">&#x27;woman&#x27;</span>, <span class="string">&#x27;son&#x27;</span>, glove_6b50d))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;beijing:china::tokyo:?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(get_analogy(<span class="string">&#x27;beijing&#x27;</span>, <span class="string">&#x27;china&#x27;</span>, <span class="string">&#x27;tokyo&#x27;</span>, glove_6b50d))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;bad:worst::big:?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(get_analogy(<span class="string">&#x27;bad&#x27;</span>, <span class="string">&#x27;worst&#x27;</span>, <span class="string">&#x27;big&#x27;</span>, glove_6b50d))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">size of vocab:</span><br><span class="line">400001</span><br><span class="line">token similarity:</span><br><span class="line">chips: cosine similarity = 0.856</span><br><span class="line">intel: cosine similarity = 0.749</span><br><span class="line">electronics: cosine similarity = 0.749</span><br><span class="line">None</span><br><span class="line">boy: cosine similarity = 0.933</span><br><span class="line">woman: cosine similarity = 0.907</span><br><span class="line">mother: cosine similarity = 0.835</span><br><span class="line">None</span><br><span class="line">lovely: cosine similarity = 0.921</span><br><span class="line">gorgeous: cosine similarity = 0.893</span><br><span class="line">wonderful: cosine similarity = 0.830</span><br><span class="line">None</span><br><span class="line">token compare:</span><br><span class="line">man:woman::son:?</span><br><span class="line">daughter</span><br><span class="line">beijing:china::tokyo:?</span><br><span class="line">japan</span><br><span class="line">bad:worst::big:?</span><br><span class="line">biggest</span><br></pre></td></tr></table></figure><h2 id="Transformers的双向编码器表示-BERT">Transformers的双向编码器表示(BERT)</h2><p>介绍到现在，上文提到的所有词嵌入模型都是<strong>上下文无关</strong>的，而现在，我们引入上下文敏感模型。</p><h3 id="上下文无关-敏感模型">上下文无关/敏感模型</h3><p>考虑之前使用的那些词嵌入模型word2vec和GloVe，它们都<strong>将相同的预训练向量分配给同一个词，而不考虑词的上下文</strong>（如果有的话）。形式上，任何词元x的上下文无关表示是函数f(x)，其仅将x作为其输入。考虑到自然语言中丰富的多义现象和复杂的语义，上下文无关表示具有明显的局限性。因为同一个词在不同的上下文中可能表达截然不同的意思。</p><p>这推动了上下文敏感模型的出现，其中词的表征取决于上下文，即词元<em>x</em>的上下文敏感表示函数<em>f(x, c(x))</em>，取决于<em>x</em>及其上下文<em>c(x)</em>。</p><h3 id="特定任务-不可知任务">特定任务/不可知任务</h3><p>现有的各种自然语言处理的解决方案都<strong>依赖于一个特定于任务的架构</strong>，然而为每一个任务设计一个特定的架构是一件很困难的事情。<strong>GPT模型为上下文的敏感表示设计了通用的任务无关模型</strong>；<strong>GPT建立在Transformer解码器的基础上</strong>，预训练了一个用于表示文本序列的语言模型。当将GPT应用于下游任务时，语言模型的输出将被送到一个附加的线性输出层，以预测任务的标签。</p><p>然而，由于语言模型的<strong>自回归特性</strong>，<strong>GPT只能向前看</strong>（从左到右）。在“i went to the bank to deposit cash”（我去银行存现金）和“i went to the bank to sit down”（我去河岸边坐下）的上下文中，由于“bank”对其左边的上下文敏感，GPT将返回“bank”的相同表示，尽管它有不同的含义。</p><h3 id="BERT">BERT</h3><p>将两个最好的（上下文敏感模型和不可知任务）结合起来。</p><p>原理见<a href="https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html#bert-combining-the-best-of-both-worlds">15.8. Bidirectional Encoder Representations from Transformers (BERT) — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get_tokens_and_segments将一个句子或两个句子作为输入，然后返回BERT输入序列的标记及其相应的片段索引。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_tokens_and_segments</span>(<span class="params">tokens_a, tokens_b=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取输入序列的词元及其片段索引&quot;&quot;&quot;</span></span><br><span class="line">    tokens = [<span class="string">&#x27;&lt;cls&gt;&#x27;</span>] + tokens_a + [<span class="string">&#x27;&lt;sep&gt;&#x27;</span>]</span><br><span class="line">    <span class="comment"># 0和1分别标记A和B</span></span><br><span class="line">    segments = [<span class="number">0</span>] * (<span class="built_in">len</span>(tokens_a) + <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> tokens_b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        tokens += tokens_b + [<span class="string">&#x27;&lt;sep&gt;&#x27;</span>]</span><br><span class="line">        segments += [<span class="number">1</span>] * (<span class="built_in">len</span>(tokens_b) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> tokens, segments</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># BERTEncoder使用片段嵌入和可学习的位置嵌入</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTEncoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hidden, norm_shape, ffn_num_input, ffn_num_hidden, num_heads,</span></span><br><span class="line"><span class="params">                 num_layers, dropout, max_len=<span class="number">1000</span>, key_size=<span class="number">768</span>, query_size=<span class="number">768</span>, value_size=<span class="number">768</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTEncoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.token_embedding = nn.Embedding(vocab_size, num_hidden)</span><br><span class="line">        <span class="variable language_">self</span>.segment_embedding = nn.Embedding(<span class="number">2</span>, num_hidden)</span><br><span class="line">        <span class="variable language_">self</span>.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            <span class="variable language_">self</span>.blks.add_module(<span class="string">f&quot;<span class="subst">&#123;i&#125;</span>&quot;</span>, d2l.EncoderBlock(</span><br><span class="line">                key_size, query_size, value_size, num_hidden, norm_shape, ffn_num_input, ffn_num_hidden, num_heads, dropout, <span class="literal">True</span></span><br><span class="line">            ))</span><br><span class="line">        <span class="comment"># 在BERT中，位置嵌入是可学习的，因此创建一个足够长的位置嵌入参数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, max_len, num_hidden))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens, segments, valid_lens</span>):</span><br><span class="line">        <span class="comment"># X的形状保持不变: (批量大小，最大序列长度，num_hidden)</span></span><br><span class="line">        X = <span class="variable language_">self</span>.token_embedding(tokens) + <span class="variable language_">self</span>.segment_embedding(segments)</span><br><span class="line">        X = X + <span class="variable language_">self</span>.pos_embedding.data[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> <span class="variable language_">self</span>.blks:</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现下面的MaskLM类来预测BERT预训练的掩蔽语言模型任务中的掩蔽标记</span></span><br><span class="line"><span class="comment"># 预测使用单隐藏层的多层感知机（self.mlp）。在前向推断中，它需要两个输入：BERTEncoder的编码结果和用于预测的词元位置</span></span><br><span class="line"><span class="comment"># 输出这些位置的预测结果</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaskLM</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT的掩蔽语言模型任务&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hidden, num_inputs=<span class="number">768</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MaskLM, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(num_inputs, num_hidden),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.LayerNorm(num_hidden),</span><br><span class="line">            nn.Linear(num_hidden, vocab_size)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, pred_positions</span>):</span><br><span class="line">        num_pred_positions = pred_positions.shape[<span class="number">1</span>]</span><br><span class="line">        pred_positions = pred_positions.reshape(-<span class="number">1</span>)</span><br><span class="line">        batch_size = X.shape[<span class="number">0</span>]</span><br><span class="line">        batch_idx = torch.arange(<span class="number">0</span>, batch_size)</span><br><span class="line">        <span class="comment"># 假设batch_size=2，num_pred_positions=3</span></span><br><span class="line">        <span class="comment"># 那么batch_idx是[0, 0, 0, 1, 1, 1]</span></span><br><span class="line">        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)</span><br><span class="line">        masked_X = X[batch_idx, num_pred_positions]</span><br><span class="line">        masked_X = masked_X.reshape((batch_size, num_pred_positions, -<span class="number">1</span>))</span><br><span class="line">        mlm_Y_hat = <span class="variable language_">self</span>.mlp(masked_X)</span><br><span class="line">        <span class="keyword">return</span> mlm_Y_hat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下一句预测模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NextSentencePred</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT的下一句预测任务&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(NextSentencePred, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Linear(num_inputs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># X的形状(batch_size, num_hidden)</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.output(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 整合代码实现完整的BERT</span></span><br><span class="line"><span class="comment"># 在预训练BERT时，最终的损失函数时掩蔽语言模型损失函数和下一句预测损失函数的线性组合</span></span><br><span class="line"><span class="comment"># 现在通过实例化三个类：BERTEncoder、MaskLM和NextSentencePred来定义BERT模型</span></span><br><span class="line"><span class="comment"># 前向推断返回编码后的BERT表示encoded_X、掩蔽语言模型预测mlm_Y_hat和下一句预测nsp_Y_hat</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers,</span></span><br><span class="line"><span class="params">                 dropout, max_len=<span class="number">1000</span>, key_size=<span class="number">768</span>, query_size=<span class="number">768</span>, value_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 hid_in_features=<span class="number">768</span>, mlm_in_features=<span class="number">768</span>, nsp_in_features=<span class="number">768</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = BERTEncoder(</span><br><span class="line">            vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers,</span><br><span class="line">            dropout, max_len=max_len, key_size=key_size, query_size=query_size, value_size=value_size</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.hidden = nn.Sequential(</span><br><span class="line">            nn.Linear(hid_in_features, num_hiddens),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)</span><br><span class="line">        <span class="variable language_">self</span>.nsp = NextSentencePred(nsp_in_features)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens, segments, valid_lens=<span class="literal">None</span>, pred_positions=<span class="literal">None</span></span>):</span><br><span class="line">        encoded_X = <span class="variable language_">self</span>.encoder(tokens, segments, valid_lens)</span><br><span class="line">        <span class="keyword">if</span> pred_positions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mlm_Y_hat = <span class="variable language_">self</span>.mlm(encoded_X, pred_positions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mlm_Y_hat = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 用于下一句预测的多层感知机分类器的隐藏层，0是&quot;&lt;cls&gt;&quot;标记的索引</span></span><br><span class="line">        nsp_Y_hat = <span class="variable language_">self</span>.nsp(<span class="variable language_">self</span>.hidden(encoded_X[:, <span class="number">0</span>, :]))</span><br><span class="line">        <span class="keyword">return</span> encoded_X, mlm_Y_hat, nsp_Y_hat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 假设词表大小为10000，为了演示BERTEncoder的前向推断，让我们创建一个实例并初始化它的参数</span></span><br><span class="line">    vocab_size, num_hidden, ffn_num_hidden, num_heads = <span class="number">10000</span>, <span class="number">768</span>, <span class="number">1024</span>, <span class="number">4</span></span><br><span class="line">    norm_shape, ffn_num_input, num_layers, dropout = [<span class="number">768</span>], <span class="number">768</span>, <span class="number">2</span>, <span class="number">0.2</span></span><br><span class="line">    encoder = BERTEncoder(vocab_size, num_hidden, norm_shape, ffn_num_input, ffn_num_hidden, num_heads, num_layers, dropout)</span><br><span class="line">    tokens = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">2</span>, <span class="number">8</span>))</span><br><span class="line">    segments = torch.tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">    encoder_X = encoder(tokens, segments, <span class="literal">None</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of encoded_X:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(encoder_X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预训练任务</span></span><br><span class="line">    <span class="comment"># 预训练任务的原理见 https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html#pretraining-tasks</span></span><br><span class="line">    <span class="comment"># 演示MaskLM的前向推断</span></span><br><span class="line">    mlm = MaskLM(vocab_size, num_hidden)</span><br><span class="line">    mlm_positions = torch.tensor([[<span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>], [<span class="number">6</span>, <span class="number">1</span>, <span class="number">5</span>]])</span><br><span class="line">    mlm_Y_hat = mlm(encoder_X, mlm_positions)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of mlm_Y_hat:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mlm_Y_hat.shape)</span><br><span class="line">    <span class="comment"># 通过掩码下的预测词元mlm_Y的真实标签mlm_Y_hat，我们可以计算在BERT预训练中的遮蔽语言模型任务的交叉熵损失</span></span><br><span class="line">    mlm_Y = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]])</span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    mlm_l = loss(mlm_Y_hat.reshape((-<span class="number">1</span>, vocab_size)), mlm_Y.reshape(-<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of mlm_l:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mlm_l.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 下一句预测</span></span><br><span class="line">    encoder_X = torch.flatten(encoder_X, start_dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># NSP的输入形状为(batch_size, num_hidden)</span></span><br><span class="line">    nsp = NextSentencePred(encoder_X.shape[-<span class="number">1</span>])</span><br><span class="line">    nsp_Y_hat = nsp(encoder_X)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of nsp_Y_hat:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(nsp_Y_hat.shape)</span><br><span class="line">    <span class="comment"># 还可以计算两个二元分类的交叉熵损失</span></span><br><span class="line">    nsp_y = torch.tensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    nsp_l = loss(nsp_Y_hat, nsp_y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of nsp_l:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(nsp_l.shape)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">shape of encoded_X:</span><br><span class="line">torch.Size([2, 8, 768])</span><br><span class="line">shape of mlm_Y_hat:</span><br><span class="line">torch.Size([2, 3, 10000])</span><br><span class="line">shape of mlm_l:</span><br><span class="line">torch.Size([6])</span><br><span class="line">shape of nsp_Y_hat:</span><br><span class="line">torch.Size([2, 2])</span><br><span class="line">shape of nsp_l:</span><br><span class="line">torch.Size([2])</span><br></pre></td></tr></table></figure><h2 id="用于预训练BERT的数据集">用于预训练BERT的数据集</h2><p>为了训练BERT模型，我们需要以理想的格式生成数据集，以便<strong>进行两个预训练任务：遮蔽语言模型和下一句预测</strong>。根据经验，在定制的数据集上对BERT进行预训练更有效，为了方便演示，使用较小的语料库<strong>WikiText-2</strong></p><p>与PTB数据集相比，WikiText-2</p><ul><li>保留了原来的标点符号，适合于下一句预测；</li><li>保留了原来的大小写和数字；</li><li>大了一倍以上。</li></ul><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_read_wiki</span>(<span class="params">data_dir</span>):</span><br><span class="line">    file_name = os.path.join(data_dir, <span class="string">&#x27;wiki.train.tokens&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_name, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="comment"># 大写转小写</span></span><br><span class="line">    paragraphs = [line.strip().lower().split(<span class="string">&#x27; . &#x27;</span>) <span class="keyword">for</span> line <span class="keyword">in</span> lines <span class="keyword">if</span> <span class="built_in">len</span>(line.split(<span class="string">&#x27; . &#x27;</span>)) &gt;= <span class="number">2</span>]</span><br><span class="line">    random.shuffle(paragraphs)</span><br><span class="line">    <span class="keyword">return</span> paragraphs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为预训练定义辅助函数</span></span><br><span class="line"><span class="comment"># 生成下一句预测任务的数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_next_sentence</span>(<span class="params">sentence, next_sentence, paragraphs</span>):</span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">        is_next = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># paragraphs 是三重列表的嵌套</span></span><br><span class="line">        next_sentence = random.choice(random.choice(paragraphs))</span><br><span class="line">        is_next = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> sentence, next_sentence, is_next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从输入paragraph生成用于下一句预测的训练样本，paragraph是句子列表，每个句子是词元列表</span></span><br><span class="line"><span class="comment"># 自变量max_len指定预训练期间的BERT输入序列的最大长度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_nsp_data_from_paragraph</span>(<span class="params">paragraph, paragraphs, vocab, max_len</span>):</span><br><span class="line">    nsp_data_from_paragraph = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(paragraph) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 从相邻的两个句子中获取NSP数据的样本</span></span><br><span class="line">        tokens_a, tokens_b, is_next = _get_next_sentence(paragraph[i], paragraph[i + <span class="number">1</span>], paragraphs)</span><br><span class="line">        <span class="comment"># 考虑1个&#x27;&lt;cls&gt;&#x27;词元和2个&#x27;&lt;sep&gt;&#x27;词元</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(tokens_a) + <span class="built_in">len</span>(tokens_b) + <span class="number">3</span> &gt; max_len:</span><br><span class="line">            <span class="comment"># +3表示&lt;cls&gt;和两个&lt;sep&gt;的长度</span></span><br><span class="line">            <span class="comment"># 如果加上特殊词元后的序列长度超过最大长度，则跳过该样本</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 获取BERT输入所需的词元序列和段落标记序列</span></span><br><span class="line">        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)</span><br><span class="line">        <span class="comment"># 将准备好的样本添加到NSP数据列表中</span></span><br><span class="line">        nsp_data_from_paragraph.append((tokens, segments, is_next))</span><br><span class="line">    <span class="keyword">return</span> nsp_data_from_paragraph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成遮蔽语言模型任务的数据</span></span><br><span class="line"><span class="comment"># 下面实现的函数返回可能替换后的输入词元、发生预测的词元索引和这些预测的标签</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_replace_mlm_tokens</span>(<span class="params">tokens, candidate_pred_positions, num_mlm_pred, vocab</span>):</span><br><span class="line">    <span class="comment"># 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的&quot;&lt;mask&gt;&quot;或随机词元</span></span><br><span class="line">    mlm_input_tokens = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    pred_positions_and_labels = []</span><br><span class="line">    <span class="comment"># 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测</span></span><br><span class="line">    random.shuffle(candidate_pred_positions)</span><br><span class="line">    <span class="keyword">for</span> mlm_pred_position <span class="keyword">in</span> candidate_pred_positions:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(pred_positions_and_labels) &gt;= num_mlm_pred:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        masked_token = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 80%的概率将词替换为“&lt;mask&gt;”词元</span></span><br><span class="line">        <span class="keyword">if</span> random.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">            masked_token = <span class="string">&#x27;&lt;mask&gt;&#x27;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 10%保持不变</span></span><br><span class="line">            <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">                masked_token = tokens[mlm_pred_position]</span><br><span class="line">            <span class="comment"># 10%随即替换</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                masked_token = random.choice(vocab.idx_to_token)</span><br><span class="line">        mlm_input_tokens[mlm_pred_position] = masked_token</span><br><span class="line">        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))</span><br><span class="line">    <span class="keyword">return</span> mlm_input_tokens, pred_positions_and_labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下函数将BERT输入序列(tokens)作为输入并返回输出词元的索引、发生预测的词元索引以及这些预测的标签索引</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_mlm_data_from_tokens</span>(<span class="params">tokens, vocab</span>):</span><br><span class="line">    candidate_pred_positions = []</span><br><span class="line">    <span class="comment"># tokens是一个字符串列表</span></span><br><span class="line">    <span class="keyword">for</span> i, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">        <span class="comment"># 在遮蔽语言模型任务中不会预测特殊词元</span></span><br><span class="line">        <span class="keyword">if</span> token <span class="keyword">in</span> [<span class="string">&#x27;&lt;cls&gt;&#x27;</span>, <span class="string">&#x27;&lt;sep&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        candidate_pred_positions.append(i)</span><br><span class="line">    <span class="comment"># 遮蔽语言模型任务中只预测15%的词元</span></span><br><span class="line">    num_mlm_pred = <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">round</span>(<span class="built_in">len</span>(tokens) * <span class="number">0.15</span>))</span><br><span class="line">    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_pred, vocab)</span><br><span class="line">    pred_positions_and_labels = <span class="built_in">sorted</span>(pred_positions_and_labels, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    pred_positions = [v[<span class="number">0</span>] <span class="keyword">for</span> v <span class="keyword">in</span> pred_positions_and_labels]</span><br><span class="line">    mlm_pred_labels = [v[<span class="number">1</span>] <span class="keyword">for</span> v <span class="keyword">in</span> pred_positions_and_labels]</span><br><span class="line">    <span class="keyword">return</span> vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将文本转换为预训练数据集, 将特殊的&lt;mask&gt;词元附加到输入</span></span><br><span class="line"><span class="comment"># 参数examples来自两个预训练任务辅助函数_get_nsp_data_from_paragraph和_get_mlm_data_from_tokens的输出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_pad_bert_inputs</span>(<span class="params">examples, max_len, vocab</span>):</span><br><span class="line">    max_num_mlm_pred = <span class="built_in">round</span>(max_len * <span class="number">0.15</span>)</span><br><span class="line">    all_token_ids, all_segments, valid_lens = [], [], []</span><br><span class="line">    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []</span><br><span class="line">    nsp_labels = []</span><br><span class="line">    <span class="keyword">for</span> (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) <span class="keyword">in</span> examples:</span><br><span class="line">        all_token_ids.append(torch.tensor(token_ids + [vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (max_len - <span class="built_in">len</span>(token_ids)), dtype=torch.long))</span><br><span class="line">        all_segments.append(torch.tensor(segments + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(segments)), dtype=torch.long))</span><br><span class="line">        <span class="comment"># valid_lens不包括&#x27;&lt;pad&gt;&#x27;的计数</span></span><br><span class="line">        valid_lens.append(torch.tensor(<span class="built_in">len</span>(token_ids), dtype=torch.float32))</span><br><span class="line">        all_pred_positions.append(torch.tensor(pred_positions + [<span class="number">0</span>] * (max_num_mlm_pred - <span class="built_in">len</span>(pred_positions)),</span><br><span class="line">                                           dtype=torch.long))</span><br><span class="line">        <span class="comment"># 填充词元的预测将通过乘以 0权重 在损失中过滤掉</span></span><br><span class="line">        all_mlm_weights.append(</span><br><span class="line">            torch.tensor([<span class="number">1.0</span>] * <span class="built_in">len</span>(mlm_pred_label_ids) + [<span class="number">0.0</span>] * (max_num_mlm_pred - <span class="built_in">len</span>(pred_positions)),</span><br><span class="line">                         dtype=torch.float32)</span><br><span class="line">        )</span><br><span class="line">        all_mlm_labels.append(</span><br><span class="line">            torch.tensor(mlm_pred_label_ids + [<span class="number">0</span>] * (max_num_mlm_pred - <span class="built_in">len</span>(mlm_pred_label_ids)),</span><br><span class="line">                         dtype=torch.long)</span><br><span class="line">        )</span><br><span class="line">        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))</span><br><span class="line">    <span class="keyword">return</span> (all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels,</span><br><span class="line">            nsp_labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># WikiText数据集，词元化时出现次数少于5次的不频繁词元将被过滤，使用d2l.tokenize进行词元化</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_WikiTextDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, paragraphs, max_len</span>):</span><br><span class="line">        <span class="comment"># 输入paragraphs[i]是代表段落的句子字符串列表</span></span><br><span class="line">        <span class="comment"># 而输出paragraphs[i]是代表段落的句子列表，其中每句都是词元列表</span></span><br><span class="line">        paragraphs = [d2l.tokenize(paragraph, token=<span class="string">&#x27;word&#x27;</span>) <span class="keyword">for</span> paragraph <span class="keyword">in</span> paragraphs]</span><br><span class="line">        sentences = [sentence <span class="keyword">for</span> paragraph <span class="keyword">in</span> paragraphs <span class="keyword">for</span> sentence <span class="keyword">in</span> paragraph]</span><br><span class="line">        <span class="variable language_">self</span>.vocab = d2l.Vocab(sentences, min_freq=<span class="number">5</span>, reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="string">&#x27;&lt;cls&gt;&#x27;</span>, <span class="string">&#x27;&lt;sep&gt;&#x27;</span>])</span><br><span class="line">        <span class="comment"># 获取下一个句子预测任务的数据</span></span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">for</span> paragraph <span class="keyword">in</span> paragraphs:</span><br><span class="line">            examples.extend(_get_nsp_data_from_paragraph(paragraph, paragraphs, <span class="variable language_">self</span>.vocab, max_len))</span><br><span class="line">        <span class="comment"># 获取遮蔽语言模型任务的数据</span></span><br><span class="line">        examples = [(_get_mlm_data_from_tokens(tokens, <span class="variable language_">self</span>.vocab) + (segments, is_next))</span><br><span class="line">                    <span class="keyword">for</span> tokens, segments, is_next <span class="keyword">in</span> examples]</span><br><span class="line">        <span class="comment"># 填充输入</span></span><br><span class="line">        (<span class="variable language_">self</span>.all_token_ids, <span class="variable language_">self</span>.all_segments, <span class="variable language_">self</span>.valid_lens, <span class="variable language_">self</span>.all_pred_positions, <span class="variable language_">self</span>.all_mlm_weights,</span><br><span class="line">         <span class="variable language_">self</span>.all_mlm_labels, <span class="variable language_">self</span>.nsp_labels) = _pad_bert_inputs(examples, max_len, <span class="variable language_">self</span>.vocab)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="variable language_">self</span>.all_token_ids[idx], <span class="variable language_">self</span>.all_segments[idx], <span class="variable language_">self</span>.valid_lens[idx], <span class="variable language_">self</span>.all_pred_positions[idx],</span><br><span class="line">                <span class="variable language_">self</span>.all_mlm_weights[idx], <span class="variable language_">self</span>.all_mlm_labels[idx], <span class="variable language_">self</span>.nsp_labels[idx])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.all_token_ids)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集并生成预训练样本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_wiki</span>(<span class="params">batch_size, max_len</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载WikiText-2数据集&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;wikitext-2&#x27;</span>, <span class="string">&#x27;wikitext-2&#x27;</span>)</span><br><span class="line">    paragraphs = _read_wiki(data_dir)</span><br><span class="line">    train_set = _WikiTextDataset(paragraphs, max_len)</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, train_set.vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 在WikiText-2 数据集中，每行代表一个段落，其中在任意标点符号及其前面的词元之间插入空格</span></span><br><span class="line">    <span class="comment"># 保留至少有两句话的段落</span></span><br><span class="line">    <span class="comment"># 使用分号作为分隔符来拆分句子</span></span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;wikitext-2&#x27;</span>] = (</span><br><span class="line">        <span class="string">&#x27;https://s3.amazonaws.com/research.metamind.io/wikitext/&#x27;</span></span><br><span class="line">        <span class="string">&#x27;wikitext-2-v1.zip&#x27;</span>, <span class="string">&#x27;3c914d17d80b1459be871a5039ac23e752a53cbe&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看一下小批量BERT预训练样本的形状</span></span><br><span class="line">    <span class="comment"># 在每个BERT输入序列中，为遮蔽语言模型任务预测10(64 * 0.15)个位置</span></span><br><span class="line">    batch_size, max_len = <span class="number">512</span>, <span class="number">64</span></span><br><span class="line">    train_iter, vocab = load_data_wiki(batch_size, max_len)</span><br><span class="line">    <span class="keyword">for</span> (token_X, segments_X, valid_lens_X, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;shape of token_X, segments_X, valid_lens_X, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(token_X.shape, segments_X.shape, valid_lens_X.shape, pred_positions_X.shape, mlm_weights_X.shape,</span><br><span class="line">              mlm_Y.shape, nsp_y.shape)</span><br><span class="line">        <span class="keyword">break</span>  <span class="comment"># 看一个就行</span></span><br><span class="line">    <span class="comment"># 看一下词表大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;vocab size:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TBD</span><br><span class="line">似乎有点bug，zip文件没法从官网下载下来</span><br></pre></td></tr></table></figure><h2 id="预训练BERT">预训练BERT</h2><p>TBD</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习第一次作业报告</title>
      <link href="/ymhui.github.io/2024/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/"/>
      <url>/ymhui.github.io/2024/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<h1>决策树实现</h1><h2 id="获取数据集">获取数据集</h2><p>获取鸢尾花数据集</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = load_iris()</span><br></pre></td></tr></table></figure><h2 id="划分数据集">划分数据集</h2><p>将数据集划分为训练集和测试集，划分比例按默认值，随便初始化随机种子。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class="number">114</span>)</span><br></pre></td></tr></table></figure><h2 id="使用决策树预估器进行分类">使用决策树预估器进行分类</h2><p>基于信息增益进行树的划分，之后用训练集训练模型</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimator = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>)</span><br><span class="line">estimator.fit(x_train, y_train)</span><br></pre></td></tr></table></figure><h2 id="模型评估">模型评估</h2><p>使用训练好的模型进行预测，查看一下基本预测情况，然后用预测值和真实值进行比较，查看正确与否；最后打分判断模型准确率。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_predict:\n&quot;</span>, y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_predict == y_test:\n&quot;</span>, y_predict == y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score = &quot;</span>, estimator.score(x_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">y_predict:</span><br><span class="line"> [2 2 1 1 2 2 2 1 2 0 2 0 1 1 2 1 0 1 1 0 2 2 1 1 2 1 0 2 2 1 1 1 0 1 1 0 1</span><br><span class="line"> 1]</span><br><span class="line">y_predict == y_test:</span><br><span class="line"> [ True  True  True  True  True  True  True  True False  True  True  True</span><br><span class="line">  True False  True  True  True  True  True  True  True  True  True  True</span><br><span class="line">  True  True  True  True  True  True  True  True  True  True  True  True</span><br><span class="line">  True  True]</span><br><span class="line">score =  0.9473684210526315</span><br></pre></td></tr></table></figure><p>从结果可以看出，经典决策树算法对经典鸢尾花数据集的分类效果是非常好的，准确率很高，模型在分类任务上表现较好，能够有效地区分不同的类别。</p><h2 id="可视化树">可视化树</h2><p>通过这一步可以可视化经过数据训练拟合后的决策树。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plot_tree(estimator, filled=<span class="literal">True</span>, feature_names=iris.feature_names, class_names=iris.target_names)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><code>信息增益的决策树</code></p><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240929085020072.png" alt="image-20240929085020072"></p><h2 id="可视化分类预测结果">可视化分类预测结果</h2><ol><li>定义可视化辅助函数</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_classification</span>(<span class="params">x_reduced, y_pred, y</span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">    scatter = plt.scatter(x_reduced[:, <span class="number">0</span>], x_reduced[:, <span class="number">1</span>], c=y_pred, marker=<span class="string">&#x27;o&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>, s=<span class="number">100</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>, alpha=<span class="number">0.7</span>, label=<span class="string">&#x27;Predicted Labels&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Decision Tree Classification Results&quot;</span>)</span><br><span class="line">    plt.colorbar(scatter, ticks=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], label=<span class="string">&#x27;Class&#x27;</span>)</span><br><span class="line">    plt.scatter(x_reduced[:, <span class="number">0</span>], x_reduced[:, <span class="number">1</span>], c=y, marker=<span class="string">&#x27;x&#x27;</span>, edgecolor=<span class="string">&#x27;red&#x27;</span>, s=<span class="number">100</span>,</span><br><span class="line">                label=<span class="string">&#x27;True Labels&#x27;</span>)  <span class="comment"># 显示真实标签</span></span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><ol start="2"><li>可视化，可以更加直观地看到预测情况与真实情况的对比（相比标准文本输出）</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_classification(x_test, y_predict, y_test)</span><br></pre></td></tr></table></figure><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240929123706634.png" alt=""></p><p>从可视化结果可以更加直观地看出，大部分预测结果与真实结果的颜色是相同的，即预测对了。</p><h2 id="剪枝优化">剪枝优化</h2><p>不妨尝试后剪枝进行优化尝试，我们遍历所有可取的剪枝系数，比较最佳的结果。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;try pruning:&quot;</span>)</span><br><span class="line">path = estimator.cost_complexity_pruning_path(x_train, y_train)</span><br><span class="line">ccp_alphas, impurities = path.ccp_alphas, path.impurities</span><br><span class="line"><span class="keyword">for</span> ccp_alpha <span class="keyword">in</span> ccp_alphas:</span><br><span class="line">    estimator_pruned = DecisionTreeClassifier(random_state=<span class="number">0</span>, ccp_alpha=ccp_alpha)</span><br><span class="line">    estimator_pruned.fit(x_train, y_train)</span><br><span class="line">    y_pred = estimator_pruned.predict(x_test)</span><br><span class="line">    score = estimator_pruned.score(x_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;ccp_alpha:&quot;</span>, ccp_alpha, <span class="string">&quot;score:&quot;</span>, score)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">try pruning:</span><br><span class="line">ccp_alpha: 0.0 score: 0.9473684210526315</span><br><span class="line">ccp_alpha: 0.024597209840745256 score: 0.868421052631579</span><br><span class="line">ccp_alpha: 0.024597209840745256 score: 0.868421052631579</span><br><span class="line">ccp_alpha: 0.03426345497041028 score: 0.868421052631579</span><br><span class="line">ccp_alpha: 0.0725895787024318 score: 0.868421052631579</span><br><span class="line">ccp_alpha: 0.4576883769083686 score: 0.18421052631578946</span><br><span class="line">ccp_alpha: 0.9607694580407837 score: 0.18421052631578946</span><br></pre></td></tr></table></figure><p>发现还是不剪枝的分类效果最好</p><h2 id="比较基尼系数与信息增益">比较基尼系数与信息增益</h2><p>最后我们比较一下基于基尼系数划分决策树和基于信息增益划分决策树的区别</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimator = DecisionTreeClassifier(criterion=<span class="string">&#x27;gini&#x27;</span>)</span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;gini:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>, estimator.score(x_test, y_test))</span><br><span class="line">estimator = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>)</span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;entropy:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>, estimator.score(x_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gini:</span><br><span class="line">score: 0.9736842105263158</span><br><span class="line">entropy:</span><br><span class="line">score: 0.9473684210526315</span><br></pre></td></tr></table></figure><p><code>基尼系数的决策树</code></p><p><img src="C:%5CUsers%5Cyuminghui%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240929094240899.png" alt="image-20240929094240899"></p><h1>完整代码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_classification</span>(<span class="params">x_reduced, y_pred, y</span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">    scatter = plt.scatter(x_reduced[:, <span class="number">0</span>], x_reduced[:, <span class="number">1</span>], c=y_pred, marker=<span class="string">&#x27;o&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>, s=<span class="number">100</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>,</span><br><span class="line">                          alpha=<span class="number">0.7</span>, label=<span class="string">&#x27;Predicted Labels&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Decision Tree Classification Results&quot;</span>)</span><br><span class="line">    plt.colorbar(scatter, ticks=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], label=<span class="string">&#x27;Class&#x27;</span>)</span><br><span class="line">    plt.scatter(x_reduced[:, <span class="number">0</span>], x_reduced[:, <span class="number">1</span>], c=y, marker=<span class="string">&#x27;x&#x27;</span>, s=<span class="number">100</span>,</span><br><span class="line">                label=<span class="string">&#x27;True Labels&#x27;</span>)  <span class="comment"># 显示真实标签</span></span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decision_tree</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    决策树对鸢尾花分类</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取数据集</span></span><br><span class="line">    iris = load_iris()</span><br><span class="line">    <span class="built_in">print</span>(iris.data)</span><br><span class="line">    <span class="built_in">print</span>(iris.target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class="number">114</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (决策树不需要计算距离，因此不需要标准化)</span></span><br><span class="line">    transfer = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 决策树预估器进行分类</span></span><br><span class="line">    estimator = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>)</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型评估</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y_predict:\n&quot;</span>, y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y_predict == y_test:\n&quot;</span>, y_predict == y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;score = &quot;</span>, estimator.score(x_test, y_test))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可视化决策树</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">    plot_tree(estimator, filled=<span class="literal">True</span>, feature_names=iris.feature_names, class_names=iris.target_names)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可视化分类结果</span></span><br><span class="line">    visualize_classification(x_test, y_predict, y_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 后剪枝</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;try pruning:&quot;</span>)</span><br><span class="line">    path = estimator.cost_complexity_pruning_path(x_train, y_train)</span><br><span class="line">    ccp_alphas, impurities = path.ccp_alphas, path.impurities</span><br><span class="line">    <span class="keyword">for</span> ccp_alpha <span class="keyword">in</span> ccp_alphas:</span><br><span class="line">        estimator_pruned = DecisionTreeClassifier(random_state=<span class="number">0</span>, ccp_alpha=ccp_alpha)</span><br><span class="line">        estimator_pruned.fit(x_train, y_train)</span><br><span class="line">        y_pred = estimator_pruned.predict(x_test)</span><br><span class="line">        score = estimator_pruned.score(x_test, y_test)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ccp_alpha:&quot;</span>, ccp_alpha, <span class="string">&quot;score:&quot;</span>, score)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 单独比较一下基尼系数与信息增益</span></span><br><span class="line">    <span class="comment"># gini</span></span><br><span class="line">    estimator = DecisionTreeClassifier(criterion=<span class="string">&#x27;gini&#x27;</span>)</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;gini:&quot;</span>)</span><br><span class="line">    <span class="comment"># 插入一下gini的树</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">    plot_tree(estimator, filled=<span class="literal">True</span>, feature_names=iris.feature_names, class_names=iris.target_names)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>, estimator.score(x_test, y_test))</span><br><span class="line">    <span class="comment"># 信息增益</span></span><br><span class="line">    estimator = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>)</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;entropy:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>, estimator.score(x_test, y_test))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    decision_tree()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 软件学院 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 课程作业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>现代卷积神经网络</title>
      <link href="/ymhui.github.io/2024/09/27/%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/ymhui.github.io/2024/09/27/%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1>写在前面</h1><blockquote><p>参考书籍</p></blockquote><p><a href="http://www.d2l.ai">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><h1>现代卷积神经网络</h1><p><strong>source code</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui/DeepLearning: Deep Learning with pytorch (github.com)</a></p><p><strong>use git to clone</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p><p><code>/modernCNN</code></p><blockquote><p><a href="http://AlexNet.py">AlexNet.py</a><a href="http://VGG.py">VGG.py</a><a href="http://NiN.py">NiN.py</a><a href="http://GoogLeNet.py">GoogLeNet.py</a>tensor_normalize_self.pytensor_normalize_lib.py</p></blockquote><h2 id="AlexNet">AlexNet</h2><p>理论部分见<a href="https://d2l.ai/chapter_convolutional-modern/alexnet.html#deep-convolutional-neural-networks-alexnet">8.1. Deep Convolutional Neural Networks (AlexNet) — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># AlexNet模型</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        <span class="comment"># 这里使用一个11*11的更大窗口来捕捉对象。</span></span><br><span class="line">        <span class="comment"># 同时，步幅为4，以减少输出的高度和宽度。</span></span><br><span class="line">        <span class="comment"># 另外，输出通道的数目远大于LeNet</span></span><br><span class="line">        <span class="comment"># 输入层到隐藏层1，卷积操作</span></span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>),</span><br><span class="line">        <span class="comment"># 激活函数，引入非线性</span></span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">        <span class="comment"># 隐藏层1到隐藏层2，卷积操作</span></span><br><span class="line">        nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># 使用三个连续的卷积层和较小的卷积窗口。</span></span><br><span class="line">        <span class="comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span></span><br><span class="line">        <span class="comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class="line">        nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 池化操作，降维减参</span></span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># 展平操作，将多维数据展平为一维数据s</span></span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">        <span class="comment"># 隐藏层到输出层，全连接层</span></span><br><span class="line">        nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># Dropout层，防止过拟合</span></span><br><span class="line">        nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">        <span class="comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line">    X = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">        X = layer(X)</span><br><span class="line">        <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: &#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load data</span></span><br><span class="line">    batch_size = <span class="number">128</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># start training</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;start training:&quot;</span>)</span><br><span class="line">    lr, num_epochs = <span class="number">0.01</span>, <span class="number">10</span></span><br><span class="line">    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, device=d2l.try_gpu())</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Conv2d output shape:  torch.Size([1, 96, 54, 54])</span><br><span class="line">ReLU output shape:  torch.Size([1, 96, 54, 54])</span><br><span class="line">MaxPool2d output shape:  torch.Size([1, 96, 26, 26])</span><br><span class="line">Conv2d output shape:  torch.Size([1, 256, 26, 26])</span><br><span class="line">ReLU output shape:  torch.Size([1, 256, 26, 26])</span><br><span class="line">MaxPool2d output shape:  torch.Size([1, 256, 12, 12])</span><br><span class="line">Conv2d output shape:  torch.Size([1, 384, 12, 12])</span><br><span class="line">ReLU output shape:  torch.Size([1, 384, 12, 12])</span><br><span class="line">Conv2d output shape:  torch.Size([1, 384, 12, 12])</span><br><span class="line">ReLU output shape:  torch.Size([1, 384, 12, 12])</span><br><span class="line">Conv2d output shape:  torch.Size([1, 256, 12, 12])</span><br><span class="line">ReLU output shape:  torch.Size([1, 256, 12, 12])</span><br><span class="line">MaxPool2d output shape:  torch.Size([1, 256, 5, 5])</span><br><span class="line">Flatten output shape:  torch.Size([1, 6400])</span><br><span class="line">Linear output shape:  torch.Size([1, 4096])</span><br><span class="line">ReLU output shape:  torch.Size([1, 4096])</span><br><span class="line">Dropout output shape:  torch.Size([1, 4096])</span><br><span class="line">Linear output shape:  torch.Size([1, 4096])</span><br><span class="line">ReLU output shape:  torch.Size([1, 4096])</span><br><span class="line">Dropout output shape:  torch.Size([1, 4096])</span><br><span class="line">Linear output shape:  torch.Size([1, 10])</span><br><span class="line">start training:</span><br><span class="line">training on cpu</span><br><span class="line">loss 0.331, train acc 0.879, test acc 0.884</span><br><span class="line">26.8 examples/sec on cpu</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/alexnet.png" alt="image-20240928002801586"></p><h2 id="使用块的网络VGG">使用块的网络VGG</h2><p>VGG可用于启发设计深层神经网络。</p><p>经典卷积神经网络的基本组成部分是下面的这个序列：</p><ul><li><p>带填充以保持分辨率的卷积层</p></li><li><p>非线性激活函数，如ReLU</p></li><li><p>汇聚层，如最大汇聚层</p></li></ul><p>一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层; <a href="https://d2l.ai/chapter_convolutional-modern/vgg.html#networks-using-blocks-vgg">8.2. Networks Using Blocks (VGG) — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现一个VGG块</span></span><br><span class="line"><span class="string">    :param num_convs: 卷积层数量</span></span><br><span class="line"><span class="string">    :param in_channels: 输入通道数量</span></span><br><span class="line"><span class="string">    :param out_channels: 输出通道数量</span></span><br><span class="line"><span class="string">    :return: 由卷积层、激活函数和池化层组成的序列模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># vgg</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全连接层部分的输入维度依赖于最后一个卷积块的输出尺寸</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks,</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># vgg网络</span></span><br><span class="line">    conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line">    net = vgg(conv_arch)</span><br><span class="line"></span><br><span class="line">    X = torch.randn(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> net:</span><br><span class="line">        X = blk(X)</span><br><span class="line">        <span class="built_in">print</span>(blk.__class__.__name__, <span class="string">&quot;output shape: &quot;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># start training</span></span><br><span class="line">    <span class="comment"># 由于VGG-11比AlexNet的计算量更大，因此构建一个通道较少的网络，足够训练该数据集</span></span><br><span class="line">    ratio = <span class="number">4</span></span><br><span class="line">    small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>] // ratio) <span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">    net = vgg(small_conv_arch)</span><br><span class="line">    lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Sequential output shape:  torch.Size([1, 64, 112, 112])</span><br><span class="line">Sequential output shape:  torch.Size([1, 128, 56, 56])</span><br><span class="line">Sequential output shape:  torch.Size([1, 256, 28, 28])</span><br><span class="line">Sequential output shape:  torch.Size([1, 512, 14, 14])</span><br><span class="line">Sequential output shape:  torch.Size([1, 512, 7, 7])</span><br><span class="line">Flatten output shape:  torch.Size([1, 25088])</span><br><span class="line">Linear output shape:  torch.Size([1, 4096])</span><br><span class="line">ReLU output shape:  torch.Size([1, 4096])</span><br><span class="line">Dropout output shape:  torch.Size([1, 4096])</span><br><span class="line">Linear output shape:  torch.Size([1, 4096])</span><br><span class="line">ReLU output shape:  torch.Size([1, 4096])</span><br><span class="line">Dropout output shape:  torch.Size([1, 4096])</span><br><span class="line">Linear output shape:  torch.Size([1, 10])</span><br><span class="line">training on cpu</span><br><span class="line">loss 0.178, train acc 0.934, test acc 0.923</span><br><span class="line">49.2 examples/sec on cpu</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/vgg.png" alt="image-20240928182512896"></p><h2 id="网络中的网络NIN">网络中的网络NIN</h2><p>原理及<strong>与VGG的比较</strong>见<a href="https://d2l.ai/chapter_convolutional-modern/nin.html#network-in-network-nin">8.3. Network in Network (NiN) — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络中的网络</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># NiN模型</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">        nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">        nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),  <span class="comment"># or (p=0.5)</span></span><br><span class="line">        <span class="comment"># 标签类别数是10</span></span><br><span class="line">        nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">        <span class="comment"># 将4维的输出转换成2维的输出，形状为(批量大小, 10)</span></span><br><span class="line">        nn.Flatten()  <span class="comment"># 展平</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查一下每个块的输出形状</span></span><br><span class="line">    X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">        X = layer(X)</span><br><span class="line">        <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&quot;output shape: &quot;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Sequential output shape:  torch.Size([1, 96, 54, 54])</span><br><span class="line">MaxPool2d output shape:  torch.Size([1, 96, 26, 26])</span><br><span class="line">Sequential output shape:  torch.Size([1, 256, 26, 26])</span><br><span class="line">MaxPool2d output shape:  torch.Size([1, 256, 12, 12])</span><br><span class="line">Sequential output shape:  torch.Size([1, 384, 12, 12])</span><br><span class="line">MaxPool2d output shape:  torch.Size([1, 384, 5, 5])</span><br><span class="line">Dropout output shape:  torch.Size([1, 384, 5, 5])</span><br><span class="line">Sequential output shape:  torch.Size([1, 10, 5, 5])</span><br><span class="line">AdaptiveAvgPool2d output shape:  torch.Size([1, 10, 1, 1])</span><br><span class="line">Flatten output shape:  torch.Size([1, 10])</span><br><span class="line">training on cpu</span><br><span class="line">loss 0.335, train acc 0.875, test acc 0.881</span><br><span class="line">42.1 examples/sec on cpu</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/nin.png" alt="image-20240928225442389"></p><h2 id="含并行连接的网络GoogLeNet">含并行连接的网络GoogLeNet</h2><p>GoogLenet的一个重要观点是：<strong>有时使用不同大小的卷积核组合是有利的</strong>。在GoogLeNet中，<strong>基本的卷积块被称为Inception块</strong>。一个Inception块的示例如下：</p><p><img src="%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/googlenet_inception.png" alt="image-20240928101112997"></p><p>这个Inception块由四条并行路径组成，<strong>前三条路径使用窗口大小为1×1、3×3和5×5的卷积层</strong>，从不同空间大小中提取信息。<strong>中间的两条路径在输入上执行1 × 1卷积</strong>，以减少通道数，从而降低模型的复杂性。<strong>第四条路径使用3 × 3最大汇聚层，然后使用1 × 1卷积层来改变通道数</strong>。这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的<strong>超参数是每层输出通道数</strong>。</p><p>现在我们来实现这样一个GoogLeNet：</p><p><img src="%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/googlenet.png" alt="image-20240928105540874"></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="comment"># c1-c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 路径1, 单1*1卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 路径2, 1*1卷积层后接3*3卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 路径3, 1*1卷积层后接5*5卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 路径4, 3*3最大池化层后接1*1卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 第1个分支：直接通过一个经过ReLU激活函数的卷积层</span></span><br><span class="line">        p1 = F.relu(<span class="variable language_">self</span>.p1_1(x))</span><br><span class="line">        <span class="comment"># 第2个分支：先通过一个ReLU激活函数的卷积层，再通过另一个</span></span><br><span class="line">        p2 = F.relu(<span class="variable language_">self</span>.p2_2(F.relu(<span class="variable language_">self</span>.p2_1(x))))</span><br><span class="line">        <span class="comment"># 第3个分支：与第2个分支相同，但使用不同的卷积层权重</span></span><br><span class="line">        p3 = F.relu(<span class="variable language_">self</span>.p3_2(F.relu(<span class="variable language_">self</span>.p3_1(x))))</span><br><span class="line">        <span class="comment"># 第4个分支：通过两个卷积层，但第二个层未使用激活函数，避免过度激活</span></span><br><span class="line">        p4 = F.relu(<span class="variable language_">self</span>.p4_2(<span class="variable language_">self</span>.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 逐一实现GoogLeNet的每个模块</span></span><br><span class="line">    b1 = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    b2 = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    b3 = nn.Sequential(</span><br><span class="line">        Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">        Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    b4 = nn.Sequential(</span><br><span class="line">        Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">        Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">        Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">        Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">        Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    b5 = nn.Sequential(</span><br><span class="line">        Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">        Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">        nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">        nn.Flatten()  <span class="comment"># 展平</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 创建GoogLeNet模型</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 由于GoogLeNet的计算非常复杂，所以适当降低输入的高和宽，提高效率</span></span><br><span class="line">    X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))  <span class="comment"># 从原来的224改为了96</span></span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">        X = layer(X)  <span class="comment"># 模仿神经网络按顺序经过每一层</span></span><br><span class="line">        <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&quot;output shape: &quot;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)  <span class="comment"># 降为96</span></span><br><span class="line">    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Sequential output shape:  torch.Size([1, 64, 24, 24])</span><br><span class="line">Sequential output shape:  torch.Size([1, 192, 12, 12])</span><br><span class="line">Sequential output shape:  torch.Size([1, 480, 6, 6])</span><br><span class="line">Sequential output shape:  torch.Size([1, 832, 3, 3])</span><br><span class="line">Sequential output shape:  torch.Size([1, 1024])</span><br><span class="line">Linear output shape:  torch.Size([1, 10])</span><br><span class="line">training on cpu</span><br><span class="line">loss 0.272, train acc 0.896, test acc 0.875</span><br><span class="line">126.8 examples/sec on cpu</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/google_net.png" alt="image-20240930152828950"></p><h2 id="批量规范化">批量规范化</h2><p>训练深层神经网络十分困难，特别是希望在短时间内使它们收敛。<strong>批量规范化</strong>是一种有效的技术，可以加速深层神经网络的收敛。</p><p>理论部分见<a href="https://d2l.ai/chapter_convolutional-modern/batch-norm.html#batch-normalization">8.5. Batch Normalization — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h3 id="从零实现批量规范化层">从零实现批量规范化层</h3><p>下面实现一个具有张量的批量规范化层。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量规范化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_tar, eps, momentum</span>):</span><br><span class="line">    <span class="comment"># 通过is_grad_enabled来判断是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        <span class="comment"># 如果在预测模式下，直接使用传入的移动平均所获得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_tar + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况, 计算通道维上(axis=1)的均值和方差</span></span><br><span class="line">            <span class="comment"># 保持X的形状以便做广播运算</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下，用当前均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_tar + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta</span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean.data, moving_var.data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># BatchNorm层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BatchNorm</span>(nn.Module):</span><br><span class="line">    <span class="comment"># num_features: 完全连接层的输出数量或卷积层的输出通道数</span></span><br><span class="line">    <span class="comment"># num_dims: 2表示完全连接层，4表示卷积层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, num_dims</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化为1和0</span></span><br><span class="line">        <span class="variable language_">self</span>.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        <span class="variable language_">self</span>.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># 非模型参数的变量初始化为0和1</span></span><br><span class="line">        <span class="variable language_">self</span>.moving_mean = torch.zeros(shape)</span><br><span class="line">        <span class="variable language_">self</span>.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var复制到X所在设备上</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.moving_mean.device != X.device:</span><br><span class="line">            <span class="variable language_">self</span>.moving_mean = <span class="variable language_">self</span>.moving_mean.to(X.device)</span><br><span class="line">            <span class="variable language_">self</span>.moving_var = <span class="variable language_">self</span>.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 保存更新过的moving_mean和moving_var</span></span><br><span class="line">        Y, <span class="variable language_">self</span>.moving_mean, <span class="variable language_">self</span>.moving_var = batch_norm(</span><br><span class="line">            X, <span class="variable language_">self</span>.gamma, <span class="variable language_">self</span>.beta, <span class="variable language_">self</span>.moving_mean, <span class="variable language_">self</span>.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 创建一个批量规范化层</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">        BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">        BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    lr, num_epochs, batch_size = <span class="number">1.0</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class="line">    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># TODO</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/tensor_norm.png" alt="image-20240930153701884"></p><h3 id="简洁实现的批量规范化层">简洁实现的批量规范化层</h3><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">120</span>),</span><br><span class="line">        nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">        nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line">    lr, num_epochs, batch_size = <span class="number">1.0</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">training on cpu</span><br><span class="line">loss 0.271, train acc 0.900, test acc 0.729</span><br><span class="line">23296.6 examples/sec on cpu</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/tensor_lib.png" alt="image-20240930160337183"></p><h2 id="残差网络ResNet">残差网络ResNet</h2><p>组件：</p><ul><li>残差块</li><li>ResNet模型</li></ul><p>原理见<a href="https://d2l.ai/chapter_convolutional-modern/resnet.html#function-classes">8.6. Residual Networks (ResNet) and ResNeXt — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a>（比较抽象）</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现残差块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels, use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            <span class="comment"># 如果使用1 * 1卷积层，添加通过1 * 1卷积调整通道和分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 不使用1 * 1卷积层，在应用ReLU函数之前，将输入添加到输出</span></span><br><span class="line">            <span class="variable language_">self</span>.conv3 = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        <span class="variable language_">self</span>.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(<span class="variable language_">self</span>.bn1(<span class="variable language_">self</span>.conv1(X)))</span><br><span class="line">        Y = <span class="variable language_">self</span>.bn2(<span class="variable language_">self</span>.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.conv3:</span><br><span class="line">            X = <span class="variable language_">self</span>.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ResNet使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。</span></span><br><span class="line"><span class="comment"># 第一个模块的通道数同输入通道数一致, 由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽</span></span><br><span class="line"><span class="comment"># 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半</span></span><br><span class="line"><span class="comment"># 下面实现这个模块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals, first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 当输入和输出形状一致时</span></span><br><span class="line">    blk = Residual(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    X = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line">    Y = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line"></span><br><span class="line">    blk = Residual(<span class="number">3</span>, <span class="number">6</span>, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(blk(X).shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7 × 7卷积层后，接步幅为2的3 × 3的最大汇聚层。</span></span><br><span class="line">    <span class="comment"># 不同之处在于ResNet每个卷积层后增加了批量规范化层</span></span><br><span class="line">    b1 = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">    b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">    b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">    b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        b1, b2, b3, b4, b5,</span><br><span class="line">        nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 观察不同模块的输入输入形状是如何变化的</span></span><br><span class="line">    X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">        X = layer(X)</span><br><span class="line">        <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&quot;output shape: &quot;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># TODO</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/resnet.png" alt="image-20240930172924144"></p><h2 id="稠密连接网络DenseNet">稠密连接网络DenseNet</h2><p>组件：</p><ul><li>稠密块体</li><li>过渡层</li><li>DenseNet模型</li></ul><p>原理见<a href="https://d2l.ai/chapter_convolutional-modern/densenet.html#from-resnet-to-densenet">8.7. Densely Connected Networks (DenseNet) — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用ResNet改良版的 批量规范化、激活和卷积 架构</span></span><br><span class="line"><span class="comment"># 实现该架构</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_block</span>(<span class="params">input_channels, num_channels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;批量规范化、激活和卷积架构&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现稠密快</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DenseBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_convs, input_channels, num_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        layer = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            layer.append(conv_block(num_channels * i + input_channels, num_channels))</span><br><span class="line">        <span class="variable language_">self</span>.net = nn.Sequential(*layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> <span class="variable language_">self</span>.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># 连接通道上每个块的输入和输出</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于每个稠密块都会带来通道数的增加，使用过多会复杂化模型，而过渡层可以用来控制模型复杂度</span></span><br><span class="line"><span class="comment"># 通过1 * 1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低复杂度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transition_block</span>(<span class="params">input_channels, num_channels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param input_channels: 输入通道数</span></span><br><span class="line"><span class="string">    :param num_channels: 通道数</span></span><br><span class="line"><span class="string">    :return: 过渡层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 创建稠密块</span></span><br><span class="line">    blk = DenseBlock(<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">    X = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">    Y = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)  <span class="comment"># 4, 23, 8, 8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对上述稠密块的输出使用通道数为10的过渡层, 高和宽减半</span></span><br><span class="line">    blk = transition_block(<span class="number">23</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(blk(Y).shape)  <span class="comment"># 4, 10, 4, 4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># DenseNet模型</span></span><br><span class="line">    b1 = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># num_channels为当前通道数</span></span><br><span class="line">    num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span></span><br><span class="line">    num_convs_in_dense_block = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">    blks = []</span><br><span class="line">    <span class="keyword">for</span> i, num_convs <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_convs_in_dense_block):</span><br><span class="line">        blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class="line">        <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">        num_channels += num_convs * growth_rate</span><br><span class="line">        <span class="comment"># 在稠密块之间添加一个转换层，使其通道数量减半</span></span><br><span class="line">        <span class="keyword">if</span> i != <span class="built_in">len</span>(num_convs_in_dense_block) - <span class="number">1</span>:</span><br><span class="line">            blks.append(transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">            num_channels //= <span class="number">2</span></span><br><span class="line">    <span class="comment"># 创建DenseNet模型，和ResNet类似，最后接上全局汇聚层和全连接层来输出结果</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        b1,</span><br><span class="line">        *blks,</span><br><span class="line">        nn.BatchNorm2d(num_channels),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">        nn.Flatten(),  <span class="comment"># 展平</span></span><br><span class="line">        nn.Linear(num_channels, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">    d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([4, 23, 8, 8])</span><br><span class="line">torch.Size([4, 10, 4, 4])</span><br><span class="line">training on cpu</span><br><span class="line">loss 0.143, train acc 0.947, test acc 0.906</span><br><span class="line">142.7 examples/sec on cpu</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/densenet.png" alt="image-20240930190254988"></p><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>现代循环神经网络</title>
      <link href="/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/ymhui.github.io/2024/09/23/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1>写在前面</h1><blockquote><p>参考书籍</p></blockquote><p><a href="http://www.d2l.ai">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><h1>现代循环神经网络</h1><p><strong>source code</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui/DeepLearning: Deep Learning with pytorch (github.com)</a></p><p><strong>use git to clone</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p><p><code>/ModernRNN</code></p><blockquote><p>GRU_self.pyGRU_lib.pyLSTM_self.pyLSTM_lib.pydeep_rnn_lib.pymachine_translation.pyencoder_decoder.py</p><p><a href="http://seq2seq.py">seq2seq.py</a></p></blockquote><h2 id="门控循环单元GRU">门控循环单元GRU</h2><p><strong>门控循环单元是长短期记忆LSTM的一个稍微简化的变体</strong>。我们从介绍它开始。</p><p>门控循环单元和普通的循环神经网络最大的区别在于：<strong>前者支持隐状态的门控</strong>。这意味着模型有专门的机制来确定应该何时更新隐状态，以及应该何时重置隐状态，并且这些机制是可学习的。</p><p>关于更新门、重置门、隐状态和候选隐状态，详见<a href="https://d2l.ai/chapter_recurrent-modern/gru.html#reset-gate-and-update-gate">10.2. Gated Recurrent Units (GRU) — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><p>下面我们着重介绍如何实现GRU。</p><h3 id="从零实现GRU">从零实现GRU</h3><p>我们依然使用《时光机器》数据集</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hidden, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            normal((num_inputs, num_hidden)),</span><br><span class="line">            normal((num_hidden, num_hidden)),</span><br><span class="line">            torch.zeros(num_hidden, device=device)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = three()  <span class="comment"># 候选隐状态参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hidden, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    <span class="comment"># 注：附加梯度是一种集成学习技术，通过将多个弱学习器组合到一起，逐步提高模型的预测性能</span></span><br><span class="line">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_gru_state</span>(<span class="params">batch_size, num_hidden, device</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.zeros((batch_size, num_hidden), device=device),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gru</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(R * H, W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练与预测</span></span><br><span class="line">    vocab_size, num_hidden, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">1</span>  <span class="comment"># 这些参数和之前一样</span></span><br><span class="line">    model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hidden, device, get_params, init_gru_state, gru)</span><br><span class="line">    d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line">    <span class="comment"># 可视化困惑度</span></span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">time traveller                                                  </span><br><span class="line">perplexity 1.2, 23127.7 tokens/sec on cpu</span><br><span class="line">time traveller but now you begin this wo legh wime yo u gan a ju</span><br><span class="line">travelleryou can show ble i have been at work upon thisgeom</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/gru_ppl.png" alt="image-20240923235435882"></p><h3 id="简洁实现的GRU">简洁实现的GRU</h3><p>深度学习框架中的API包含了前面介绍的所有细节，我们可以直接实例化门控循环单元模型。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line">    vocab_size, device = <span class="built_in">len</span>(vocab), d2l.try_gpu()</span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    num_inputs, num_hidden = vocab_size, <span class="number">256</span></span><br><span class="line">    gru_layer = nn.GRU(num_inputs, num_hidden)  <span class="comment"># 实例化GRU </span></span><br><span class="line">    model = d2l.RNNModel(gru_layer, vocab_size)</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perplexity 1.0, 16676.1 tokens/sec on cpu</span><br><span class="line">time traveller for so it will be convenient to speak of himwas e</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/gru_lib.png" alt="image-20240924091819458"></p><h2 id="长短期记忆网络LSTM">长短期记忆网络LSTM</h2><p>长期以来，<strong>隐变量模型存在着长期信息保存和短期输入缺失的问题</strong>，首次解决这一问题使用的是<strong>长短期记忆网络</strong>模型。</p><p>关于输入门、忘记门、输出门、候选记忆元、记忆元和隐状态，详见<a href="https://d2l.ai/chapter_recurrent-modern/lstm.html#long-short-term-memory-lstm">门控记忆元</a></p><h3 id="从零实现LSTM">从零实现LSTM</h3><p>依然使用时光机器数据集。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_lstm_params</span>(<span class="params">vocab_size, num_hidden, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            normal((num_inputs, num_hidden)),</span><br><span class="line">            normal((num_hidden, num_hidden)),</span><br><span class="line">            torch.zeros(num_hidden, device=device)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xi, W_hi, b_i = three()</span><br><span class="line">    <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = three()</span><br><span class="line">    <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = three()</span><br><span class="line">    <span class="comment"># 候选记忆元参数</span></span><br><span class="line">    W_xc, W_hc, b_c = three()</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hidden, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_lstm_state</span>(<span class="params">batch_size, num_hidden, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;初始化函数：LSTM的隐状态需要返回一个额外的记忆元，其单元值为0，形状为(批量大小，隐藏单元数)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hidden), device=device),</span><br><span class="line">            torch.zeros((batch_size, num_hidden), device=device))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处实际模型的定义与GRU格式类似</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lstm</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练和预测</span></span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">    vocab_size, num_hidden, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">    model = d2l.RNNModelScratch(vocab_size, num_hidden, device, get_lstm_params, init_lstm_state, lstm)</span><br><span class="line">    d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line">    <span class="comment"># 可视化困惑度</span></span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perplexity 1.1, 10388.7 tokens/sec on cpu</span><br><span class="line">time travellerit wollareftrev ssich aly lemesyou back an whree a</span><br><span class="line">travellerbyuccouco bain the psychologistyes so it seemed to</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/lstm_self.png" alt="image-20240924104204139"></p><h3 id="简洁实现的LSTM">简洁实现的LSTM</h3><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line">    vocab_size, num_hidden, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">    num_inputs = vocab_size</span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">    lstm_layer = nn.LSTM(num_inputs, num_hidden)</span><br><span class="line">    model = d2l.RNNModel(lstm_layer, vocab_size)</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perplexity 1.0, 10847.3 tokens/sec on cpu</span><br><span class="line">time traveller for so it will be convenient to speak of himwas e</span><br><span class="line">traveller with a slight accession ofcheerfulness really thi</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/lstm_lib.png" alt="image-20240924110246905"></p><h2 id="深度循环神经网络">深度循环神经网络</h2><p>到目前为止，我们只讨论了具有一个单向隐藏层的循环神经网络，事实上，<strong>我们可以将多层循环神经网络叠在一起</strong>。</p><p>理论部分详见<a href="https://d2l.ai/chapter_recurrent-modern/deep-rnn.html#deep-recurrent-neural-networks">10.3. Deep Recurrent Neural Networks — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h3 id="简洁实现的深度循环神经网络">简洁实现的深度循环神经网络</h3><p>现有的API已经实现了该模型中的所有逻辑细节，方便起见，我们直接介绍简洁版本。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">    vocab_size, num_hidden, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">    num_inputs = vocab_size</span><br><span class="line">    num_epochs, lr, device = <span class="number">500</span>, <span class="number">2</span>, d2l.try_gpu()</span><br><span class="line">    lstm_layer = nn.LSTM(num_inputs, num_hidden, num_layers)</span><br><span class="line">    model = d2l.RNNModel(lstm_layer, vocab_size)</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    d2l.train_ch8(model, train_iter, vocab, lr * <span class="number">1.0</span>, num_epochs, device)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perplexity 1.0, 8220.6 tokens/sec on cpu</span><br><span class="line">time travelleryou can show black is white by argument said filby</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/deep_rnn.png" alt="image-20240924123403050"></p><h2 id="双向循环神经网络">双向循环神经网络</h2><p>基本理论详见<a href="https://d2l.ai/chapter_recurrent-modern/bi-rnn.html#bidirectional-recurrent-neural-networks">10.4. Bidirectional Recurrent Neural Networks — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="机器翻译与数据集">机器翻译与数据集</h2><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_data_nmt</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;载入\&quot;英语-法语\&quot;数据集&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;fra-eng&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;fra.txt&#x27;</span>), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> f.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_nmt</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对原始数据做一些预处理，如将不间断空格替换为一个空格，小写替换大写，单词和标点之间插入空格&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">no_space</span>(<span class="params">char, prev_char</span>):</span><br><span class="line">        <span class="keyword">return</span> char <span class="keyword">in</span> <span class="built_in">set</span>(<span class="string">&#x27;,.!?&#x27;</span>) <span class="keyword">and</span> prev_char != <span class="string">&#x27; &#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用空格替换不间断空格</span></span><br><span class="line">    <span class="comment"># 使用小写替换大写</span></span><br><span class="line">    text = text.replace(<span class="string">&#x27;\u202f&#x27;</span>, <span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27; &#x27;</span>).lower()</span><br><span class="line">    <span class="comment"># 在单词和标点符号之间插入空格</span></span><br><span class="line">    out = [<span class="string">&#x27; &#x27;</span> + char <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> no_space(char, text[i - <span class="number">1</span>]) <span class="keyword">else</span></span><br><span class="line">           char <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(text)]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词元化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_nmt</span>(<span class="params">text, num_examples=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;词元化\&quot;英语-法语\&quot;数据集&quot;&quot;&quot;</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.split(<span class="string">&#x27;\n&#x27;</span>)):</span><br><span class="line">        <span class="keyword">if</span> num_examples <span class="keyword">and</span> i &gt; num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) == <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> source, target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制每个文本序列所包含的词元数量的直方图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_list_len_pair_hist</span>(<span class="params">legend, xlabel, ylabel, xlist, ylist</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制列表长度对的直方图&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 使用原生pyplot 绘制直方图</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line">    _, _, patches = plt.hist([[<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> xlist], [<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> ylist]])</span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.ylabel(ylabel)</span><br><span class="line">    <span class="keyword">for</span> patch <span class="keyword">in</span> patches[<span class="number">1</span>].patches:</span><br><span class="line">        patch.set_hatch(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">    plt.legend(legend)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]  <span class="comment"># 截断</span></span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))  <span class="comment"># 填充</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将机器翻译的文本序列转换成小批量&quot;&quot;&quot;</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span>  l <span class="keyword">in</span> lines]</span><br><span class="line">    lines = [l + [vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">    ) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).<span class="built_in">type</span>(torch.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    reserved_tokens = [<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>]</span><br><span class="line">    <span class="comment"># 从文件中读取原始数据，并进行预处理</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    <span class="comment"># 对预处理后的数据进行分词，并按需限制样本数量</span></span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    <span class="comment"># 构建源语言和目标语言的词表，最小频率设为2</span></span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>, reserved_tokens=reserved_tokens)</span><br><span class="line">    target_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>, reserved_tokens=reserved_tokens)</span><br><span class="line">    <span class="comment"># 将分词后的文本序列转换为索引数组和有效长度数组</span></span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    target_array, target_valid_len = build_array_nmt(target, target_vocab, num_steps)</span><br><span class="line">    <span class="comment"># 组合所有数据数组，以便加载到迭代器中</span></span><br><span class="line">    data_arrays = [src_array, src_valid_len, target_array, target_valid_len]</span><br><span class="line">    <span class="comment"># 创建并返回数据迭代器，以及源语言和目标语言的词表</span></span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, target_vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;fra-eng&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;fra-eng.zip&#x27;</span>,</span><br><span class="line">                               <span class="string">&#x27;94646ad1522d915e7b0f9296181140edcf86a4f5&#x27;</span>)</span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    raw_text = read_data_nmt()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;raw data:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(raw_text[:<span class="number">75</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据预处理</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;after preprocessing:&quot;</span>)</span><br><span class="line">    text = preprocess_nmt(raw_text)</span><br><span class="line">    <span class="built_in">print</span>(text[:<span class="number">75</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词元化</span></span><br><span class="line">    source, target = tokenize_nmt(text)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;after tokenizing:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(source[:<span class="number">6</span>])</span><br><span class="line">    <span class="built_in">print</span>(target[:<span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">    show_list_len_pair_hist([<span class="string">&#x27;source&#x27;</span>, <span class="string">&#x27;target&#x27;</span>], <span class="string">&#x27;# tokens per sequence&#x27;</span>, <span class="string">&#x27;count&#x27;</span>, source, target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词表</span></span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>, reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bcs&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;vocab size:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(src_vocab))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 截断或填充文本</span></span><br><span class="line">    <span class="built_in">print</span>(truncate_pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载迭代器和词表</span></span><br><span class="line">    train_iter, src_vocab, target_vocab = load_data_nmt(batch_size=<span class="number">2</span>, num_steps=<span class="number">8</span>)</span><br><span class="line">    <span class="comment"># 可视化一部分数据</span></span><br><span class="line">    <span class="keyword">for</span> X, X_valid_len, Y, Y_valid_len <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;X:&#x27;</span>, X.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;X的有效长度:&#x27;</span>, X_valid_len)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Y:&#x27;</span>, Y.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Y的有效长度:&#x27;</span>, Y_valid_len)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">raw data:</span><br><span class="line">Go.Va !</span><br><span class="line">Hi.Salut !</span><br><span class="line">Run!Cours !</span><br><span class="line">Run!Courez !</span><br><span class="line">Who?Qui ?</span><br><span class="line">Wow!Ça alors !</span><br><span class="line"></span><br><span class="line">after preprocessing:</span><br><span class="line">go .va !</span><br><span class="line">hi .salut !</span><br><span class="line">run !cours !</span><br><span class="line">run !courez !</span><br><span class="line">who ?qui ?</span><br><span class="line">wow !ça al</span><br><span class="line">after tokenizing:</span><br><span class="line">[[&#x27;go&#x27;, &#x27;.&#x27;], [&#x27;hi&#x27;, &#x27;.&#x27;], [&#x27;run&#x27;, &#x27;!&#x27;], [&#x27;run&#x27;, &#x27;!&#x27;], [&#x27;who&#x27;, &#x27;?&#x27;], [&#x27;wow&#x27;, &#x27;!&#x27;]]</span><br><span class="line">[[&#x27;va&#x27;, &#x27;!&#x27;], [&#x27;salut&#x27;, &#x27;!&#x27;], [&#x27;cours&#x27;, &#x27;!&#x27;], [&#x27;courez&#x27;, &#x27;!&#x27;], [&#x27;qui&#x27;, &#x27;?&#x27;], [&#x27;ça&#x27;, &#x27;alors&#x27;, &#x27;!&#x27;]]</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/vocab_freq.png" alt="image-20240924172311048"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vocab size:</span><br><span class="line">10012</span><br><span class="line">[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]</span><br><span class="line">X: tensor([[ 9, 28,  4,  3,  1,  1,  1,  1],</span><br><span class="line">        [16, 51,  4,  3,  1,  1,  1,  1]], dtype=torch.int32)</span><br><span class="line">X的有效长度: tensor([4, 4])</span><br><span class="line">Y: tensor([[73,  0,  4,  3,  1,  1,  1,  1],</span><br><span class="line">        [35, 53,  5,  3,  1,  1,  1,  1]], dtype=torch.int32)</span><br><span class="line">Y的有效长度: tensor([4, 4])</span><br></pre></td></tr></table></figure><h1>编码器-解码器架构</h1><p>接着上面的内容来说，机器翻译是序列转换模型中的一个关键问题，其困难点主要在于输入与输出序列都是可变长的。为了解决这个问题，我们设计一个全新的架构：<strong>编码器-解码器</strong>。在这个架构中，<strong>编码器负责把变长的输入序列转化为具有固定形状的编码状态，而解码器负责把这个固定形状的编码状态再转化为变长的输出序列。</strong></p><h2 id="接口">接口</h2><p>声明几个接口</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><br><span class="line">        enc_outputs = <span class="variable language_">self</span>.encoder(enc_X, *args)</span><br><span class="line">        dec_state = <span class="variable language_">self</span>.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure><h2 id="序列到序列学习seq2seq">序列到序列学习seq2seq</h2><p>本部分我们将使用两个循环神经网络的编码器和解码器，并将其应用于序列到序列类的学习任务</p><p>理论部分详见<a href="https://d2l.ai/chapter_recurrent-modern/seq2seq.html#sequence-to-sequence-learning-for-machine-translation">10.7. Sequence-to-Sequence Learning for Machine Translation — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> encoder_decoder <span class="keyword">import</span> Encoder</span><br><span class="line"><span class="keyword">from</span> encoder_decoder <span class="keyword">import</span> Decoder</span><br><span class="line"><span class="keyword">from</span> encoder_decoder <span class="keyword">import</span> EncoderDecoder</span><br><span class="line"><span class="keyword">from</span> machine_translation <span class="keyword">import</span> load_data_nmt</span><br><span class="line"><span class="keyword">from</span> RNN.rnn_self <span class="keyword">import</span> grad_clipping</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqEncoder</span>(<span class="title class_ inherited__">Encoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hidden, num_layers, dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqEncoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 嵌入层</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.GRU(embed_size, num_hidden, num_layers, dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="comment"># X的形状：(batch_size, num_steps, embed_size_</span></span><br><span class="line">        X = <span class="variable language_">self</span>.embedding(X)</span><br><span class="line">        <span class="comment"># 在循环神经网络模型中，第一个轴对应于时间步</span></span><br><span class="line">        X = X.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 若未提及状态，默认0</span></span><br><span class="line">        output, state = <span class="variable language_">self</span>.rnn(X)</span><br><span class="line">        <span class="comment"># output的形状:(num_steps, batch_size, num_hidden)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers, batch_size, num_hidden)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqDecoder</span>(<span class="title class_ inherited__">Decoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hidden, num_layer, dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 定义词汇嵌入层，将词汇ID转换为词嵌入向量</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="comment"># 定义GRU网络，用于处理序列数据</span></span><br><span class="line">        <span class="comment"># 输入维度为词嵌入向量维度embed_size与隐藏层单元数num_hidden之和</span></span><br><span class="line">        <span class="comment"># 隐藏层单元数为num_hidden，用于捕捉序列中的长期依赖关系</span></span><br><span class="line">        <span class="comment"># 设置多层GRU，num_layer表示GRU的层数</span></span><br><span class="line">        <span class="comment"># 添加dropout，用于在训练过程中防止过拟合</span></span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.GRU(embed_size + num_hidden, num_hidden, num_layer, dropout=dropout)</span><br><span class="line">        <span class="comment"># 定义全连接层，将GRU的输出转换为词汇大小的预测值</span></span><br><span class="line">        <span class="comment"># 输入维度为GRU的隐藏层单元数num_hidden，输出维度为词汇表大小vocab_size</span></span><br><span class="line">        <span class="variable language_">self</span>.dense = nn.Linear(num_hidden, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param enc_outputs: 编码器的输出</span></span><br><span class="line"><span class="string">        :param args: 其余参数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># X的形状：(batch_size, num_steps, embed_size)</span></span><br><span class="line">        X = <span class="variable language_">self</span>.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 广播context，使其具有与X相同的num_steps</span></span><br><span class="line">        context = state[-<span class="number">1</span>].repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        output, state = <span class="variable language_">self</span>.rnn(X_and_context, state)</span><br><span class="line">        output = <span class="variable language_">self</span>.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># output的形状:(batch_size, num_steps, vocab_size)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers, batch_size, num_hidden)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面将通过计算交叉熵损失函数来进行优化</span></span><br><span class="line"><span class="comment"># 首先定义一个遮蔽函数通过零值化来屏蔽不相关预测</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sequence_mask</span>(<span class="params">X, valid_len, value=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;屏蔽序列中不相关的项&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取当前批次中序列的最大长度</span></span><br><span class="line">    max_len = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 创建一个形状为(batch_size, max_len)的掩码，其中valid_len对应位置为True，其余为False</span></span><br><span class="line">    mask = torch.arange((max_len), dtype=torch.float32, device=X.device)[<span class="literal">None</span>, :] &lt; valid_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment"># 将不符合掩码条件的元素替换为指定的value</span></span><br><span class="line">    X[~mask] = value</span><br><span class="line">    <span class="comment"># 返回应用掩码后的序列数据</span></span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaskedSoftmaxCELoss</span>(nn.CrossEntropyLoss):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># pred形状：(batch_size, num_steps, vocab_size)</span></span><br><span class="line">    <span class="comment"># label形状：(batch_size, num_steps)</span></span><br><span class="line">    <span class="comment"># valid_length形状：(batch_size,)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pred, label, valid_len</span>):</span><br><span class="line">        <span class="comment"># 初始化与标签形状相同的权重张量，初始权重都为1</span></span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        <span class="comment"># 应用sequence_mask以根据有效长度对权重进行掩码，超出有效长度的部分权重设为0</span></span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        <span class="comment"># 设置reduction参数为&#x27;none&#x27;，确保损失函数为每个元素返回一个未减少的损失值</span></span><br><span class="line">        <span class="variable language_">self</span>.reduction = <span class="string">&#x27;none&#x27;</span></span><br><span class="line">        <span class="comment"># 调用父类的forward方法计算未加权的损失，调整预测数据的维度以适应损失函数的要求</span></span><br><span class="line">        unweighted_loss = <span class="built_in">super</span>(MaskedSoftmaxCELoss, <span class="variable language_">self</span>).forward(pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>), label)</span><br><span class="line">        <span class="comment"># 将未加权的损失与掩码权重相乘，然后在序列维度上计算加权损失的平均值</span></span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> weighted_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练序列到序列学习模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_seq2seq</span>(<span class="params">net, train_iter, lr, num_epochs, target_vocab, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">xavier_init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.GRU:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> m._flat_weights_names:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;weight&quot;</span> <span class="keyword">in</span> param:</span><br><span class="line">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line">    net.apply(xavier_init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    loas = MaskedSoftmaxCELoss()</span><br><span class="line">    net.train()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer = d2l.Timer()</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> train_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            bos = torch.tensor([target_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class="number">0</span>], device=device).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            dec_input = torch.cat([bos, Y[:, :-<span class="number">1</span>]], <span class="number">1</span>)  <span class="comment"># 强制教学</span></span><br><span class="line">            Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class="line">            l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            <span class="comment"># 这里不做l.sum()会报RuntimeError: Boolean value of Tensor with more than one value is ambiguous</span></span><br><span class="line">            grad_clipping(net, l.<span class="built_in">sum</span>())</span><br><span class="line">            num_tokens = Y_valid_len.<span class="built_in">sum</span>()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l.<span class="built_in">sum</span>(), num_tokens)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>], ))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop()&#125;</span> tokens / sec on <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化损失曲线</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line"><span class="comment"># 为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都来自前一时间步的预测词元</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_seq2seq</span>(<span class="params">net, src_sentence, src_vocab, target_vocab, num_steps, device, save_attention_weights=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span></span><br><span class="line">    net.<span class="built_in">eval</span>()  <span class="comment"># 评估模式</span></span><br><span class="line">    <span class="comment"># 将源句子转换为词元序列，并添加序列结束符</span></span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">&#x27; &#x27;</span>)] + [src_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class="line">    <span class="comment"># 记录有效长度，用于处理padding</span></span><br><span class="line">    end_valid_len = torch.tensor([<span class="built_in">len</span>(src_tokens)], device=device)</span><br><span class="line">    <span class="comment"># 确保序列长度不超过num_steps，不足则填充</span></span><br><span class="line">    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    enc_X = torch.unsqueeze(</span><br><span class="line">        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 通过编码器编码源句子</span></span><br><span class="line">    enc_outputs = net.encoder(enc_X, end_valid_len)</span><br><span class="line">    <span class="comment"># 初始化解码器状态</span></span><br><span class="line">    dec_state = net.decoder.init_state(enc_outputs, end_valid_len)</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    <span class="comment"># 解码器的输入开始于开始符号</span></span><br><span class="line">    dec_X = torch.unsqueeze(</span><br><span class="line">        torch.tensor([target_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), dim=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    output_seq, attention_weight_seq = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        <span class="comment"># 使用解码器生成下一个词元</span></span><br><span class="line">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># 使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 挤压批量轴，获取预测的词元ID</span></span><br><span class="line">        pred = dec_X.squeeze(dim=<span class="number">0</span>).<span class="built_in">type</span>(torch.int32).item()</span><br><span class="line">        <span class="comment"># 保存注意力权重</span></span><br><span class="line">        <span class="keyword">if</span> save_attention_weights:</span><br><span class="line">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class="line">        <span class="comment"># 若序列结束词元被预测，输出序列的生成就完成了</span></span><br><span class="line">        <span class="keyword">if</span> pred == target_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 累积预测的词元序列</span></span><br><span class="line">        output_seq.append(pred)</span><br><span class="line">    <span class="comment"># 将词元ID序列转换为目标句子</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(target_vocab.to_tokens(output_seq)), attention_weight_seq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测序列的评估</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bleu</span>(<span class="params">pred_seq, label_seq, k</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将序列分割成词 token</span></span><br><span class="line">    pred_tokens, label_tokens = pred_seq.split(<span class="string">&#x27; &#x27;</span>), label_seq.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    <span class="comment"># 计算预测序列和标签序列的长度</span></span><br><span class="line">    len_pred, len_label = <span class="built_in">len</span>(pred_tokens), <span class="built_in">len</span>(label_tokens)</span><br><span class="line">    <span class="comment"># 计算精度的初步分数部分</span></span><br><span class="line">    score = math.exp(<span class="built_in">min</span>(<span class="number">0</span>, <span class="number">1</span> - len_label / len_pred))</span><br><span class="line">    <span class="comment"># 遍历不同的n-gram长度</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 初始化匹配数量和标签子序列的字典</span></span><br><span class="line">        num_matches, label_subs = <span class="number">0</span>, collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="comment"># 构建标签子序列的n-gram并计数</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_label - n + <span class="number">1</span>):</span><br><span class="line">            label_subs[<span class="string">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 在预测序列中查找匹配的n-gram</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_pred - n + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 如果在标签序列中找到匹配，则增加匹配数量</span></span><br><span class="line">            <span class="keyword">if</span> label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class="number">0</span>:</span><br><span class="line">                num_matches += <span class="number">1</span></span><br><span class="line">                label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 根据匹配数量和n-gram长度更新分数</span></span><br><span class="line">        score *= math.<span class="built_in">pow</span>(num_matches / (len_pred - n + <span class="number">1</span>), math.<span class="built_in">pow</span>(<span class="number">0.5</span>, n))</span><br><span class="line">    <span class="comment"># 返回最终的BLEU分数</span></span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 实例化一个Seq2SeqEncoder对象，用于编码序列</span></span><br><span class="line">    <span class="comment"># 参数说明：</span></span><br><span class="line">    <span class="comment"># vocab_size: 词汇表大小，表示输入数据的唯一词汇数量</span></span><br><span class="line">    <span class="comment"># embed_size: 嵌入层大小，表示将词汇嵌入到多少维度的向量空间</span></span><br><span class="line">    <span class="comment"># num_hidden: 隐藏层单元数量，决定了模型的复杂度</span></span><br><span class="line">    <span class="comment"># num_layers: RNN的层数，多层可以提高模型的表达能力</span></span><br><span class="line">    encoder = Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hidden=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line">    encoder.<span class="built_in">eval</span>()  <span class="comment"># 评估模式</span></span><br><span class="line">    X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">    output, state = encoder(X)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;encoder:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;output shape:&#x27;</span>, output.shape, <span class="string">&#x27;state shape:&#x27;</span>, state.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用与上面编码器相同的超参数来实例化解码器</span></span><br><span class="line">    decoder = Seq2SeqDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hidden=<span class="number">16</span>, num_layer=<span class="number">2</span>)</span><br><span class="line">    decoder.<span class="built_in">eval</span>()</span><br><span class="line">    state = decoder.init_state(encoder(X))</span><br><span class="line">    output, state = decoder(X, state)  <span class="comment"># 获得输出，并更新state</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;decoder:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;output shape:&#x27;</span>, output.shape, <span class="string">&#x27;state shape:&#x27;</span>, state.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义损失函数</span></span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;loss demo:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(loss(torch.ones(<span class="number">3</span>, <span class="number">4</span>, <span class="number">10</span>), torch.ones((<span class="number">3</span>, <span class="number">4</span>), dtype=torch.long), torch.tensor([<span class="number">4</span>, <span class="number">2</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 现在在机器翻译数据集上，我们可以创建和训练一个循环神经网络“编码器-解码器”模型用于序列到序列的学习</span></span><br><span class="line">    embed_size , num_hidden, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">    batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">    lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line">    train_iter, src_vocab, target_vocab = load_data_nmt(batch_size, num_steps)  <span class="comment"># 用d2l的库会报编码错误</span></span><br><span class="line">    encoder = Seq2SeqEncoder(<span class="built_in">len</span>(src_vocab), embed_size, num_hidden, num_layers, dropout)</span><br><span class="line">    decoder = Seq2SeqDecoder(<span class="built_in">len</span>(target_vocab), embed_size, num_hidden, num_layers, dropout)</span><br><span class="line">    net = EncoderDecoder(encoder, decoder)</span><br><span class="line">    train_seq2seq(net, train_iter, lr, num_epochs, target_vocab, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 利用训练好的“编码器-解码器”模型，将几个英语句子翻译为法语，并计算BLEU最终结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;start translating:&quot;</span>)</span><br><span class="line">    engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&#x27;i lost .&#x27;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">    fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">        translation, attention_weight_seq = predict_seq2seq(net, eng, src_vocab, target_vocab, num_steps, device)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, bleu <span class="subst">&#123;bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">encoder:</span><br><span class="line">output shape: torch.Size([7, 4, 16]) state shape: torch.Size([2, 4, 16])</span><br><span class="line">decoder:</span><br><span class="line">output shape: torch.Size([4, 7, 10]) state shape: torch.Size([2, 4, 16])</span><br><span class="line">loss demo:</span><br><span class="line">tensor([2.3026, 1.1513, 0.0000])</span><br><span class="line">loss 0.01964012612887416, 5921.959299792854 tokens / sec on cpu</span><br></pre></td></tr></table></figure><p><img src="%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/loss_seq2seq.png" alt="image-20240925102613044"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start translating:</span><br><span class="line">go . =&gt; va !, bleu 1.000</span><br><span class="line">i lost . =&gt; j&#x27;ai perdu ., bleu 1.000</span><br><span class="line">he&#x27;s calm . =&gt; il est tom bon ?, bleu 0.447</span><br><span class="line">i&#x27;m home . =&gt; je suis &lt;unk&gt; ., bleu 0.512</span><br></pre></td></tr></table></figure><h1>束搜索</h1><p>在正式介绍束搜索之前，先介绍一下贪心搜索，并探讨其存在的问题。</p><p>本节主要以理论知识为主。</p><h2 id="贪心搜索">贪心搜索</h2><p>理论部分详见<a href="https://d2l.ai/chapter_recurrent-modern/beam-search.html#greedy-search">10.8. Greedy Search — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="穷举搜索">穷举搜索</h2><p>理论部分详见<a href="https://d2l.ai/chapter_recurrent-modern/beam-search.html#exhaustive-search">10.8. Exhaustive Search — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="束搜索">束搜索</h2><p>理论部分详见<a href="https://d2l.ai/chapter_recurrent-modern/beam-search.html#id1">10.8. Beam Search — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贪心</title>
      <link href="/ymhui.github.io/2024/09/20/%E8%B4%AA%E5%BF%83/"/>
      <url>/ymhui.github.io/2024/09/20/%E8%B4%AA%E5%BF%83/</url>
      
        <content type="html"><![CDATA[<h1>贪心算法（含动态规划思想）</h1><p>贪心算法是一种局部最优选择策略。它通过在每一步都选择当前状态下的最优解，试图最终得到全局最优解; 动态规划通过将问题分解为 <strong>子问题</strong>，逐步求解子问题的最优解，最终得到整个问题的最优解。它利用子问题的重叠性和最优子结构性质。</p><h2 id="核心">核心</h2><p>贪心算法的核心思想是<strong>总在每一步抉择中挑选出当前最大/最小（即最值）对应的情况进行处理</strong>，从而达到最好/最坏的情况，满足贪心的目的</p><h2 id="例题">例题</h2><h3 id="通配符匹配">通配符匹配</h3><h4 id="题目">题目</h4><p>给你一个输入字符串 (<code>s</code>) 和一个字符模式 (<code>p</code>) ，请你实现一个支持 <code>'?'</code> 和 <code>'*'</code> 匹配规则的通配符匹配：</p><ul><li><code>'?'</code> 可以匹配任何单个字符。</li><li><code>'*'</code> 可以匹配任意字符序列（包括空字符序列）。</li></ul><p>判定匹配成功的充要条件是：字符模式必须能够 <strong>完全匹配</strong> 输入字符串（而不是部分匹配）。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：s = &quot;aa&quot;, p = &quot;a&quot;</span><br><span class="line">输出：false</span><br><span class="line">解释：&quot;a&quot; 无法匹配 &quot;aa&quot; 整个字符串。</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：s = &quot;aa&quot;, p = &quot;*&quot;</span><br><span class="line">输出：true</span><br><span class="line">解释：&#x27;*&#x27; 可以匹配任意字符串。</span><br></pre></td></tr></table></figure><p><strong>示例 3：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：s = &quot;cb&quot;, p = &quot;?a&quot;</span><br><span class="line">输出：false</span><br><span class="line">解释：&#x27;?&#x27; 可以匹配 &#x27;c&#x27;, 但第二个 &#x27;a&#x27; 无法匹配 &#x27;b&#x27;。</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>0 &lt;= s.length, p.length &lt;= 2000</code></li><li><code>s</code> 仅由小写英文字母组成</li><li><code>p</code> 仅由小写英文字母、<code>'?'</code> 或 <code>'*'</code> 组成</li></ul><h4 id="分析">分析</h4><p>暴力枚举的做法在此题是不可想象的，而暴力递归进行枚举贪心的做法也会因为复杂度过高而超时，因此<strong>必须结合动态规划的思想进行贪心优化</strong>。这个匹配规则类似于正则匹配，考虑用分治的思想进行动态规划：对于输入串<code>s</code>和匹配规则<code>p</code>，若对于串<code>s</code>的某个前缀<code>s'</code>可以和<code>p</code>的某个前缀<code>p'</code>匹配，则只要用相同的方法验证剩余的内容是否匹配即可（动态规划），对于不同的匹配规则采取不同的匹配策略，期望一次匹配掉所有可能的情况来保证后续判断的准确（贪心）；匹配规则：</p><p><code>alpha</code>: 匹配指定字符</p><p><code>?</code>: 匹配单个任意字符</p><p><code>*</code>: 匹配任意串，包括空串</p><p>为了使用动态规划，我们考虑建立一个由<code>s</code>长度和<code>p</code>长度共同组成的一个二维数组<code>dp</code>代表我们的迭代更新记录，下面尝试寻找并建立状态转移函数。<strong>想象一个棋盘，这个棋盘的行索引是输入串的每个字符，列索引是匹配规则的每个字符</strong>，不过<strong>最上面一行和最左边一列留空</strong>(为了留出起点且防止后续操作越界)，我们在这个<strong>棋盘上移动</strong>，根据特定的匹配规则和输入字符，判断能不能<strong>从左上角（起点），走到右下角（终点）</strong>，因此：</p><p>对于某个位置，如果<strong>可达，则记为true</strong>，反之为false，<strong>只能从记录为true的格子出发</strong>。<strong>初始时置最左上角即起点的格子为true</strong>，从这个位置开始移动。对于不同的通配符匹配规则，我们将其转化为对应的移动规则：</p><p><code>alpha</code>: 如果当前读入一个字母通配符，则查看左边一列所有标记为true的格子，试者从这些格子往右下方走一步，如果<strong>右下方格子的行索引和读入的通配符一致</strong></p><p><code>?</code>: 同上查看规则，但在判断移动时不需要一致，<strong>只要左边一列的格子为true，就往右下方格子走一步</strong></p><p><code>*</code>: 因为’*'可以匹配所有读入字符，所以只要<strong>在左边一列从上往下找到第一个标记为true的格子</strong>，那么从这个格子开始<strong>先往右走一步</strong>（*可以匹配空串），之后从这个格子开始往下的每个格子都能走到（因为*可以匹配任意字符，有啥都能匹配，因此在自己这个位置来多少匹配多少），<strong>即左边一列找到第一个为true的行，此列中所有该行之下（含该行）的格子都标记为true。</strong></p><p>最后查看最右下角的格子是否标记为true就知道是否匹配。</p><p>示例:</p><table><thead><tr><th></th><th>0</th><th>a</th><th>*</th><th>?</th><th>e</th></tr></thead><tbody><tr><td>0</td><td>start(true)</td><td>留空(false)</td><td>留空(false)</td><td>留空(false)</td><td>留空(false)</td></tr><tr><td>a</td><td>留空(false)</td><td>true</td><td>true</td><td></td><td></td></tr><tr><td>b</td><td>留空(false)</td><td></td><td>true</td><td>true</td><td></td></tr><tr><td>c</td><td>留空(false)</td><td></td><td>true</td><td>true</td><td></td></tr><tr><td>d</td><td>留空(false)</td><td></td><td>true</td><td>true</td><td></td></tr><tr><td>e</td><td>留空(false)</td><td></td><td>true</td><td>true</td><td>true</td></tr></tbody></table><p>代码如下</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> Optimize (<span class="string">&quot;Ofast&quot;</span>)</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isMatch</span><span class="params">(string s, string p)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// s是输入串，p是匹配规则</span></span><br><span class="line">        <span class="type">int</span> m = s.<span class="built_in">size</span>(), n = p.<span class="built_in">size</span>();</span><br><span class="line">        vector&lt;vector&lt;<span class="type">bool</span>&gt;&gt; dp;</span><br><span class="line">        dp.<span class="built_in">resize</span>(m + <span class="number">1</span>, <span class="built_in">vector</span>&lt;<span class="type">bool</span>&gt;(n + <span class="number">1</span>, <span class="literal">false</span>)); <span class="comment">// dp[i][j] 表示前i子串和前j通配符匹配</span></span><br><span class="line">        <span class="keyword">if</span> (s.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span> c : p) &#123;</span><br><span class="line">                <span class="keyword">if</span> (c != <span class="string">&#x27;*&#x27;</span>) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (p.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> s.<span class="built_in">empty</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 下面按指定规则模拟棋盘</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">isalpha</span>(p[j - <span class="number">1</span>])) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (dp[i][j - <span class="number">1</span>] &amp;&amp; s[i] == p[j - <span class="number">1</span>]) &#123;</span><br><span class="line">                        dp[i + <span class="number">1</span>][j] = <span class="literal">true</span>;</span><br><span class="line">                    &#125; </span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (p[j - <span class="number">1</span>] == <span class="string">&#x27;*&#x27;</span>) &#123;</span><br><span class="line">                <span class="comment">// 从左边一列最前面一行的那个true开始，当前列这一行之后（包含此行）都是true</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= m; i++) &#123; <span class="comment">// * 要找到最后一行防止遗漏，因为*不需要检验原串，所以不用担心访问越界</span></span><br><span class="line">                    <span class="keyword">if</span> (dp[i][j - <span class="number">1</span>]) &#123;</span><br><span class="line">                        <span class="keyword">for</span> (<span class="type">int</span> k = i; k &lt;= m; k++) &#123;</span><br><span class="line">                            dp[k][j] = <span class="literal">true</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (p[j - <span class="number">1</span>] == <span class="string">&#x27;?&#x27;</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (dp[i][j - <span class="number">1</span>]) &#123;</span><br><span class="line">                        dp[i + <span class="number">1</span>][j] = <span class="literal">true</span>; <span class="comment">// ?不需要检验是否满足，匹配任何一个</span></span><br><span class="line">                    &#125; </span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[m][n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="加油站">加油站</h3><h4 id="题目-2">题目</h4><p>在一条环路上有 <code>n</code> 个加油站，其中第 <code>i</code> 个加油站有汽油 <code>gas[i]</code> 升。</p><p>你有一辆油箱容量无限的的汽车，从第 <code>i</code> 个加油站开往第 <code>i+1</code> 个加油站需要消耗汽油 <code>cost[i]</code> 升。你从其中的一个加油站出发，开始时油箱为空。</p><p>给定两个整数数组 <code>gas</code> 和 <code>cost</code> ，如果你可以按顺序绕环路行驶一周，则返回出发时加油站的编号，否则返回 <code>-1</code> 。如果存在解，则 <strong>保证</strong> 它是 <strong>唯一</strong> 的。</p><p><strong>示例 1:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: gas = [1,2,3,4,5], cost = [3,4,5,1,2]</span><br><span class="line">输出: 3</span><br><span class="line">解释:</span><br><span class="line">从 3 号加油站(索引为 3 处)出发，可获得 4 升汽油。此时油箱有 = 0 + 4 = 4 升汽油</span><br><span class="line">开往 4 号加油站，此时油箱有 4 - 1 + 5 = 8 升汽油</span><br><span class="line">开往 0 号加油站，此时油箱有 8 - 2 + 1 = 7 升汽油</span><br><span class="line">开往 1 号加油站，此时油箱有 7 - 3 + 2 = 6 升汽油</span><br><span class="line">开往 2 号加油站，此时油箱有 6 - 4 + 3 = 5 升汽油</span><br><span class="line">开往 3 号加油站，你需要消耗 5 升汽油，正好足够你返回到 3 号加油站。</span><br><span class="line">因此，3 可为起始索引。</span><br></pre></td></tr></table></figure><p><strong>示例 2:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: gas = [2,3,4], cost = [3,4,3]</span><br><span class="line">输出: -1</span><br><span class="line">解释:</span><br><span class="line">你不能从 0 号或 1 号加油站出发，因为没有足够的汽油可以让你行驶到下一个加油站。</span><br><span class="line">我们从 2 号加油站出发，可以获得 4 升汽油。 此时油箱有 = 0 + 4 = 4 升汽油</span><br><span class="line">开往 0 号加油站，此时油箱有 4 - 3 + 2 = 3 升汽油</span><br><span class="line">开往 1 号加油站，此时油箱有 3 - 3 + 3 = 3 升汽油</span><br><span class="line">你无法返回 2 号加油站，因为返程需要消耗 4 升汽油，但是你的油箱只有 3 升汽油。</span><br><span class="line">因此，无论怎样，你都不可能绕环路行驶一周。</span><br></pre></td></tr></table></figure><p><strong>提示:</strong></p><ul><li><code>gas.length == n</code></li><li><code>cost.length == n</code></li><li><code>1 &lt;= n &lt;= 105</code></li><li><code>0 &lt;= gas[i], cost[i] &lt;= 104</code></li></ul><h4 id="分析-2">分析</h4><p>这题主要是找到贪心的策略，即某个符合特性的点。我们首先计算出每个加油站走到下一个站时获油和耗油间的差距，来得到未来一步净油量变化。这个净油量变化代表着走到下一加油站处油箱的实际变化。我们考虑一个“折线图”，从第一个加油站出发，计算净油量变化的<strong>前缀和</strong>代表一个点，每个点代表着每个加油站处油箱的油量，整张图表示油箱的变化。我们考虑一个最特殊的点：<strong>油箱油量最低的点——谷底（可能是负数）</strong>；在这个谷底点之后的每个点都不会比这个谷底的油量还要低，即若从谷底之后的那个点出发，油箱中的油总是够用的，也就是题目所求的出发点。</p><p><strong>“当你跌入谷底，之后的每一步都在爬升”</strong></p><p>代码如下</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">canCompleteCircuit</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; gas, vector&lt;<span class="type">int</span>&gt;&amp; cost)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 再思考一下这一题的贪心策略 </span></span><br><span class="line">        <span class="type">int</span> have = <span class="number">0</span>, cos = <span class="number">0</span>;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">diff</span><span class="params">(gas.size())</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; gas.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            have += gas[i];</span><br><span class="line">            cos += cost[i];</span><br><span class="line">            diff[i] = gas[i] - cost[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (have &gt;= cos) &#123;</span><br><span class="line">            <span class="comment">// 求diff的前缀和，最小的值即油量的谷底，从这个点后一个点出发一定能绕一圈</span></span><br><span class="line">            <span class="comment">// 为什么呢？因为以这个“谷底”为参照视角，之后走到的每一个加油站都不会低于“谷底”</span></span><br><span class="line">            <span class="comment">// 也就是说从之后的那个加油站出发，无论走到什么位置，油箱中的油都是够用的（不会比“谷底”还低）</span></span><br><span class="line">            <span class="comment">// “当你已经处于最低谷，之后的每一步都是爬升”</span></span><br><span class="line">            <span class="type">int</span> mmin = INT_MAX, idx = <span class="number">-1</span>, sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; diff.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">                sum += diff[i];</span><br><span class="line">                <span class="keyword">if</span> (sum &lt; mmin) &#123;</span><br><span class="line">                    mmin = sum;</span><br><span class="line">                    idx = i;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> (idx + <span class="number">1</span>) % diff.<span class="built_in">size</span>();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="切蛋糕的最小开销">切蛋糕的最小开销</h3><h4 id="题目-3">题目</h4><p>有一个 <code>m x n</code> 大小的矩形蛋糕，需要切成 <code>1 x 1</code> 的小块。</p><p>给你整数 <code>m</code> ，<code>n</code> 和两个数组：</p><ul><li><code>horizontalCut</code> 的大小为 <code>m - 1</code> ，其中 <code>horizontalCut[i]</code> 表示沿着水平线 <code>i</code> 切蛋糕的开销。</li><li><code>verticalCut</code> 的大小为 <code>n - 1</code> ，其中 <code>verticalCut[j]</code> 表示沿着垂直线 <code>j</code> 切蛋糕的开销。</li></ul><p>一次操作中，你可以选择任意不是 <code>1 x 1</code> 大小的矩形蛋糕并执行以下操作之一：</p><ol><li>沿着水平线 <code>i</code> 切开蛋糕，开销为 <code>horizontalCut[i]</code> 。</li><li>沿着垂直线 <code>j</code> 切开蛋糕，开销为 <code>verticalCut[j]</code> 。</li></ol><p>每次操作后，这块蛋糕都被切成两个独立的小蛋糕。</p><p>每次操作的开销都为最开始对应切割线的开销，并且不会改变。</p><p>请你返回将蛋糕全部切成 <code>1 x 1</code> 的蛋糕块的 <strong>最小</strong> 总开销。</p><p><strong>示例 1：</strong></p><p>**输入：**m = 3, n = 2, horizontalCut = [1,3], verticalCut = [5]</p><p>**输出：**13</p><p><strong>解释：</strong></p><p><img src="%E8%B4%AA%E5%BF%83/ezgifcom-animated-gif-maker-1.gif" alt="img"></p><ul><li>沿着垂直线 0 切开蛋糕，开销为 5 。</li><li>沿着水平线 0 切开 <code>3 x 1</code> 的蛋糕块，开销为 1 。</li><li>沿着水平线 0 切开 <code>3 x 1</code> 的蛋糕块，开销为 1 。</li><li>沿着水平线 1 切开 <code>2 x 1</code> 的蛋糕块，开销为 3 。</li><li>沿着水平线 1 切开 <code>2 x 1</code> 的蛋糕块，开销为 3 。</li></ul><p>总开销为 <code>5 + 1 + 1 + 3 + 3 = 13</code> 。</p><p><strong>示例 2：</strong></p><p>**输入：**m = 2, n = 2, horizontalCut = [7], verticalCut = [4]</p><p>**输出：**15</p><p><strong>解释：</strong></p><ul><li>沿着水平线 0 切开蛋糕，开销为 7 。</li><li>沿着垂直线 0 切开 <code>1 x 2</code> 的蛋糕块，开销为 4 。</li><li>沿着垂直线 0 切开 <code>1 x 2</code> 的蛋糕块，开销为 4 。</li></ul><p>总开销为 <code>7 + 4 + 4 = 15</code> 。</p><p><strong>提示：</strong></p><ul><li><code>1 &lt;= m, n &lt;= 20</code></li><li><code>horizontalCut.length == m - 1</code></li><li><code>verticalCut.length == n - 1</code></li><li><code>1 &lt;= horizontalCut[i], verticalCut[i] &lt;= 103</code></li></ul><h4 id="分析-3">分析</h4><p>本题期望得到最小开销，可以采用贪心的思想。根据贪心的核心思想，我们在每一刀都要选择使得总开销最小的切法，那选择开销最大的还是最小的下刀呢？考虑到蛋糕每切一刀，产生的“碎片”就越多，比如我按行切了一刀，那就会多一行出来，之后所有没切的列都要多切一下。因为每个<code>1 * 1</code>蛋糕的连接处总是要下刀的，因此如果把开销大的留到后面切，需要处理的蛋糕碎片就会更多，也就是切更多刀来切断每个蛋糕的这一刀，这就会导致总开销增加，因此不能把开销大的留到后面（因为蛋糕在变成<code>1 * 1</code>前总会产生碎片的，总要有某些刀需要对很多碎片处理的，那把这些需要频繁切的刀留给开销小的，就可以使得开销降低），因此在每一刀选择时选择当前开销最大的，尽快切掉避免之后处理更多的碎片，并在切完后维护更新当前需要处理的行蛋糕数和列蛋糕数。按行切增加行，此刀开销由单次开销和列数决定；按列切增加列数，此刀开销由单次开销和行数决定。</p><p>代码如下</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="comment">// 开销越大的需要越早切，因为越往后蛋糕的片数越多，而每个地方总是要下刀的</span></span><br><span class="line">    <span class="comment">// 开销越大的的地方越往后留需要的总开销越大</span></span><br><span class="line">    <span class="comment">// 按贪心的思想先把大开销的切了</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minimumCost</span><span class="params">(<span class="type">int</span> m, <span class="type">int</span> n, vector&lt;<span class="type">int</span>&gt;&amp; horizontalCut, vector&lt;<span class="type">int</span>&gt;&amp; verticalCut)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> row = <span class="number">1</span>, col = <span class="number">1</span>, ans = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">sort</span>(horizontalCut.<span class="built_in">begin</span>(), horizontalCut.<span class="built_in">end</span>(), [](<span class="keyword">auto</span> a, <span class="keyword">auto</span> b) &#123;</span><br><span class="line">            <span class="keyword">return</span> a &gt; b;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="built_in">sort</span>(verticalCut.<span class="built_in">begin</span>(), verticalCut.<span class="built_in">end</span>(), [](<span class="keyword">auto</span> a, <span class="keyword">auto</span> b) &#123;</span><br><span class="line">            <span class="keyword">return</span> a &gt; b;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="type">int</span> l = <span class="number">0</span>, r = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(l &lt; horizontalCut.<span class="built_in">size</span>() &amp;&amp; r &lt; verticalCut.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (horizontalCut[l] &gt; verticalCut[r]) &#123;</span><br><span class="line">                ans += horizontalCut[l++] * col;</span><br><span class="line">                row++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                ans += verticalCut[r++] * row;</span><br><span class="line">                col++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(l &lt; horizontalCut.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            ans += horizontalCut[l++] * col;</span><br><span class="line">            row++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(r &lt; verticalCut.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            ans += verticalCut[r++] * row;</span><br><span class="line">            col++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>循环神经网络</title>
      <link href="/ymhui.github.io/2024/09/20/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/ymhui.github.io/2024/09/20/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1>写在前面</h1><blockquote><p>参考书籍</p></blockquote><p><a href="http://www.d2l.ai">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><h1>循环神经网络前导</h1><p><strong>source code</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui/DeepLearning: Deep Learning with pytorch (github.com)</a></p><p><strong>use git to clone</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p><p><code>/RNN</code></p><blockquote><p><a href="http://markov.py">markov.py</a>text_preprocess.pynl_statistics.pyrandom_sampling.pysequential_partition.py<a href="http://util.py">util.py</a>rnn_self.pyrnn_lib.py</p></blockquote><p>到目前为止，我们一直都默认数据来自于某种分布，并且所有数据都是<em>i.i.d</em>（独立同分布）的，但是现实并非总是如此。比如一篇文章的文字是按某种顺序出现的，视频中的图像帧也按照特定顺序出现，网站的浏览行为也是有规律可循的…<strong>因此我们需要一个全新的模型来刻画这种现象。</strong></p><p>本文介绍的<strong>循环神经网络</strong>可以很好地处理序列信息，通过引入状态变量存储过去的信息和当前的输入，可以给出当前的输出。</p><p>由于许多循环神经网络的例子都基于文本数据，因此<strong>本文着重介绍语言模型</strong>。</p><h2 id="序列模型">序列模型</h2><p>处理序列数据需要统计工具和新的深度神经网络架构。</p><p>我们以股市交易数据为例入门，不妨用<em>x~t~<em>表示在</em>t</em>时间步(time step)时观测到的价格（<em>注：t通常是离散的并在整数或其子集上变化</em>）；如果希望在<em>t</em>日时较为准确地预测当日价格<em>x~t~</em>，应当有<em>x~t~</em> ~ <em><strong>P</strong></em>(<em>x~t~|x~t-1~…x~1~</em>), 即在已知前t - 1日结果的前提下预测当日结果。</p><h3 id="自回归模型">自回归模型</h3><p>第一种策略，假设在现实情况下相当长的序列x~t−1~, . . . , x~1~可能是不必要的，因此我们只需要满足某个长度为τ的时间跨度，即使用观测序列x~t−1~, . . . , x~t−τ~ 。当下获得的最直接的好处就是参数的数量总是不变的，至少在t &gt; τ时如此，这就使我们能够训练一个上面提及的深度网络。这种模型被称为<strong>自回归模型</strong>，因为它们是对自己执行回归。</p><p>第二种策略，如图8.1.2所示，是保留一些对过去观测的总结ht，并且同时更新预测ˆx~t~和总结ht。这就产生了基于ˆx~t~ = P(x~t~ | h~t~)估计x~t~，以及公式h~t~ = g(h~t−1~, x~t−1~)更新的模型。由于ht从未被观测到，这类模型也被称为<strong>隐变量自回归模型</strong></p><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/self_reg.png" alt="image-20240920170123607"></p><p>现在遇到一个新的问题，如何生成训练数据？一个常见的假设是，虽然特定值x~t~会改变，但<strong>序列本身的动力学不会改变</strong>，因为新的动力学一定受新数据的影响，而我们不可能用现有的数据预测出新的动力学。因此，整个序列的估计值都将通过以下的方式获得：</p><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5Carray_data.png" alt="image-20240920171153333"></p><p>当处理对象离散时（比如单词）上述公式仍有效，只不过要用分类器而不是回归模型来估计<strong>P</strong></p><h3 id="马尔可夫模型Markov">马尔可夫模型Markov</h3><h4 id="理论部分">理论部分</h4><p>简单来说我们在上述公式的基础上，取<em>τ</em> = 1，得到一个一阶马尔可夫模型；再考虑*x~t~*仅是离散值，使用动态规划沿着马尔科夫链精确地计算结果。</p><p>详见<a href="https://d2l.ai/chapter_recurrent-neural-networks/sequence.html#markov-models">9.1. Working with Sequences — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h4 id="训练">训练</h4><p>接下来我们使用正弦函数在背景噪声下生成一些序列数据。</p><p>然后基于这些数据做一些预测</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_data</span>(<span class="params">num</span>):</span><br><span class="line">    x = torch.arange(<span class="number">1</span>, num + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    y = torch.sin(<span class="number">0.01</span> * x) + torch.normal(<span class="number">0</span>, <span class="number">0.25</span>, (num, ))</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;初始化网络权重&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_normal_(m.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个简单的多层感知机&quot;&quot;&quot;</span></span><br><span class="line">    net = nn.Sequential(  <span class="comment"># 一个有两个全连接层的多层感知机，使用ReLU激活函数</span></span><br><span class="line">        nn.Linear(<span class="number">4</span>, <span class="number">10</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, loss, epochs, lr</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型，与前面格式一致，不再赘述&quot;&quot;&quot;</span></span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr)  <span class="comment"># Adam优化器</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;loss: <span class="subst">&#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span> :</span><br><span class="line">    t = <span class="number">1000</span></span><br><span class="line">    time, x = generate_data(t)</span><br><span class="line">    d2l.plot(time, [x], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, legend=[<span class="string">&#x27;x&#x27;</span>], xlim=[<span class="number">1</span>, t], figsize=(<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">    d2l.plt.show()</span><br><span class="line">    <span class="comment"># 接下来，我们将这个序列转换为模型的特征－标签（feature‐label）对; features-labels就是前面讲过的的自变量-因变量，还记得吗？</span></span><br><span class="line">    <span class="comment"># 基于嵌入维度τ，我们将数据映射为数据对yt = xt 和xt = [xt−τ , . . . , xt−1]。</span></span><br><span class="line">    <span class="comment"># 这比我们提供的数据样本少了τ个，因为我们没有足够的历史记录来</span></span><br><span class="line">    <span class="comment"># 描述前τ个数据样本。一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；另一个方法是用零填充序列</span></span><br><span class="line">    <span class="comment"># 使用前600个“特征－标签”对进行训练</span></span><br><span class="line">    tau = <span class="number">4</span>  <span class="comment"># 取tau = 4</span></span><br><span class="line">    <span class="comment"># 初始化特征矩阵，其中t是时间序列的长度，tau是时间步的大小</span></span><br><span class="line">    features = torch.zeros((t - tau, tau))</span><br><span class="line">    <span class="comment"># 遍历时间步，构建特征矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">        features[:, i] = x[i:t - tau + i]  <span class="comment"># 列是时间步，行是数据序列</span></span><br><span class="line">    labels = x[tau:].reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    batch_size, n_train = <span class="number">16</span>, <span class="number">600</span>  <span class="comment"># 用前600个数据对来训练</span></span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    train_iter = d2l.load_array((features[:n_train], labels[:n_train]), batch_size, is_train=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 下面使用一个相当简单的架构训练模型：</span></span><br><span class="line">    <span class="comment"># 一个拥有两个全连接层的多层感知机，ReLU激活函数，平方损失函数</span></span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    net = get_net()</span><br><span class="line">    epochs, lr = <span class="number">5</span>, <span class="number">0.01</span></span><br><span class="line">    train(net, train_iter, loss, epochs, lr)</span><br><span class="line">    <span class="comment"># 接下来开始预测</span></span><br><span class="line">    onestep_pred = net(features)  <span class="comment"># net(...)应用模型进行预测, 这里net(features)对其中的每个值依次作用产生输出，因此可以视作单步预测</span></span><br><span class="line">    d2l.plot([time, time[tau:]], [x.detach().numpy(), onestep_pred.detach().numpy()],</span><br><span class="line">             xlabel=<span class="string">&#x27;time&#x27;</span>, ylabel=<span class="string">&#x27;x&#x27;</span>, xlim=[<span class="number">1</span>, t], legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step_pred&#x27;</span>], figsize=(<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">    d2l.plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 以上均为单步预测，下面使用我们的预测(而不是原始数据)进行多步预测</span></span><br><span class="line">    multistep_pred = torch.zeros(t)</span><br><span class="line">    <span class="comment"># 用我们的预测数据填充</span></span><br><span class="line">    multistep_pred[:n_train + tau] = x[:n_train + tau]</span><br><span class="line">    <span class="comment"># 利用之前的预测值进行多步预测</span></span><br><span class="line">    <span class="comment"># f(xt) = f(xt-1, xt-2, ..., xt-tau)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_train + tau, t):</span><br><span class="line">        multistep_pred[i] = net(multistep_pred[i - tau:i].reshape((<span class="number">1</span>, -<span class="number">1</span>)))  <span class="comment"># 这步预测结果出来后会被后面继续使用</span></span><br><span class="line">    d2l.plot([time, time[n_train + tau:]], [x.detach().numpy(), multistep_pred[n_train + tau:].detach().numpy()],</span><br><span class="line">             xlabel=<span class="string">&#x27;time&#x27;</span>, ylabel=<span class="string">&#x27;x&#x27;</span>, xlim=[<span class="number">1</span>, t], legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;multi-step_pred&#x27;</span>], figsize=(<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">    d2l.plt.show()</span><br><span class="line">    <span class="comment"># 可以看到超过某个值后预测的效果很差，几乎趋于一个常数，这是由于错误的累积</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于k = 1, 4, 16, 64，通过对整个序列预测的计算，让我们更仔细地看一下k步预测的困难</span></span><br><span class="line">    max_steps = <span class="number">64</span></span><br><span class="line">    features = torch.zeros((t - tau - max_steps + <span class="number">1</span>, tau + max_steps))</span><br><span class="line">    <span class="comment"># features的列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">        features[:, i] = x[i:i + t - tau - max_steps + <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau, tau + max_steps):</span><br><span class="line">        features[:, i] = net(features[:, i - tau:i]).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可视化1, 4, 16, 64步预测的结果</span></span><br><span class="line">    steps = (<span class="number">1</span>, <span class="number">4</span>, <span class="number">16</span>, <span class="number">64</span>)</span><br><span class="line">    d2l.plot([time[tau + i - <span class="number">1</span>: t - max_steps + i] <span class="keyword">for</span> i <span class="keyword">in</span> steps],</span><br><span class="line">             [features[:, (tau + i - <span class="number">1</span>)].detach().numpy() <span class="keyword">for</span> i <span class="keyword">in</span> steps], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">             legend=[<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>-step pred&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> steps], xlim=[<span class="number">5</span>, t],</span><br><span class="line">             figsize=(<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/data.png" alt="image-20240922011313291"></p><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/onestep.png" alt="image-20240922011322697"></p><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/multistep.png" alt="image-20240922011333865"></p><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/141664steps.png" alt="image-20240922011348281"></p><h2 id="文本预处理">文本预处理</h2><p>序列数据的另一种常见形式是文本；例如，一篇文章可以被看作单词甚至是字符序列。文本预处理通常采用以下步骤：</p><ol><li>将文本作为字符串加载到内存中。</li><li>将字符串拆分为词元（如单词和字符）。</li><li>建立一个词表，将拆分的词元映射到数字索引。(因为词元的类型是字符/字符串，而模型需要的是数字)</li><li>将文本转换为数字索引序列，方便模型操作。</li></ol><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(d2l.download(<span class="string">&quot;time_machine&quot;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]  <span class="comment"># 只要英文字母且全小写</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">lines, token=<span class="string">&quot;word&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将输入行拆分为词元</span></span><br><span class="line"><span class="string">    :param lines: 输入的文本行列表</span></span><br><span class="line"><span class="string">    :param token: 次元类型</span></span><br><span class="line"><span class="string">    :return: 拆分后的列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid token flag: &quot;</span> + token)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立一个词表，记录词元到数字的映射</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        <span class="comment"># 按词元出现频率排序，降序</span></span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        <span class="variable language_">self</span>._tokens_freq = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 未知词元索引为0</span></span><br><span class="line">        <span class="variable language_">self</span>.idx2token = [<span class="string">&#x27;&lt;ink&gt;&#x27;</span>] + reserved_tokens  <span class="comment"># 索引到词元</span></span><br><span class="line">        <span class="variable language_">self</span>.token2idx = &#123;token: idx <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.idx2token)&#125;  <span class="comment"># 词元到索引</span></span><br><span class="line">        <span class="keyword">for</span> token, freq <span class="keyword">in</span> <span class="variable language_">self</span>._tokens_freq:</span><br><span class="line">            <span class="keyword">if</span> freq &lt; min_freq:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.token2idx:</span><br><span class="line">                <span class="variable language_">self</span>.idx2token.append(token)</span><br><span class="line">                <span class="variable language_">self</span>.token2idx[token] = <span class="built_in">len</span>(<span class="variable language_">self</span>.idx2token) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回词汇表中词汇的数量&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.idx2token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;将一个或多个词汇转换为对应的索引，若词汇不存在，则返回未知词汇标识&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 若tokens不是列表或元组，则直接返回该词汇的索引或未知词汇标识</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.token2idx.get(tokens, <span class="variable language_">self</span>.unk)</span><br><span class="line">        <span class="comment"># 若tokens是列表或元组，则逐个转换为索引</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_tokens</span>(<span class="params">self, indices</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;将一个或多个索引转换为对应的词汇&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 若indices不是列表或元组，则直接返回该索引对应的词汇</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.idx2token[indices]</span><br><span class="line">        <span class="comment"># 若indices是列表或元组，则逐个转换为词汇</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.idx2token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property  </span><span class="comment"># unk可以像属性一样被访问，而不需要调用方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unk</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">token_freq</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._tokens_freq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_corpus</span>(<span class="params">tokens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计词元频率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;整合所有功能，返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, token=<span class="string">&#x27;char&#x27;</span>)  <span class="comment"># 次元类型改为char</span></span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="comment"># 因为数据集中的每一个文本行不一定是一个句子或者段落</span></span><br><span class="line">    <span class="comment"># 所以展平到一个列表中</span></span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 如果限定了最大tokens的数量，我们就只取前max行</span></span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 从时光机器文本中读取数据</span></span><br><span class="line">    d2l.DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>, <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(lines))</span><br><span class="line">    <span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(lines[<span class="number">114</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词元化</span></span><br><span class="line">    tokens = tokenize(lines)</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens[:<span class="number">10</span>]:</span><br><span class="line">        <span class="built_in">print</span>(token)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用上面拿到的数据集构建词表，看几个高频词及其索引</span></span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;high frequency token and its index:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token2idx.items())[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 现在就可以把每一行文本转化成索引序列了</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;line text -&gt; indices:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>]:</span><br><span class="line">        <span class="built_in">print</span>(tokens[i], <span class="string">&#x27;-&gt;&#x27;</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 验证整合的功能</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;check all functions in one:&quot;</span>)</span><br><span class="line">    tokens, vocab = load_corpus_time_machine()  <span class="comment"># 接受词元索引列表和词表</span></span><br><span class="line">    <span class="built_in">print</span>((<span class="built_in">len</span>(tokens), <span class="built_in">len</span>(vocab)))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">3221</span><br><span class="line">the time machine by h g wells</span><br><span class="line">but said the medical man staring hard at a coal in the fire if</span><br><span class="line">[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[&#x27;i&#x27;]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;traveller&#x27;, &#x27;for&#x27;, &#x27;so&#x27;, &#x27;it&#x27;, &#x27;will&#x27;, &#x27;be&#x27;, &#x27;convenient&#x27;, &#x27;to&#x27;, &#x27;speak&#x27;, &#x27;of&#x27;, &#x27;him&#x27;]</span><br><span class="line">[&#x27;was&#x27;, &#x27;expounding&#x27;, &#x27;a&#x27;, &#x27;recondite&#x27;, &#x27;matter&#x27;, &#x27;to&#x27;, &#x27;us&#x27;, &#x27;his&#x27;, &#x27;grey&#x27;, &#x27;eyes&#x27;, &#x27;shone&#x27;, &#x27;and&#x27;]</span><br><span class="line">high frequency token and its index:</span><br><span class="line">[(&#x27;&lt;ink&gt;&#x27;, 0), (&#x27;the&#x27;, 1), (&#x27;i&#x27;, 2), (&#x27;and&#x27;, 3), (&#x27;of&#x27;, 4), (&#x27;a&#x27;, 5), (&#x27;to&#x27;, 6), (&#x27;was&#x27;, 7), (&#x27;in&#x27;, 8), (&#x27;that&#x27;, 9)]</span><br><span class="line">line text -&gt; indices:</span><br><span class="line">[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;] -&gt; [1, 19, 50, 40, 2183, 2184, 400]</span><br><span class="line">[&#x27;twinkled&#x27;, &#x27;and&#x27;, &#x27;his&#x27;, &#x27;usually&#x27;, &#x27;pale&#x27;, &#x27;face&#x27;, &#x27;was&#x27;, &#x27;flushed&#x27;, &#x27;and&#x27;, &#x27;animated&#x27;, &#x27;the&#x27;] -&gt; [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]</span><br><span class="line">check all functions in one:</span><br><span class="line">(170580, 28)</span><br></pre></td></tr></table></figure><h2 id="语言模型和数据集">语言模型和数据集</h2><p>模型原理部分参考<a href="https://d2l.ai/chapter_recurrent-neural-networks/language-model.html">9.3. Language Models — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h3 id="自然语言统计">自然语言统计</h3><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    lines = d2l.read_time_machine()</span><br><span class="line">    tokens = d2l.tokenize(lines)</span><br><span class="line">    corpus = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    vocab = d2l.Vocab(corpus)</span><br><span class="line">    <span class="built_in">print</span>(vocab.token_freqs[:<span class="number">10</span>])</span><br><span class="line">    frequencies = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> vocab.token_freqs]</span><br><span class="line">    d2l.plot(frequencies, xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;n(x)&#x27;</span>, xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">    d2l.plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看一下二元语法出现的概率</span></span><br><span class="line">    bigram_tokens = [pair <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">1</span>], corpus[<span class="number">1</span>:])]</span><br><span class="line">    bigram_vocab = d2l.Vocab(bigram_tokens)</span><br><span class="line">    <span class="built_in">print</span>(bigram_vocab.token_freqs[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 再来看一下三元组</span></span><br><span class="line">    trigram_tokens = [triple <span class="keyword">for</span> triple <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">2</span>], corpus[<span class="number">1</span>:-<span class="number">1</span>], corpus[<span class="number">2</span>:])]</span><br><span class="line">    trigram_vocab = d2l.Vocab(trigram_tokens)</span><br><span class="line">    <span class="built_in">print</span>(trigram_vocab.token_freqs[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对比一下三种组合的出现概率图</span></span><br><span class="line">    bigram_freq = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> bigram_vocab.token_freqs]</span><br><span class="line">    trigram_freq = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> trigram_vocab.token_freqs]</span><br><span class="line">    d2l.plot([frequencies, bigram_freq, trigram_freq], xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;n(x)&#x27;</span>, xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">             legend=[<span class="string">&#x27;single&#x27;</span>, <span class="string">&#x27;double&#x27;</span>, <span class="string">&#x27;triple&#x27;</span>])</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[(&#x27;the&#x27;, 2261), (&#x27;i&#x27;, 1267), (&#x27;and&#x27;, 1245), (&#x27;of&#x27;, 1155), (&#x27;a&#x27;, 816), (&#x27;to&#x27;, 695), (&#x27;was&#x27;, 552), (&#x27;in&#x27;, 541), (&#x27;that&#x27;, 443), (&#x27;my&#x27;, 440)]</span><br><span class="line">[((&#x27;of&#x27;, &#x27;the&#x27;), 309), ((&#x27;in&#x27;, &#x27;the&#x27;), 169), ((&#x27;i&#x27;, &#x27;had&#x27;), 130), ((&#x27;i&#x27;, &#x27;was&#x27;), 112), ((&#x27;and&#x27;, &#x27;the&#x27;), 109), ((&#x27;the&#x27;, &#x27;time&#x27;), 102), ((&#x27;it&#x27;, &#x27;was&#x27;), 99), ((&#x27;to&#x27;, &#x27;the&#x27;), 85), ((&#x27;as&#x27;, &#x27;i&#x27;), 78), ((&#x27;of&#x27;, &#x27;a&#x27;), 73)]</span><br><span class="line">[((&#x27;the&#x27;, &#x27;time&#x27;, &#x27;traveller&#x27;), 59), ((&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;), 30), ((&#x27;the&#x27;, &#x27;medical&#x27;, &#x27;man&#x27;), 24), ((&#x27;it&#x27;, &#x27;seemed&#x27;, &#x27;to&#x27;), 16), ((&#x27;it&#x27;, &#x27;was&#x27;, &#x27;a&#x27;), 15), ((&#x27;here&#x27;, &#x27;and&#x27;, &#x27;there&#x27;), 15), ((&#x27;seemed&#x27;, &#x27;to&#x27;, &#x27;me&#x27;), 14), ((&#x27;i&#x27;, &#x27;did&#x27;, &#x27;not&#x27;), 14), ((&#x27;i&#x27;, &#x27;saw&#x27;, &#x27;the&#x27;), 13), ((&#x27;i&#x27;, &#x27;began&#x27;, &#x27;to&#x27;), 13)]</span><br></pre></td></tr></table></figure><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/nls.png" alt="image-20240922133225914"></p><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/three_cmp.png" alt="image-20240922202317964"></p><h3 id="读取长序列数据">读取长序列数据</h3><p>因为序列数据本质上是连续的，因此我们在处理数据时<strong>需要解决读取长序列数据的问题</strong>。</p><p>一种处理办法是：<strong>当序列过长而不能被模型一次性处理时，拆分这样的序列方便模型读取</strong>。假设我们将使用神经网络来训练语言模型，<strong>模型中的网络一次处</strong><br><strong>理具有预定义长度（例如n个时间步）的一个小批量序列</strong>。现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。</p><p>首先，由于文本序列可以是任意长的，例如整本《时光机器》，于是任意长的序列可以被我们划分为具有相同时间步数的子序列。当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。假设网络一次只处理具有n个时间步的子序列。</p><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/long_spilit.png" alt="image-20240922204539384"></p><p>上图画出了从原始文本序列获得子序列的所有不同的方式，其中n = 5，并且每个时间步的词元对应于一个字符。</p><p>那么，我们应该选择哪一个呢？如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的；因此，我们可以从随机偏移量开始划分序列，以同时获得覆盖性（coverage）和随机性（randomness）。下面介绍两个策略：<strong>随机采样，顺序分区</strong></p><h4 id="随机采样">随机采样</h4><p>在此策略下，每个样本都是在原始的长序列上任意捕获的子序列。在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。<strong>对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元</strong>，因此标签是移位了一个词元的原始序列。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">seq_fata_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param corpus:</span></span><br><span class="line"><span class="string">    :param batch_size: 每个小批量中子序列样本的数量</span></span><br><span class="line"><span class="string">    :param num_steps: 每个子序列中预定义的时间步数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从语料库中随机选择一个片段作为开始, 切片内容包括num_steps - 1</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    <span class="comment"># 计算基于当前语料库长度和序列长度能够生成的序列数量</span></span><br><span class="line">    <span class="comment"># 减去1，是因为我们需要考虑标签</span></span><br><span class="line">    num_sequences = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># 创建一个列表，包含所有序列的起始索引，即长度为num_steps的子序列的起始索引</span></span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_sequences * num_steps, num_steps))</span><br><span class="line">    <span class="comment"># 在随机抽样的迭代过程中，</span></span><br><span class="line">    <span class="comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span></span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个辅助函数，根据给定的起始位置从语料库中提取序列</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="comment"># 返回从pos位置开始的长度为num_steps的序列</span></span><br><span class="line">        <span class="keyword">return</span> corpus[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    num_batches = num_sequences // batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_batches * batch_size, batch_size):</span><br><span class="line">        <span class="comment"># initial_indices包含子序列的随机起始索引</span></span><br><span class="line">        initial_indices_per_batch = initial_indices[i: i + batch_size]  <span class="comment"># 从打乱顺序的起始索引列表中获取当前批次的起始索引</span></span><br><span class="line">        <span class="comment"># 根据当前批次中每个序列的起始索引，创建X（输入序列）和Y（目标序列）</span></span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="comment"># 生成并提供输入和目标序列的张量表示</span></span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 生成一个0 ~ 34的序列，并设置批量大小 = 2，时间步数 = 5</span></span><br><span class="line">    <span class="comment"># 这样可以生成(35 - 1) // 5 = 6个特征-标签子序列对</span></span><br><span class="line">    seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;每个小批量中有两个子序列对:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_fata_iter_random(seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;第%d个\&quot;特征-标签\&quot;子序列对小批量&quot;</span> % i)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;X: <span class="subst">&#123;X&#125;</span>, \nY: <span class="subst">&#123;Y&#125;</span>&#x27;</span>)</span><br><span class="line">        i += <span class="number">1</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">每个小批量中有两个子序列对:</span><br><span class="line">第1个&quot;特征-标签&quot;子序列对小批量</span><br><span class="line">X: tensor([[0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9]]), </span><br><span class="line">Y: tensor([[ 1,  2,  3,  4,  5],</span><br><span class="line">        [ 6,  7,  8,  9, 10]])</span><br><span class="line">第2个&quot;特征-标签&quot;子序列对小批量</span><br><span class="line">X: tensor([[20, 21, 22, 23, 24],</span><br><span class="line">        [10, 11, 12, 13, 14]]), </span><br><span class="line">Y: tensor([[21, 22, 23, 24, 25],</span><br><span class="line">        [11, 12, 13, 14, 15]])</span><br><span class="line">第3个&quot;特征-标签&quot;子序列对小批量</span><br><span class="line">X: tensor([[15, 16, 17, 18, 19],</span><br><span class="line">        [25, 26, 27, 28, 29]]), </span><br><span class="line">Y: tensor([[16, 17, 18, 19, 20],</span><br><span class="line">        [26, 27, 28, 29, 30]])</span><br></pre></td></tr></table></figure><h4 id="顺序分区">顺序分区</h4><p>在迭代过程中，除了对原始序列可以随机抽样外，我们还<strong>可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的</strong>。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为<strong>顺序分区</strong>。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;顺序分区策略生成小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始划分序列</span></span><br><span class="line">    <span class="comment"># 生成一个随机偏移量，用于乱序数据</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>, num_steps)</span><br><span class="line">    <span class="comment"># 计算基于批次大小可处理的令牌数量</span></span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    <span class="comment"># 从乱序后的数据中创建输入序列Xs</span></span><br><span class="line">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class="line">    <span class="comment"># 从乱序后的数据中创建目标序列Ys，相比Xs向后移动了一个位置</span></span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>: offset + <span class="number">1</span> + num_tokens])</span><br><span class="line">    <span class="comment"># 将序列Xs和Ys重塑为批次大小，以便于训练</span></span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 计算批次的数量</span></span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps</span><br><span class="line">    <span class="comment"># 遍历所有序列，生成批次数据</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        <span class="comment"># X是当前批次的输入序列，长度为num_steps</span></span><br><span class="line">        X = Xs[:, i: i + num_steps]</span><br><span class="line">        <span class="comment"># Y是当前批次的目标序列，长度为num_steps</span></span><br><span class="line">        Y = Ys[:, i: i + num_steps]</span><br><span class="line">        <span class="comment"># 产出当前批次的输入序列X和目标序列Y</span></span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 数据配置与随机采样策略一致</span></span><br><span class="line">    seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_sequential(seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;X: <span class="subst">&#123;X&#125;</span>,\nY: <span class="subst">&#123;Y&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">X: tensor([[ 2,  3,  4,  5,  6],</span><br><span class="line">        [18, 19, 20, 21, 22]]),</span><br><span class="line">Y: tensor([[ 3,  4,  5,  6,  7],</span><br><span class="line">        [19, 20, 21, 22, 23]])</span><br><span class="line">X: tensor([[ 7,  8,  9, 10, 11],</span><br><span class="line">        [23, 24, 25, 26, 27]]),</span><br><span class="line">Y: tensor([[ 8,  9, 10, 11, 12],</span><br><span class="line">        [24, 25, 26, 27, 28]])</span><br><span class="line">X: tensor([[12, 13, 14, 15, 16],</span><br><span class="line">        [28, 29, 30, 31, 32]]),</span><br><span class="line">Y: tensor([[13, 14, 15, 16, 17],</span><br><span class="line">        [29, 30, 31, 32, 33]])</span><br></pre></td></tr></table></figure><p><strong>将两种策略整合出辅助类</strong></p><p><code>util.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SeqDataLoader</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_steps, use_random, max_tokens</span>):</span><br><span class="line">        <span class="keyword">if</span> use_random:</span><br><span class="line">            <span class="variable language_">self</span>.data_iter_fn = d2l.seq_data_iter_random</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.data_iter_fn = d2l.seq_data_iter_sequential</span><br><span class="line">        <span class="variable language_">self</span>.corpus, <span class="variable language_">self</span>.vocab = d2l.load_corpus_time_machine(max_tokens)</span><br><span class="line">        <span class="variable language_">self</span>.batch_size, <span class="variable language_">self</span>.num_steps = batch_size, num_steps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data_iter_fn(<span class="variable language_">self</span>.corpus, batch_size=<span class="variable language_">self</span>.batch_size, num_steps=<span class="variable language_">self</span>.num_steps)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_time_machine</span>(<span class="params">batch_size, num_steps, use_random=<span class="literal">False</span>, max_tokens = <span class="number">1e5</span></span>):</span><br><span class="line">    data_iter = SeqDataLoader(batch_size, num_steps, use_random, max_tokens)</span><br><span class="line">    <span class="keyword">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></table></figure><h1>循环神经网络</h1><p>下面我们开始正式介绍循环神经网络！</p><p>理论部分详见<a href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#recurrent-neural-networks">9.4. Recurrent Neural Networks — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><p>无隐状态的神经网络：比如一个有单隐藏层的多层感知机</p><p>有隐状态的循环神经网络：<a href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#recurrent-neural-networks-with-hidden-states">…</a></p><h2 id="基于循环神经网络的字符级语言模型、困惑度">基于循环神经网络的字符级语言模型、困惑度</h2><p>详见<a href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#rnn-based-character-level-language-models">9.4. Recurrent Neural Networks — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="从零实现循环神经网络">从零实现循环神经网络</h2><p>此处将从头开始基于循环神经网络实现字符级语言模型，在时光机器数据集上训练。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hidden, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    初始化神经网络的模型参数</span></span><br><span class="line"><span class="string">    :param vocab_size 语言模型的输入输出来自同一个词表，因此他们具有相同的维度即词表大小</span></span><br><span class="line"><span class="string">    :param num_hidden 隐藏层单元数，可调的超参数</span></span><br><span class="line"><span class="string">    :param device</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = normal((num_inputs, num_hidden))</span><br><span class="line">    W_hh = normal((num_hidden, num_hidden))</span><br><span class="line">    b_h = torch.zeros(num_hidden, device=device)</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hidden, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_rnn_state</span>(<span class="params">batch_size, num_hidden, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;初始化时返回隐状态，返回值全0填充&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.zeros((batch_size, num_hidden), device=device),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># rnn函数定义了如何在一个时间步内计算隐状态和输出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rnn</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    <span class="comment"># inputs形状: (时间步数量，批量大小，词表大小)</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment"># X的形状: (批量大小，词表大小)</span></span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        <span class="comment"># 使用tanh激活函数</span></span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 封装上述函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModelScratch</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从零实现循环神经网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hidden, device, get_params, init_state, forward_fn</span>):</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size, <span class="variable language_">self</span>.num_hidden = vocab_size, num_hidden</span><br><span class="line">        <span class="variable language_">self</span>.params = get_params(vocab_size, num_hidden, device)</span><br><span class="line">        <span class="variable language_">self</span>.init_state, <span class="variable language_">self</span>.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = F.one_hot(X.T, <span class="variable language_">self</span>.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.forward_fn(X, state, <span class="variable language_">self</span>.params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.init_state(batch_size, <span class="variable language_">self</span>.num_hidden, device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch8</span>(<span class="params">prefix, num_pred, net, vocab, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测字符串prefix后面的内容&quot;&quot;&quot;</span></span><br><span class="line">    state = net.begin_state(batch_size=<span class="number">1</span>, device=device)</span><br><span class="line">    outputs = [vocab[prefix[<span class="number">0</span>]]]</span><br><span class="line">    get_input = <span class="keyword">lambda</span>: torch.tensor([outputs[-<span class="number">1</span>]], device=device).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 预热期</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> prefix[<span class="number">1</span>:]:</span><br><span class="line">        _, state = net(get_input(), state)</span><br><span class="line">        outputs.append(vocab[y])</span><br><span class="line">    <span class="comment"># 预测num_pred步</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_pred):</span><br><span class="line">        y, state = net(get_input(), state)</span><br><span class="line">        outputs.append(<span class="built_in">int</span>(y.argmax(dim=<span class="number">1</span>).reshape(<span class="number">1</span>)))  <span class="comment"># 把向量转化为索引</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="keyword">for</span> i <span class="keyword">in</span> outputs])  <span class="comment"># 索引转化为token</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grad_clipping</span>(<span class="params">net, theta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        params = net.params</span><br><span class="line">    norm = torch.sqrt(<span class="built_in">sum</span>(torch.<span class="built_in">sum</span>(p.grad ** <span class="number">2</span>) <span class="keyword">for</span> p <span class="keyword">in</span> params))</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updator, device, use_random_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练网络一个迭代周期&quot;&quot;&quot;</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, d2l.Timer()</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失之和</span></span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            <span class="comment"># 如果state还没有初始化或者使用随机采样</span></span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updator, torch.optim.Optimizer):</span><br><span class="line">            updator.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updator.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 因为已经调用过mean方法</span></span><br><span class="line">            updator(batch_size=<span class="number">1</span>)</span><br><span class="line">        metric.add(y.numel() * l, y.numel())</span><br><span class="line">    <span class="comment"># 第一个返回值是困惑度perplexity，用于衡量语言模型的性能</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环神经网络模型的训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device, use_random=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型&quot;&quot;&quot;</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;perplexity&#x27;</span>, legend=[<span class="string">&#x27;train&#x27;</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updator = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updator = <span class="keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)  <span class="comment"># 接受一个参数批量大小</span></span><br><span class="line">    <span class="comment"># 预测函数</span></span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)  <span class="comment"># 接受一个参数初始序列</span></span><br><span class="line">    <span class="comment"># 训练和预测</span></span><br><span class="line">    perplexity, speed = -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        perplexity, speed = train_epoch_ch8(net, train_iter, loss, updator, device, use_random)</span><br><span class="line">        <span class="comment"># print(f&#x27;perplexity:  &#123;perplexity&#125;&#x27;)</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (perplexity,))</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化困惑度动态迭代结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;perplexity:  <span class="subst">&#123;perplexity&#125;</span>, <span class="subst">&#123;speed&#125;</span> tokens / per second, on <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predict &#x27;time traveller&#x27;:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&quot;time traveller&quot;</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&quot;traveller&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 之前一直将词元表示为一个索引, 但这样会使得模型难以学习(一个标量), 因此引入独热编码将词元映射为向量(互不相同的索引映射为互不相同的单位向量)</span></span><br><span class="line">    <span class="built_in">print</span>(F.one_hot(torch.tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>]), <span class="built_in">len</span>(vocab)))</span><br><span class="line">    <span class="comment"># 每次采样的小批量数据形状是二维张量：（批量大小，时间步数）</span></span><br><span class="line">    <span class="comment"># one_hot函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小</span></span><br><span class="line">    <span class="comment"># 转换输入的维度，以便获得形状为（时间步数，批量大小，词表大小）的输出</span></span><br><span class="line">    X = torch.arange(<span class="number">10</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="built_in">print</span>(F.one_hot(X.T, <span class="built_in">len</span>(vocab)).shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 验证一下我们手搓的循环神经网络是否输出正确的形状</span></span><br><span class="line">    num_hidden = <span class="number">512</span></span><br><span class="line">    net = RNNModelScratch(<span class="built_in">len</span>(vocab), num_hidden, d2l.try_gpu(), get_params, init_rnn_state, rnn)</span><br><span class="line">    state = net.begin_state(X.shape[<span class="number">0</span>], d2l.try_gpu())</span><br><span class="line">    Y, new_state = net(X.to(d2l.try_gpu()), state)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(new_state))</span><br><span class="line">    <span class="built_in">print</span>(new_state[<span class="number">0</span>].shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不训练直接预测</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predict without training:\ntime traveller ...? -&gt;&quot;</span>)</span><br><span class="line">    pred = predict_ch8(<span class="string">&quot;time traveller &quot;</span>, <span class="number">10</span>, net, vocab, d2l.try_gpu())  <span class="comment"># 生成离谱的 预测结果</span></span><br><span class="line">    <span class="built_in">print</span>(pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练后再预测</span></span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;not random sample:&quot;</span>)</span><br><span class="line">    train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), <span class="literal">False</span>)  <span class="comment"># 不使用随机采样</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;random sample:&quot;</span>)</span><br><span class="line">    train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), <span class="literal">True</span>)  <span class="comment"># 使用随机采样</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span><br><span class="line">         0, 0, 0, 0],</span><br><span class="line">        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span><br><span class="line">         0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span><br><span class="line">         0, 0, 0, 0]])</span><br><span class="line">torch.Size([5, 2, 28])</span><br><span class="line"></span><br><span class="line">torch.Size([10, 28])</span><br><span class="line">1</span><br><span class="line">torch.Size([2, 512])</span><br><span class="line">predict without training:</span><br><span class="line">time traveller ...? -&gt;</span><br><span class="line">time traveller jckmckmckm</span><br><span class="line">not random sample:</span><br><span class="line">perplexity:  1.0546326766270984, 14819.88615243743 tokens / per second, on cpu</span><br><span class="line">predict &#x27;time traveller&#x27;:</span><br><span class="line">time travelleryou can show black is white by argument said filby</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></table></figure><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/not_random.png" alt="image-20240923151710672"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">random sample:</span><br><span class="line">perplexity:  1.30568447689415, 14425.676215978849 tokens / per second, on cpu</span><br><span class="line">predict &#x27;time traveller&#x27;:</span><br><span class="line">time travellerit s against reason said filbywan a oft reaverathe</span><br><span class="line">travellerit s against reason said filbywhat had in an at re</span><br></pre></td></tr></table></figure><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/random.png" alt="image-20240923152727857"></p><h2 id="简洁实现的循环神经网络">简洁实现的循环神经网络</h2><p>此节中我们将使用深度学习框架中的高级API实现循环神经网络</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为一个完整的循环神经网络模型定义一个RNNModule类</span></span><br><span class="line"><span class="comment"># 由于rnn_layer只包含隐藏的循环层，因此还需要创建一个单独的输出层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = rnn_layer</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = vocab_size</span><br><span class="line">        <span class="variable language_">self</span>.num_hidden = <span class="variable language_">self</span>.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 如果RNN是双向的，num_directions应该是 2，否则是 1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.rnn.bidirectional:</span><br><span class="line">            <span class="variable language_">self</span>.num_directions = <span class="number">1</span></span><br><span class="line">            <span class="variable language_">self</span>.linear = nn.Linear(<span class="variable language_">self</span>.num_hidden, <span class="variable language_">self</span>.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.num_directions = <span class="number">2</span></span><br><span class="line">            <span class="variable language_">self</span>.linear = nn.Linear(<span class="variable language_">self</span>.num_hidden * <span class="number">2</span>, <span class="variable language_">self</span>.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, state</span>):</span><br><span class="line">        X = F.one_hot(inputs.T.long(), <span class="variable language_">self</span>.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = <span class="variable language_">self</span>.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层首先将Y的形状改为(num_steps * batch_size, num_hidden)</span></span><br><span class="line">        <span class="comment"># 它的输出形状是(num_steps * batch_size, vocab_size)</span></span><br><span class="line">        output = <span class="variable language_">self</span>.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span><br><span class="line">        <span class="comment"># LSTM: 长短期记忆网络：一种特殊的循环神经网络</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(<span class="variable language_">self</span>.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> torch.zeros((<span class="variable language_">self</span>.num_directions * <span class="variable language_">self</span>.rnn.num_layers,</span><br><span class="line">                                batch_size, <span class="variable language_">self</span>.num_hidden), device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((<span class="variable language_">self</span>.num_directions * <span class="variable language_">self</span>.rnn.num_layers, batch_size, <span class="variable language_">self</span>.num_hidden),</span><br><span class="line">                                device=device), torch.zeros((<span class="variable language_">self</span>.num_directions * <span class="variable language_">self</span>.rnn.num_layers,</span><br><span class="line">                                                             batch_size, <span class="variable language_">self</span>.num_hidden), device=device))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line">    num_hidden = <span class="number">256</span></span><br><span class="line">    <span class="comment"># 定义模型</span></span><br><span class="line">    rnn_layer = nn.RNN(<span class="built_in">len</span>(vocab), num_hidden)</span><br><span class="line">    <span class="comment"># 初始化隐状态</span></span><br><span class="line">    state = torch.zeros((<span class="number">1</span>, batch_size, num_hidden))</span><br><span class="line">    <span class="built_in">print</span>(state.shape)</span><br><span class="line"></span><br><span class="line">    X = torch.rand(size=(num_steps, batch_size, <span class="built_in">len</span>(vocab)))</span><br><span class="line">    Y, state_new = rnn_layer(X, state)  <span class="comment"># rnn_layer就是之前的net</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练与预测</span></span><br><span class="line">    device = d2l.try_gpu()</span><br><span class="line">    net = RNNModel(rnn_layer, vocab_size=<span class="built_in">len</span>(vocab))</span><br><span class="line">    net = net.to(device=device)</span><br><span class="line">    predict = d2l.predict_ch8(<span class="string">&quot;time traveller&quot;</span>, <span class="number">10</span>, net, vocab, device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predict of &#x27;time traveller&#x27; without training:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict)  <span class="comment"># 这样得到的是一个胡扯的结果，因为没有训练</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;start training:&quot;</span>)</span><br><span class="line">    num_epochs, lr = <span class="number">500</span>, <span class="number">0.1</span></span><br><span class="line">    d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device=device)</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化困惑度</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([1, 32, 256])</span><br><span class="line">predict of &#x27;time traveller&#x27; without training:</span><br><span class="line">time travellerxvxcxvfxxx</span><br><span class="line">start training:</span><br><span class="line">perplexity 4.0, 50341.3 tokens/sec on cpu</span><br><span class="line">time travelleryou ong thave dery ald har he hare ard an therimet</span><br><span class="line">traveller and thas ed ane tore red ane trever tinntalle som</span><br></pre></td></tr></table></figure><p><img src="%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/rnn_api.png" alt="image-20240923163751477"></p><h1>通过时间反向传播</h1><p>本部分为纯理论介绍，详见<a href="https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#backpropagation-through-time">9.7. Backpropagation Through Time — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>注意力机制</title>
      <link href="/ymhui.github.io/2024/09/16/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/ymhui.github.io/2024/09/16/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1>写在前面</h1><blockquote><p>参考书籍</p></blockquote><p><a href="http://www.d2l.ai">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><h1>注意力机制</h1><p><strong>source code</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui/DeepLearning: Deep Learning with pytorch (github.com)</a></p><p><strong>use git to clone</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p><p><code>/attention</code></p><blockquote><p><a href="http://visualization.py">visualization.py</a>batch_matrix.pynadaraya_watson.py<a href="http://score.py">score.py</a><a href="http://bahdanau.py">bahdanau.py</a>multi_head.pyself_attention.pyposition_encoding.py</p></blockquote><p><strong>注意力机制</strong>（Attention Mechanism）是一种在神经网络中用于提升模型性能和效率的重要技术，特别是在处理序列数据和复杂输入时。它通过让模型专注于输入的特定部分，从而提高对相关信息的利用。</p><p>其核心思想是：在处理数据时，模型不需要对所有输入数据赋予相等的关注，而是根据上下文动态调整关注的重点。</p><h2 id="查询、键和值">查询、键和值</h2><p><code>查询</code>：自主性提示</p><p><code>键</code>：非自主性提示</p><p><code>值</code>：感官输入</p><h3 id="非自主性提示">非自主性提示</h3><p>指的是输入数据的特征，类似于感官输入，可以使用全连接层或汇聚层（如最大汇聚层或平均汇聚层）来处理。</p><h3 id="自主性提示">自主性提示</h3><p>在注意力机制中被称为查询（query），用于引导模型的注意力方向。</p><h3 id="注意力机制的工作原理">注意力机制的工作原理</h3><ul><li>给定一个查询（<strong>自主性提示</strong>），注意力机制<strong>通过计算查询与多个键（非自主性提示）的匹配程度</strong>，<strong>决定哪些值（感官输入）应该被关注</strong>。</li><li><strong>每个值（感官输入）都有一个对应的键</strong>。模型根据查询和键的匹配情况，通过注意力汇聚来选择最相关的值。</li></ul><p>即通过训练引导模型朝着指定方向搜索来加快效率，提升精确度</p><h2 id="注意力的可视化">注意力的可视化</h2><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_heatmaps</span>(<span class="params">matrices, xlabel, ylabel, titles=<span class="literal">None</span>, figsize=(<span class="params"><span class="number">2.5</span>, <span class="number">2.5</span></span>),</span></span><br><span class="line"><span class="params">                  cmap=<span class="string">&#x27;Reds&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Show heatmaps of matrices.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 使用SVG格式显示图像，以获得更清晰的视觉效果</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取矩阵的行数和列数，用于后续的图形网格布局</span></span><br><span class="line">    num_rows, num_cols, _, _ = matrices.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个图形和子图网格，根据矩阵的行数和列数进行布局</span></span><br><span class="line">    <span class="comment"># figsize参数用于设置图形的大小，sharex和sharey参数确保子图之间共享x和y轴的刻度，squeeze=False以保持子图数组的维度</span></span><br><span class="line">    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,</span><br><span class="line">                                 sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>, squeeze=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历子图网格和矩阵，将矩阵可视化在相应的子图上</span></span><br><span class="line">    <span class="keyword">for</span> i, (row_axes, row_matrices) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, matrices)):</span><br><span class="line">        <span class="keyword">for</span> j, (ax, matrix) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(row_axes, row_matrices)):</span><br><span class="line">            <span class="comment"># 在子图上绘制矩阵的图像</span></span><br><span class="line">            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)</span><br><span class="line">            <span class="comment"># 为最后一行的子图设置x轴标签</span></span><br><span class="line">            <span class="keyword">if</span> i == num_rows - <span class="number">1</span>:</span><br><span class="line">                ax.set_xlabel(xlabel)</span><br><span class="line">            <span class="comment"># 为第一列的子图设置y轴标签</span></span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                ax.set_ylabel(ylabel)</span><br><span class="line">            <span class="comment"># 如果提供了标题，则为子图设置标题</span></span><br><span class="line">            <span class="keyword">if</span> titles:</span><br><span class="line">                ax.set_title(titles[j])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在图形的右侧添加一个颜色条，用于表示矩阵值的含义</span></span><br><span class="line">    <span class="comment"># shrink参数用于调整颜色条的大小</span></span><br><span class="line">    fig.colorbar(pcm, ax=axes, shrink=<span class="number">0.6</span>)</span><br><span class="line">    d2l.plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    attention_weights = torch.eye(<span class="number">10</span>).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    show_heatmaps(attention_weights, xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/visualization.png" alt="image-20240916233247808"></p><h2 id="批量矩阵乘法">批量矩阵乘法</h2><p>为了更有效地计算小批量数据的注意力，我们可以使用<strong>批量矩阵乘法</strong></p><p>假设第一个小批量有n个矩阵，A1, A2, …, An，形状为a * b，第二个小批量也有n个矩阵B1, B2, …, Bn，形状为b * c，它们的批量矩阵乘法结果为A1B1, A2B2, …, AnBn，因此我们<strong>利用两个三维张量(A1, A2, …, An) (B1, B2, …, Bn)</strong>，<strong>形状分别为(n, a, b) (n, b, c)</strong>，<strong>批量矩阵乘法后的形状为(n, a, c)</strong></p><p><code>torch.bmm(...)</code>：批量矩阵乘法</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    a = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">    b = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">    <span class="built_in">print</span>(torch.bmm(a, b).shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注意力背景下的批量矩阵乘法</span></span><br><span class="line">    weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;weights:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;after unsqueeze:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(weights.unsqueeze(<span class="number">1</span>))  <span class="comment"># 通过添加维度从而可以利用批量矩阵乘法</span></span><br><span class="line">    values = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;values:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(values)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;after unsqueeze:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(values.unsqueeze(-<span class="number">1</span>))  <span class="comment"># 通过添加维度从而可以利用批量矩阵乘法</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;calculate:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>)))</span><br><span class="line">    <span class="built_in">print</span>(torch.mm(weights, values.T))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([2, 1, 6])</span><br><span class="line">weights:</span><br><span class="line">tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span><br><span class="line">         0.1000],</span><br><span class="line">        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span><br><span class="line">         0.1000]])</span><br><span class="line">after unsqueeze:</span><br><span class="line">tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span><br><span class="line">          0.1000, 0.1000]],</span><br><span class="line"></span><br><span class="line">        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,</span><br><span class="line">          0.1000, 0.1000]]])</span><br><span class="line">values:</span><br><span class="line">tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],</span><br><span class="line">        [10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]])</span><br><span class="line">after unsqueeze:</span><br><span class="line">tensor([[[ 0.],</span><br><span class="line">         [ 1.],</span><br><span class="line">         [ 2.],</span><br><span class="line">         [ 3.],</span><br><span class="line">         [ 4.],</span><br><span class="line">         [ 5.],</span><br><span class="line">         [ 6.],</span><br><span class="line">         [ 7.],</span><br><span class="line">         [ 8.],</span><br><span class="line">         [ 9.]],</span><br><span class="line"></span><br><span class="line">        [[10.],</span><br><span class="line">         [11.],</span><br><span class="line">         [12.],</span><br><span class="line">         [13.],</span><br><span class="line">         [14.],</span><br><span class="line">         [15.],</span><br><span class="line">         [16.],</span><br><span class="line">         [17.],</span><br><span class="line">         [18.],</span><br><span class="line">         [19.]]])</span><br><span class="line">calculate:</span><br><span class="line">tensor([[[ 4.5000]],</span><br><span class="line"></span><br><span class="line">        [[14.5000]]])</span><br><span class="line">tensor([[ 4.5000, 14.5000],</span><br><span class="line">        [ 4.5000, 14.5000]])</span><br></pre></td></tr></table></figure><h2 id="注意力汇聚">注意力汇聚</h2><p>目前，我们已经知道，<strong>查询（自主性提示）和键（非自主性提示）之间的交互形成了注意力汇聚</strong>；而<strong>注意力汇聚有选择地聚合了值</strong>（感官输入）从而确定了最后的输出。本节将介绍注意力汇聚的一些细节，以便了解其工作方式。</p><h3 id="Nadaraya-Watson核回归">Nadaraya-Watson核回归</h3><p>Nadaraya-Watson核回归模型可用于演示带有注意力机制的机器学习。</p><p>我们用这样一个函数来模拟生成数据</p><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/nad-wat.png" alt="image-20240917094615806"></p><p>其中噪声<em>ϵ ~ N(0, 0.5^2)</em></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> visualization <span class="keyword">import</span> show_heatmaps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成训练数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x ** <span class="number">0.8</span> + torch.normal(<span class="number">0</span>, <span class="number">0.5</span>, x.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">true_f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成真实数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x ** <span class="number">0.8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 生成数据集</span></span><br><span class="line">    n_train = <span class="number">50</span></span><br><span class="line">    x_train, _ = torch.sort(torch.rand(n_train) * <span class="number">5</span>)  <span class="comment"># 为了之后更好地可视化，将训练数据集排序</span></span><br><span class="line">    y_train = data_f(x_train)</span><br><span class="line">    x_test = torch.arange(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">    y_test = true_f(x_test)</span><br><span class="line">    n_test = <span class="built_in">len</span>(x_test)</span><br><span class="line">    <span class="built_in">print</span>(n_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">plot_kernel_reg</span>(<span class="params">y_hat</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;绘制样本，不带噪声项的真实函数记为Truth，预测出的函数记为Pred&quot;&quot;&quot;</span></span><br><span class="line">        d2l.plot(x_test, [y_test, y_hat], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, legend=[<span class="string">&#x27;Truth&#x27;</span>, <span class="string">&#x27;Pred&#x27;</span>],</span><br><span class="line">                 xlim=[<span class="number">0</span>, <span class="number">5</span>], ylim=[-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">        d2l.plt.plot(x_train, y_train, <span class="string">&#x27;o&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">        d2l.plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于平均汇聚</span></span><br><span class="line">    y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class="line">    plot_kernel_reg(y_hat)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于非参数的注意力汇聚</span></span><br><span class="line">    <span class="comment"># 将x_test重复n_train次，然后重塑为矩阵，以准备进行注意力计算</span></span><br><span class="line">    x_repeat = torch.repeat_interleave(x_test, n_train).reshape((-<span class="number">1</span>, n_train))</span><br><span class="line">    <span class="comment"># 计算注意力权重，通过计算x_repeat和x_train之间的差异，应用softmax函数进行标准化</span></span><br><span class="line">    attention_weights = nn.functional.softmax(-(x_repeat - x_train) ** <span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 通过加权训练数据的目标变量y_train来预测y_hat，权重为attention_weights</span></span><br><span class="line">    y_hat = torch.matmul(attention_weights, y_train)</span><br><span class="line">    plot_kernel_reg(y_hat)</span><br><span class="line">    show_heatmaps(attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),  <span class="comment"># unsqueeze添加两个维度</span></span><br><span class="line">                  xlabel=<span class="string">&#x27;sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;sorted testing inputs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于带参数的注意力汇聚</span></span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">NWKernelReg</span>(nn.Module):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">            <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">            <span class="variable language_">self</span>.w = nn.Parameter(torch.rand((<span class="number">1</span>, ), requires_grad=<span class="literal">True</span>))  <span class="comment"># 初始化一个可训练参数w</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values</span>):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            前向传播方法，实现自注意力机制的计算。</span></span><br><span class="line"><span class="string">            :param queries: 查询张量，模型的输入之一。</span></span><br><span class="line"><span class="string">            :param keys: 键张量，用于计算注意力权重。</span></span><br><span class="line"><span class="string">            :param values: 值张量，用于加权计算最终输出。</span></span><br><span class="line"><span class="string">            :return: 经过注意力机制计算后的输出张量。</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            <span class="comment"># 将查询张量(queries)重复展开，以匹配键张量(keys)的维度</span></span><br><span class="line">            queries = torch.repeat_interleave(queries, keys.shape[<span class="number">1</span>]).reshape((-<span class="number">1</span>, keys.shape[<span class="number">1</span>]))</span><br><span class="line">            <span class="comment"># 计算注意力权重</span></span><br><span class="line">            <span class="comment"># 通过softmax函数对计算出的权重进行归一化，使其和为1</span></span><br><span class="line">            <span class="variable language_">self</span>.attention_weights = nn.functional.softmax(-((queries - keys) * <span class="variable language_">self</span>.w) ** <span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 应用注意力权重</span></span><br><span class="line">            <span class="comment"># 通过加权求和得到最终的输出</span></span><br><span class="line">            <span class="keyword">return</span> torch.bmm(<span class="variable language_">self</span>.attention_weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据处理</span></span><br><span class="line">    <span class="comment"># 将训练集中的每个样本沿着第一维度重复n_train次，生成新的训练数据</span></span><br><span class="line">    <span class="comment"># 这样做的目的是为了在模型训练过程中增加样本多样性，增强模型的泛化能力</span></span><br><span class="line">    x_tile = x_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 同样的操作应用于标签数据，保证每个重复的样本仍然保留其正确的标签</span></span><br><span class="line">    <span class="comment"># 这是为了在增加样本多样性的同时，保持样本与其标签的一一对应关系</span></span><br><span class="line">    y_tile = y_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 通过屏蔽掉对角线元素，选择键值对</span></span><br><span class="line">    keys = x_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br><span class="line">    values = y_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型选择，且在带参数的注意力汇聚中使用平方误差损失函数和随机梯度下降优化</span></span><br><span class="line">    net = NWKernelReg()</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="comment"># 将模型参数的梯度归零</span></span><br><span class="line">        net.zero_grad()</span><br><span class="line">        <span class="comment"># 计算当前周期的损失</span></span><br><span class="line">        l = loss(net(x_train, keys, values), y_train)</span><br><span class="line">        <span class="comment"># 反向传播损失</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        <span class="comment"># 更新模型参数</span></span><br><span class="line">        trainer.step()</span><br><span class="line">        <span class="comment"># 可视化</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch %d, loss: %f&#x27;</span> % (epoch + <span class="number">1</span>, l.<span class="built_in">sum</span>()))</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, <span class="built_in">float</span>(l.<span class="built_in">sum</span>()))</span><br><span class="line">    <span class="comment"># 查看损失迭代情况</span></span><br><span class="line">    d2l.plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试</span></span><br><span class="line">    keys = x_train.repeat((n_test, <span class="number">1</span>))  <span class="comment"># 注意要用训练数据（带噪声的）去测试拟合，不然就完全一致了（因为带噪声的才是真实情况）</span></span><br><span class="line">    values = y_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line">    y_hat = net(x_test, keys, values).unsqueeze(<span class="number">1</span>).detach()</span><br><span class="line">    <span class="comment"># 可视化</span></span><br><span class="line">    plot_kernel_reg(y_hat)</span><br><span class="line">    <span class="comment"># 可视化注意力汇聚情况</span></span><br><span class="line">    show_heatmaps(net.attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">50</span><br></pre></td></tr></table></figure><p><code>基于平均汇聚的拟合结果图</code></p><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/mean.png" alt="image-20240917162713128"></p><p><code>基于非参数的注意力汇聚的拟合结果图</code></p><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/noarg.png" alt="image-20240917162746037"></p><p><code>基于非参数的注意力可视化</code></p><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/attention_noarg.png" alt="image-20240917162807533"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">epoch 1, loss: 45.345715</span><br><span class="line">epoch 2, loss: 16.821575</span><br><span class="line">epoch 3, loss: 16.805058</span><br><span class="line">epoch 4, loss: 16.788040</span><br><span class="line">epoch 5, loss: 16.770472</span><br></pre></td></tr></table></figure><p><code>损失值迭代结果图</code></p><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/loss.png" alt="image-20240917162903479"></p><p><code>基于参数的注意力汇聚拟合结果图</code></p><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/witharg.png" alt="image-20240917162922848"></p><p><code>基于参数的注意力可视化</code></p><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/attention_witharg.png" alt="image-20240917162939096"></p><h2 id="注意力评分函数">注意力评分函数</h2><p>在之前的例子中，代入softmax函数中的公式（高斯核的指数部分）可以视作评分函数。</p><h3 id="掩蔽softmax操作">掩蔽softmax操作</h3><p>在某些情况下，并非所有的值都应该被纳入注意力汇聚中，为了仅将有意义的词元作为值来获取注意力汇聚，可以指定一个有效序列长度（词元数量），以便过滤无意义值。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在最后一个轴上掩蔽元素来执行softmax操作</span></span><br><span class="line"><span class="string">    :param X: 三维张量</span></span><br><span class="line"><span class="string">    :param valid_lens: 一维或二维张量 </span></span><br><span class="line"><span class="string">    :return: softmax操作后的结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>  <span class="comment"># 最后一个维度代表特征数，因此在最后一个维度上操作即对每个特征都操作，且为了处理序列中不同元素的有效长度不同的情况</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:  <span class="comment"># 1D张量，扩展为2D张量，重复元素即为复制这个向量</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])  <span class="comment"># shape[1]为三维张量X第二个轴的元素个数</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 对序列进行掩码，将超过有效长度(valid_lens)的部分设置为极小值(value=-1e6), 以便后续处理中这些位置的影响会被忽略</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens, value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="comment"># reshape保证形状不变，dim=-1在最后一个维度上计算softmax</span></span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 随机生成一个有两个矩阵的张量</span></span><br><span class="line">    <span class="built_in">print</span>(masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>])))  <span class="comment"># 第一个矩阵有效长度2，第二个矩阵有效长度3</span></span><br><span class="line">    <span class="comment"># 也可以为矩阵的每一行也指定长度</span></span><br><span class="line">    <span class="built_in">print</span>(masked_softmax(torch.rand((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)), torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>]])))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[[0.4078, 0.5922, 0.0000, 0.0000],</span><br><span class="line">         [0.5290, 0.4710, 0.0000, 0.0000],</span><br><span class="line">         [0.5066, 0.4934, 0.0000, 0.0000]],</span><br><span class="line"></span><br><span class="line">        [[0.3932, 0.3709, 0.2359, 0.0000],</span><br><span class="line">         [0.4833, 0.2517, 0.2650, 0.0000],</span><br><span class="line">         [0.2944, 0.2732, 0.4324, 0.0000]]])</span><br><span class="line">tensor([[[1.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="line">         [0.5413, 0.4587, 0.0000, 0.0000],</span><br><span class="line">         [1.0000, 0.0000, 0.0000, 0.0000]],</span><br><span class="line"></span><br><span class="line">        [[0.3292, 0.3518, 0.3190, 0.0000],</span><br><span class="line">         [0.2038, 0.2110, 0.2027, 0.3825],</span><br><span class="line">         [0.6417, 0.3583, 0.0000, 0.0000]]])</span><br></pre></td></tr></table></figure><h3 id="加性注意力">加性注意力</h3><p>一般来说，当查询和键是不同长度的向量时，可以使用加性注意力作为评分函数。</p><p>一般做法：将查询和键连接起来后送入一个多层感知机，该感知机有一个隐藏层，隐藏层单元数是一个超参数，使用tanh作为激活函数。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在最后一个轴上掩蔽元素来执行softmax操作</span></span><br><span class="line"><span class="string">    :param X: 三维张量</span></span><br><span class="line"><span class="string">    :param valid_lens: 一维或二维张量 </span></span><br><span class="line"><span class="string">    :return: softmax操作后的结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>  <span class="comment"># 最后一个维度代表特征数，因此在最后一个维度上操作即对每个特征都操作，且为了处理序列中不同元素的有效长度不同的情况</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:  <span class="comment"># 1D张量，扩展为2D张量，重复元素即为复制这个向量</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])  <span class="comment"># shape[1]为三维张量X第二个轴的元素个数</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 对序列进行掩码，将超过有效长度(valid_lens)的部分设置为极小值(value=-1e6), 以便后续处理中这些位置的影响会被忽略</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens, value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="comment"># reshape保证形状不变，dim=-1在最后一个维度上计算softmax</span></span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AdditiveAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, queries_size, keys_size, num_hidden, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.w_q = nn.Linear(queries_size, num_hidden, bias=<span class="literal">False</span>)  <span class="comment"># 用于key的输入层到隐藏层变换</span></span><br><span class="line">        <span class="variable language_">self</span>.w_k = nn.Linear(keys_size, num_hidden, bias=<span class="literal">False</span>)  <span class="comment"># 用于query的输入层到隐藏层变换</span></span><br><span class="line">        <span class="variable language_">self</span>.w_v = nn.Linear(num_hidden, <span class="number">1</span>, bias=<span class="literal">False</span>)  <span class="comment"># 隐藏层到输出层，即打分</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># 传入暂退概率</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        <span class="comment"># 在多层感知机中对queries和keys作从输入层到隐藏层的变换</span></span><br><span class="line">        queries = <span class="variable language_">self</span>.w_q(queries)</span><br><span class="line">        keys = <span class="variable language_">self</span>.w_k(keys)</span><br><span class="line">        <span class="comment"># 假设queries: (batch_sz, n, d)-&gt;(batch_sz, n, 1, d); keys: (batch_sz, m, d)-&gt;(batch_sz, 1, m, d)</span></span><br><span class="line">        <span class="comment"># 广播机制求和后: (batch_sz, n, m, d)</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)  <span class="comment"># queries在第二维加入新维度，keys在第一维加入新维度，可以利用广播机制相加</span></span><br><span class="line">        features = torch.tanh(features)  <span class="comment"># 激活</span></span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span></span><br><span class="line">        <span class="comment"># 初始输出的一个scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span></span><br><span class="line">        scores = <span class="variable language_">self</span>.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(<span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.attention_weights), values)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 准备数据</span></span><br><span class="line">    <span class="comment"># 初始化查询、键、值和有效长度</span></span><br><span class="line">    <span class="comment"># 查询: 使用正态分布生成的随机数组</span></span><br><span class="line">    <span class="comment"># 键: 一组全为1的数组</span></span><br><span class="line">    <span class="comment"># 值: 从0到39的连续数字的数组，每个数字重复两次</span></span><br><span class="line">    queries, keys = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">20</span>)), torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># values的小批量，两个值矩阵是相同的</span></span><br><span class="line">    values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>).repeat(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">    attention = AdditiveAttention(keys_size=<span class="number">2</span>, queries_size=<span class="number">20</span>, num_hidden=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span><br><span class="line">    <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    attention.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 计算注意力权重</span></span><br><span class="line">    <span class="comment"># 注意：这里直接调用attention(queries, keys, values, valid_lens)，而不是使用.forward()方法</span></span><br><span class="line">    <span class="comment"># 因为在PyTorch中，如果模型没有处于训练模式，通常可以直接调用模型实例来进行预测或推理</span></span><br><span class="line">    <span class="built_in">print</span>(attention(queries, keys, values, valid_lens))  <span class="comment"># 不要错用成attention.forward()</span></span><br><span class="line">    <span class="comment"># 可视化注意力</span></span><br><span class="line">    d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)), xlabel=<span class="string">&#x27;keys&#x27;</span>, ylabel=<span class="string">&#x27;queries&#x27;</span>)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],</span><br><span class="line"></span><br><span class="line">        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=&lt;BmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/additive.png" alt="image-20240918153847881"></p><h3 id="缩放点积注意力">缩放点积注意力</h3><p>使用点积可以得到计算效率更高的评分函数，但是点积要求查询和键具有相同的长度。</p><p>缩放点积注意力的配分函数是</p><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/dot_math.png" alt="image-20240918155934947"></p><p>实践中，我们常通过小批量来提高计算效率，例如基于n个查询和m个键-值对计算注意力，其中查询和键的长度为<em>d</em>，值的长度为<em>v</em>，则查询<strong>Q</strong>、键<strong>K</strong>和值<strong>V</strong>的缩放点积注意力为</p><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/dot_attention.png" alt="image-20240918160519082"></p><p>具体推导见<a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#dot-product-attention">11.3. Attention Scoring Functions — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在最后一个轴上掩蔽元素来执行softmax操作</span></span><br><span class="line"><span class="string">    :param X: 三维张量</span></span><br><span class="line"><span class="string">    :param valid_lens: 一维或二维张量 </span></span><br><span class="line"><span class="string">    :return: softmax操作后的结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>  <span class="comment"># 最后一个维度代表特征数，因此在最后一个维度上操作即对每个特征都操作，且为了处理序列中不同元素的有效长度不同的情况</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:  <span class="comment"># 1D张量，扩展为2D张量，重复元素即为复制这个向量</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])  <span class="comment"># shape[1]为三维张量X第二个轴的元素个数</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 对序列进行掩码，将超过有效长度(valid_lens)的部分设置为极小值(value=-1e6), 以便后续处理中这些位置的影响会被忽略</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens, value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="comment"># reshape保证形状不变，dim=-1在最后一个维度上计算softmax</span></span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AdditiveAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, queries_size, keys_size, num_hidden, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.w_q = nn.Linear(queries_size, num_hidden, bias=<span class="literal">False</span>)  <span class="comment"># 用于key的输入层到隐藏层变换</span></span><br><span class="line">        <span class="variable language_">self</span>.w_k = nn.Linear(keys_size, num_hidden, bias=<span class="literal">False</span>)  <span class="comment"># 用于query的输入层到隐藏层变换</span></span><br><span class="line">        <span class="variable language_">self</span>.w_v = nn.Linear(num_hidden, <span class="number">1</span>, bias=<span class="literal">False</span>)  <span class="comment"># 隐藏层到输出层，即打分</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># 传入暂退概率</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        <span class="comment"># 在多层感知机中对queries和keys作从输入层到隐藏层的变换</span></span><br><span class="line">        queries = <span class="variable language_">self</span>.w_q(queries)</span><br><span class="line">        keys = <span class="variable language_">self</span>.w_k(keys)</span><br><span class="line">        <span class="comment"># 假设queries: (batch_sz, n, d)-&gt;(batch_sz, n, 1, d); keys: (batch_sz, m, d)-&gt;(batch_sz, 1, m, d)</span></span><br><span class="line">        <span class="comment"># 广播机制求和后: (batch_sz, n, m, d)</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)  <span class="comment"># queries在第二维加入新维度，keys在第一维加入新维度，可以利用广播机制相加</span></span><br><span class="line">        features = torch.tanh(features)  <span class="comment"># 激活</span></span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span></span><br><span class="line">        <span class="comment"># 初始输出的一个scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span></span><br><span class="line">        scores = <span class="variable language_">self</span>.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(<span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.attention_weights), values)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(DotProductAttention, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># queries的形状：(batch_size，查询的个数，d)</span></span><br><span class="line">    <span class="comment"># keys的形状：(batch_size，“键－值”对的个数，d)</span></span><br><span class="line">    <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">    <span class="comment"># valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]  <span class="comment"># d取queries最后一个维度的大小, 即查询和键的长度</span></span><br><span class="line">        <span class="comment"># 计算评分</span></span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>, <span class="number">2</span>)) / math.sqrt(d)  <span class="comment"># 套公式, 不过要注意keys的转置不能用.T，会报错(?)，要用transpose</span></span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(<span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.attention_weights), values)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># # 准备数据</span></span><br><span class="line">    <span class="comment"># # 初始化查询、键、值和有效长度</span></span><br><span class="line">    <span class="comment"># # 查询: 使用正态分布生成的随机数组</span></span><br><span class="line">    <span class="comment"># # 键: 一组全为1的数组</span></span><br><span class="line">    <span class="comment"># # 值: 从0到39的连续数字的数组，每个数字重复两次</span></span><br><span class="line">    queries, keys = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">20</span>)), torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># values的小批量，两个值矩阵是相同的</span></span><br><span class="line">    values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>).repeat(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缩放点积注意力</span></span><br><span class="line"></span><br><span class="line">    queries = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    attention = DotProductAttention(dropout=<span class="number">0.5</span>)</span><br><span class="line">    attention.<span class="built_in">eval</span>()  <span class="comment"># 不进入训练模式，而是评估模式</span></span><br><span class="line">    <span class="built_in">print</span>(attention(queries, keys, values, valid_lens))</span><br><span class="line">    d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)), xlabel=<span class="string">&#x27;keys&#x27;</span>, ylabel=<span class="string">&#x27;queries&#x27;</span>)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],</span><br><span class="line"></span><br><span class="line">        [[10.0000, 11.0000, 12.0000, 13.0000]]])</span><br></pre></td></tr></table></figure><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/dot_attention_vis.png" alt="image-20240918170053279"></p><h2 id="Bahdanau注意力">Bahdanau注意力</h2><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/bahdanau_model.png" alt="image-20240918172150856"></p><p>首先定义Bahdanau注意力，实现循环神经网络编码器-解码器；事实上，我们只需要重新定义解码器即可。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AttentionDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>接下来，实现带有Bahdanau注意力的循环神经网络解码器</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TBD 先去看循环神经网络</span></span><br></pre></td></tr></table></figure><h2 id="多头注意力">多头注意力</h2><p>模型原理及理论知识详见<a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#multi-head-attention">11.5. Multi-Head Attention — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><p>下面主要介绍实现</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了能使多个头并行计算，定义两个转置函数方便MultiHeadAttention类调用</span></span><br><span class="line"><span class="comment"># 具体来说，transpose_output反转了transpose_qkv的操作</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多头注意力的并行计算而改变形状&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，</span></span><br><span class="line">    <span class="comment"># num_hidden / num_heads)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hidden / num_heads)</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hidden / num_heads)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_output</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># 为什么写成X.permute(0, 2, 1, 3).reshape(X.shape[0], X.shape[1], -1)不行</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hidden, num_heads, dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.attention = d2l.DotProductAttention(dropout=dropout)</span><br><span class="line">        <span class="variable language_">self</span>.w_q = nn.Linear(query_size, num_hidden, bias=bias)</span><br><span class="line">        <span class="variable language_">self</span>.w_k = nn.Linear(key_size, num_hidden, bias=bias)</span><br><span class="line">        <span class="variable language_">self</span>.w_v = nn.Linear(value_size, num_hidden, bias=bias)</span><br><span class="line">        <span class="variable language_">self</span>.w_o = nn.Linear(num_hidden, num_hidden, bias=bias)</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        <span class="comment"># 使用不同的线性变换（w_q, w_k, w_v）处理查询、键和值之后，再根据多头注意力机制的需要，重新排列这些处理后的数据</span></span><br><span class="line">        queries = transpose_qkv(<span class="variable language_">self</span>.w_q(queries), <span class="variable language_">self</span>.num_heads)</span><br><span class="line">        keys = transpose_qkv(<span class="variable language_">self</span>.w_k(keys), <span class="variable language_">self</span>.num_heads)</span><br><span class="line">        values = transpose_qkv(<span class="variable language_">self</span>.w_v(values), <span class="variable language_">self</span>.num_heads)</span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 在轴0，将第一项（标量或者矢量）复制num_heads次，</span></span><br><span class="line">            <span class="comment"># 然后如此复制第二项，以此类推</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, repeats=<span class="variable language_">self</span>.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output的形状:(batch_size*num_heads, 查询的个数, num_hidden / num_heads)</span></span><br><span class="line">        output = <span class="variable language_">self</span>.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output_concat的形状: (batch_size, 查询的个数, num_hidden)</span></span><br><span class="line">        output_concat = transpose_output(output, <span class="variable language_">self</span>.num_heads)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w_o(output_concat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 测试MultiHeadAttention类</span></span><br><span class="line">    num_hidden, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">    attention = MultiHeadAttention(num_hidden, num_hidden, num_hidden, num_hidden, num_heads, dropout=<span class="number">0.5</span>)</span><br><span class="line">    <span class="built_in">print</span>(attention.<span class="built_in">eval</span>())</span><br><span class="line">    batch_size, num_queries = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">    num_kvpairs, valid_lens = <span class="number">6</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">    X = torch.ones((batch_size, num_queries, num_hidden))</span><br><span class="line">    Y = torch.ones((batch_size, num_kvpairs, num_hidden))</span><br><span class="line">    <span class="built_in">print</span>(attention(X, Y, Y, valid_lens).shape)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">  )</span><br><span class="line">  (w_q): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (w_k): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (w_v): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (w_o): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">)</span><br><span class="line">torch.Size([2, 4, 100])</span><br></pre></td></tr></table></figure><h2 id="自注意力和位置编码">自注意力和位置编码</h2><h3 id="自注意力">自注意力</h3><p>有了注意力机制之后，我们将词元序列输入注意力池中，以便同一组词元同时充当查询、键和值。具体来说，每个查询都会关注所有的键-值对并生成一个注意力输出。由于查询、键和值来自同一组输入，因此被称为<strong>自注意力</strong>。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 初始化隐藏层神经元数量、头的数量</span></span><br><span class="line">    num_hidden, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">    attention = d2l.MultiHeadAttention(num_hidden, num_hidden, num_hidden, num_hidden, num_heads, dropout=<span class="number">0.5</span>)</span><br><span class="line">    <span class="built_in">print</span>(attention.<span class="built_in">eval</span>())  <span class="comment"># 不训练</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化批量大小，查询的数量，有效词元长度</span></span><br><span class="line">    batch_size, num_queries, valid_lens = <span class="number">6</span>, <span class="number">6</span>, torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">    X = torch.ones((batch_size, num_queries, num_hidden))  <span class="comment"># 6个矩阵，每个矩阵都是6 * 100的</span></span><br><span class="line">    tmp = attention(X, X, X, valid_lens)</span><br><span class="line">    <span class="built_in">print</span>(tmp)</span><br><span class="line">    <span class="built_in">print</span>(tmp.shape)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (W_k): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (W_v): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (W_o): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">)</span><br><span class="line">tensor([[[-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735]],</span><br><span class="line"></span><br><span class="line">        [[-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735]],</span><br><span class="line"></span><br><span class="line">        [[-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735]],</span><br><span class="line"></span><br><span class="line">        [[-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735]],</span><br><span class="line"></span><br><span class="line">        [[-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735]],</span><br><span class="line"></span><br><span class="line">        [[-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735],</span><br><span class="line">         [-0.1328,  0.1216, -0.0917,  ...,  0.0770, -0.4573, -0.2735]]],</span><br><span class="line">       grad_fn=&lt;UnsafeViewBackward0&gt;)</span><br><span class="line">torch.Size([6, 6, 100])</span><br></pre></td></tr></table></figure><h3 id="位置编码">位置编码</h3><p>自注意力为了并行计算放弃了顺序处理元素，为了使用序列的顺序信息，可以在输入表示中添加位置编码。</p><p>接下来介绍基于正弦函数和余弦函数的固定位置编码。</p><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/pos_sincos.png" alt="image-20240919155109358"></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;位置编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_hidden, dropout, max_len=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 创建一个足够长的P, 准备之后填充正弦值和余弦值</span></span><br><span class="line">        <span class="variable language_">self</span>.P = torch.zeros((<span class="number">1</span>, max_len, num_hidden))</span><br><span class="line">        <span class="comment"># 套公式</span></span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(-<span class="number">1</span>, <span class="number">1</span>) / \</span><br><span class="line">            torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(<span class="number">0</span>, num_hidden, <span class="number">2</span>, dtype=torch.float32) / num_hidden)</span><br><span class="line">        <span class="comment"># 填充</span></span><br><span class="line">        <span class="variable language_">self</span>.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X)</span><br><span class="line">        <span class="variable language_">self</span>.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;将输入张量X与位置编码矩阵相加，使模型能够感知序列中元素的位置信息&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 相加时只取X也有的维度大小部分，因为self.P可能与X的维度不同，所以只取X维度相同的部分，即从0取到X.shape[1]</span></span><br><span class="line">        X = X + <span class="variable language_">self</span>.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="comment"># .to(X.device)确保所有参与运算的张量都在同一个设备上，避免跨设备运算导致的错误或性能问题。</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 定义序列长度和编码维度</span></span><br><span class="line">    num_steps, num_encoding = <span class="number">60</span>, <span class="number">32</span></span><br><span class="line">    <span class="comment"># 初始化位置编码对象</span></span><br><span class="line">    pos_encode = PositionalEncoding(num_encoding, <span class="number">0</span>)</span><br><span class="line">    pos_encode.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 处理一个全零张量</span></span><br><span class="line">    X = pos_encode(torch.zeros((<span class="number">1</span>, num_steps, num_encoding)))</span><br><span class="line">    <span class="comment"># 获取处理后的张量</span></span><br><span class="line">    P = pos_encode.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">    <span class="comment"># 可视化</span></span><br><span class="line">    d2l.plot(torch.arange(num_steps), P[<span class="number">0</span>, :, <span class="number">6</span>:<span class="number">10</span>].T, xlabel=<span class="string">&#x27;Row (position)&#x27;</span>,</span><br><span class="line">             figsize=(<span class="number">6</span>, <span class="number">2.5</span>), legend=[<span class="string">&quot;Col %d&quot;</span> % d <span class="keyword">for</span> d <span class="keyword">in</span> torch.arange(<span class="number">6</span>, <span class="number">10</span>)])</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><p><img src="%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/pos_sincos_vis.png" alt="image-20240919161230542"></p><h3 id="绝对位置信息和相对位置信息">绝对位置信息和相对位置信息</h3><p>主要为理论介绍，详见<a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html#absolute-positional-information">11.6. Self-Attention and Positional Encoding — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h1>Transformer</h1><p>TBD（等学完循环神经网络回来补~）</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络</title>
      <link href="/ymhui.github.io/2024/09/16/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/ymhui.github.io/2024/09/16/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1>写在前面</h1><blockquote><p>参考书籍</p></blockquote><p><a href="http://www.d2l.ai">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><h1>卷积神经网络</h1><p><strong>source code</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui/DeepLearning: Deep Learning with pytorch (github.com)</a></p><p><strong>use git to clone</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p><p><code>/CNN</code></p><blockquote><p>cross_correlation.py<a href="http://fill.py">fill.py</a>multi_pipe.pypool_layer.py<a href="http://LeNet.py">LeNet.py</a></p></blockquote><h2 id="多层感知机的局限性">多层感知机的局限性</h2><p>详见<a href="https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html#constraining-the-mlp">7.1. From Fully Connected Layers to Convolutions — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="卷积">卷积</h2><p>详见<a href="https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html#convolutions">7.1. From Fully Connected Layers to Convolutions — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="图像卷积">图像卷积</h2><p>卷积神经网络主要用于探索图像数据，因此此处以图象为例</p><h3 id="互相关运算">互相关运算</h3><p>严格地讲，卷积层是不严谨的，因为它所表达的运算实际上是<strong>互相关运算</strong>，而不是卷积运算。在这种卷积层中，输入张量和核张量通过互相关运算得到输出张量。</p><p>理论部分详见<a href="https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#the-cross-correlation-operation">7.2. Cross-Correlation Operation — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">    K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">    <span class="built_in">print</span>(corr2d(X, K))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[19., 25.],</span><br><span class="line">        [37., 43.]])</span><br></pre></td></tr></table></figure><h3 id="卷积层">卷积层</h3><p>卷积层对<strong>输入和卷积核权重进行互相关运算</strong>，并在添加标量偏置之后产生输出。所以卷积层中两个被训练的参数是卷积核权重和标量偏置。因此当我们初始化参数时，要对卷积核权重进行随机初始化，同时给偏置一个初值。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="comment"># kernel_size (tuple): 卷积核的大小，用于初始化权重矩阵。</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.rand(kernel_size))  <span class="comment"># 随机初始化权重</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))  <span class="comment"># 为偏置赋初值</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 应用卷积操作并加上偏置项</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, <span class="variable language_">self</span>.weight) + <span class="variable language_">self</span>.bias</span><br></pre></td></tr></table></figure><h3 id="边缘检测">边缘检测</h3><p>下面介绍卷积层的一个简单应用：通过找到像素变化的位置，检测图像中不同颜色的边缘。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 边缘检测</span></span><br><span class="line">  X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))  <span class="comment"># 构造一个6 * 8的黑白像素图象，0黑1白</span></span><br><span class="line">   X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">   <span class="built_in">print</span>(X)</span><br><span class="line">   <span class="comment"># 接下来构造一个长为1宽为2的卷积核，当相邻元素相同时，输出0</span></span><br><span class="line">   K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])  <span class="comment"># [1, 0] * K = 1, [0, 1] * K = -1</span></span><br><span class="line">   <span class="comment"># 执行互相关运算</span></span><br><span class="line">   Y = corr2d(X, K)</span><br><span class="line">   <span class="comment"># 输出中1为白色到黑色的边缘, -1为黑色到白色的边缘</span></span><br><span class="line">   <span class="built_in">print</span>(Y)  <span class="comment"># 根据输出发现这种方法只能检测出垂直边缘，水平边缘消失了</span></span><br><span class="line">   <span class="comment"># 更直观的感受一下</span></span><br><span class="line">   <span class="built_in">print</span>(corr2d(X.T, K))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span><br><span class="line">tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</span><br><span class="line">tensor([[0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.]])</span><br></pre></td></tr></table></figure><h3 id="学习卷积核">学习卷积核</h3><p>如果我们只需寻找黑白边缘，那么以上[1, -1]的边缘检测器足以。然而，当有了更复杂数值的卷积核，或者连续的卷积层时，我们不可能手动设计滤波器，因此考虑<strong>通过学习由X生成Y的卷积核</strong>。</p><p>现在我们尝试仅通过查看&quot;输入-输出&quot;对来学习由X生成Y的卷积核</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，它有1个输出通道和形状为(1, 2)的卷积核</span></span><br><span class="line">    conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)  <span class="comment"># 卷积核权重存在这里</span></span><br><span class="line">    <span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高、宽）</span></span><br><span class="line">    <span class="comment"># 批量大小和通道都为1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">    Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">    <span class="comment"># 准备训练</span></span><br><span class="line">    num_epochs, lr = <span class="number">10</span>, <span class="number">0.03</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;before training:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;weight:&quot;</span>, conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        Y_hat = conv2d(X)</span><br><span class="line">        l = (Y - Y_hat) ** <span class="number">2</span></span><br><span class="line">        conv2d.zero_grad()</span><br><span class="line">        l.<span class="built_in">sum</span>().backward()  <span class="comment"># 先梯度归零，再反向传播</span></span><br><span class="line">        <span class="comment"># 更新权重</span></span><br><span class="line">        conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.5</span>f&#125;</span>&#x27;</span>)  <span class="comment"># 可视化损失变化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 看一下迭代后的权重如何</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;after training:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;weight:&quot;</span>, conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">before training:</span><br><span class="line">weight: tensor([[0.1613, 0.4069]])</span><br><span class="line">epoch 1, loss 19.97194</span><br><span class="line">epoch 2, loss 9.29642</span><br><span class="line">epoch 3, loss 4.52200</span><br><span class="line">epoch 4, loss 2.30929</span><br><span class="line">epoch 5, loss 1.23841</span><br><span class="line">epoch 6, loss 0.69447</span><br><span class="line">epoch 7, loss 0.40428</span><br><span class="line">epoch 8, loss 0.24228</span><br><span class="line">epoch 9, loss 0.14831</span><br><span class="line">epoch 10, loss 0.09216</span><br><span class="line">after training:</span><br><span class="line">weight: tensor([[ 1.0176, -0.9565]])</span><br></pre></td></tr></table></figure><h2 id="填充和步幅">填充和步幅</h2><p>在应用多层卷积时，我们常常丢失边缘像素；解决这个问题的简单办法即为<strong>填充</strong>。</p><h3 id="填充">填充</h3><p>原理部分详见<a href="https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html#padding">7.3. Padding — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个计算卷积层的函数</span></span><br><span class="line"><span class="comment"># 为函数初始化卷积层权重，并对输入和输出提高和缩减相应的维度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># (1, 1)表示批量大小和通道数都是1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># 省略前两个维度批量大小和通道数</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 这里每边都填充了1行或1列，因此共添加2行或2列</span></span><br><span class="line">    conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">    X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">    Y = comp_conv2d(conv2d, X)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line">    conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">    Y = comp_conv2d(conv2d, X)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([8, 8])</span><br><span class="line">torch.Size([8, 8])</span><br></pre></td></tr></table></figure><h3 id="步幅">步幅</h3><p><a href="https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html#stride">7.3. Stride — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 步幅 2</span></span><br><span class="line">   conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">   Y = comp_conv2d(conv2d, X)</span><br><span class="line">   <span class="built_in">print</span>(Y.shape)</span><br><span class="line">   conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">   Y = comp_conv2d(conv2d, X)</span><br><span class="line">   <span class="built_in">print</span>(Y.shape)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([4, 4])</span><br><span class="line">torch.Size([2, 2])</span><br></pre></td></tr></table></figure><h2 id="多输入多输出通道">多输入多输出通道</h2><p>之前我们一直在讨论单通道时的情况，但实际情况往往是更加复杂的。例如彩色图像往往采用<strong>标准RGB通道来代表红、绿、蓝</strong>，这就已经有<strong>三个通道</strong>了。</p><h3 id="多输入通道">多输入通道</h3><p>当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多输入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 先遍历X和K的第0个维度(通道维度)，再把它们加在一起</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">                      [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">    K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line">    <span class="built_in">print</span>(corr2d_multi_in(X, K))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[ 56.,  72.],</span><br><span class="line">        [104., 120.]])</span><br></pre></td></tr></table></figure><h3 id="多输出通道">多输出通道</h3><p>目前尽管我们已经实现了多输入通道，但是输出通道还是只有一个。在当下的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多输入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 先遍历X和K的第0个维度(通道维度)，再把它们加在一起</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多输出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 遍历K的第0个维度，每次都把一个卷积层应用于X(执行互相关运算)，然后把结果收集起来</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;multi in:&quot;</span>)</span><br><span class="line">    X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">                      [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">    K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line">    <span class="built_in">print</span>(corr2d_multi_in(X, K))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;multi out:&quot;</span>)</span><br><span class="line">    K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line">    <span class="built_in">print</span>(K.shape)</span><br><span class="line">    <span class="built_in">print</span>(corr2d_multi_in_out(X, K))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">multi in:</span><br><span class="line">tensor([[ 56.,  72.],</span><br><span class="line">        [104., 120.]])</span><br><span class="line">multi out:</span><br><span class="line">torch.Size([3, 2, 2, 2])</span><br><span class="line">tensor([[[ 56.,  72.],</span><br><span class="line">         [104., 120.]],</span><br><span class="line"></span><br><span class="line">        [[ 76., 100.],</span><br><span class="line">         [148., 172.]],</span><br><span class="line"></span><br><span class="line">        [[ 96., 128.],</span><br><span class="line">         [192., 224.]]])</span><br></pre></td></tr></table></figure><h3 id="1-1卷积层">1 * 1卷积层</h3><p>作用详见<a href="https://d2l.ai/chapter_convolutional-neural-networks/channels.html#times-1-convolutional-layer">7.4. Multiple Input and Multiple Output Channels — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多输入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 先遍历X和K的第0个维度(通道维度)，再把它们加在一起</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多输出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 遍历K的第0个维度，每次都把一个卷积层应用于X(执行互相关运算)，然后把结果收集起来</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 * 1卷积</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    <span class="comment"># 全连接层的矩阵乘法</span></span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;1 * 1 correlation:&quot;</span>)</span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">    K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">    Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()))</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1 * 1 correlation:</span><br><span class="line">0.0</span><br></pre></td></tr></table></figure><h2 id="汇聚层">汇聚层</h2><p>通常当我们处理图像时，我们希望<strong>逐渐降低隐藏表示的空间分辨率、聚集信息</strong>，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。</p><p>而机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”），所以<strong>最后一层的神经元应该对整个输入的全局敏感</strong>。<strong>通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标</strong>，同时<strong>将卷积图层的所有优势保留在中间层</strong>。</p><p>原理部分见<a href="https://d2l.ai/chapter_convolutional-neural-networks/pooling.html#pooling">7.5. Pooling — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大汇聚层 or 平均汇聚层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;汇聚层实现，mode选择最大/平均&quot;&quot;&quot;</span></span><br><span class="line">    p_h , p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">    <span class="built_in">print</span>(pool2d(X, (<span class="number">2</span>, <span class="number">2</span>)))  <span class="comment"># 输出最大汇聚层</span></span><br><span class="line">    <span class="built_in">print</span>(pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>))  <span class="comment"># 输出平均汇聚层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 填充和步幅</span></span><br><span class="line">    X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">    pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(pool2d(X))</span><br><span class="line">    pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(pool2d(X))</span><br><span class="line">    pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(pool2d(X))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 多个通道</span></span><br><span class="line">    X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">    pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(pool2d(X))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[4., 5.],</span><br><span class="line">        [7., 8.]])</span><br><span class="line">tensor([[2., 3.],</span><br><span class="line">        [5., 6.]])</span><br><span class="line">tensor([[[[10.]]]])</span><br><span class="line">tensor([[[[ 5.,  7.],</span><br><span class="line">          [13., 15.]]]])</span><br><span class="line">tensor([[[[ 5.,  7.],</span><br><span class="line">          [13., 15.]]]])</span><br><span class="line">tensor([[[[ 5.,  7.],</span><br><span class="line">          [13., 15.]],</span><br><span class="line"></span><br><span class="line">         [[ 6.,  8.],</span><br><span class="line">          [14., 16.]]]])</span><br></pre></td></tr></table></figure><h2 id="LeNet">LeNet</h2><p>到目前，我们已经掌握了构建一个完整卷积神经网络的所需组件。之前在处理Fashion-MNIST数据集时，我们使用了softmax回归和多层感知机模型，但这样需要将28 * 28的图像展平为一个784维的向量，破坏了其空间结构。而现在，通过卷积层，我们可以保留图像中的空间结构。</p><p><strong>LeNet</strong>是一种卷积神经网络之一，是一种监督学习。</p><p>主要有两部分组成：</p><ul><li>卷积编码器：由两个卷积层组成</li><li>全连接层密集块：由三个全连接层组成</li></ul><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评估</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    <span class="comment"># 准确预测的数量， 总预测的数量</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                <span class="comment"># Bert微调所需</span></span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型，使用Xavier随机初始化模型参数，使用交叉熵损失函数和小批量随机梯度下降</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU训练模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;training on&quot;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练损失之和， 训练准确率之和， 样本数</span></span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>(): <span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化结果</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># LeNet卷积神经网络</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line">    X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">        X = layer(X)</span><br><span class="line">        <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&quot;output shape: &quot;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    batch_size = <span class="number">256</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class="line">    lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">    train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Conv2d output shape:  torch.Size([1, 6, 28, 28])</span><br><span class="line">Sigmoid output shape:  torch.Size([1, 6, 28, 28])</span><br><span class="line">AvgPool2d output shape:  torch.Size([1, 6, 14, 14])</span><br><span class="line">Conv2d output shape:  torch.Size([1, 16, 10, 10])</span><br><span class="line">Sigmoid output shape:  torch.Size([1, 16, 10, 10])</span><br><span class="line">AvgPool2d output shape:  torch.Size([1, 16, 5, 5])</span><br><span class="line">Flatten output shape:  torch.Size([1, 400])</span><br><span class="line">Linear output shape:  torch.Size([1, 120])</span><br><span class="line">Sigmoid output shape:  torch.Size([1, 120])</span><br><span class="line">Linear output shape:  torch.Size([1, 84])</span><br><span class="line">Sigmoid output shape:  torch.Size([1, 84])</span><br><span class="line">Linear output shape:  torch.Size([1, 10])</span><br><span class="line">training on cpu</span><br><span class="line">loss 0.477, train acc 0.820, test acc 0.807</span><br><span class="line"> 7299.6 examples/sec on cpu</span><br></pre></td></tr></table></figure><p><img src="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/lenet.png" alt="image-20240927151211020"></p><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多层感知机</title>
      <link href="/ymhui.github.io/2024/09/15/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/ymhui.github.io/2024/09/15/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h1>写在前面</h1><blockquote><p>参考书籍</p></blockquote><p><a href="http://www.d2l.ai">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><h1>多层感知机</h1><p><strong>source code</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui/DeepLearning: Deep Learning with pytorch (github.com)</a></p><p><strong>use git to clone</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p><p><code>/MLP</code></p><blockquote><p><a href="http://mlp.py">mlp.py</a>mlp_self.pymlp_lib.py<a href="http://polynomial.py">polynomial.py</a>high_dim.pydropout_self.pydropout_lib.py</p></blockquote><h2 id="为什么需要非线性模型">为什么需要非线性模型</h2><p>因为线性模型可能出错，<strong>线性即意味着做出了单调性假设</strong>，但现实世界并不总是满足单调性的，即使满足单调性，也<strong>不一定是线性变化的</strong>。</p><p>例如，我们想要根据体温预测死亡率。对体温高于37摄氏度的人来说，温度越高风险越大；然而，对体温低于37摄氏度的人来说，温度越高风险就越低。在这种情况下，或许还可以通过一些预处理解决问题，比如以37摄氏度为切入点，以温差为特征。</p><p>但如果是在处理图像呢？假设我们以像素点的强度来区分A和B，那么一个像素点的增强是否一定意味着似然性的加强呢？这个像素点的强度又是否有明确的转折点呢？反转一张图像，图片类别不变，然而单个像素点的强度可能会发生天翻地覆的变化…在这样一个世界中，<strong>只用线性方法注定会失败。</strong></p><h2 id="加入隐藏层">加入隐藏层</h2><p>首先看一张多层感知机的示意图</p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/mlp.png" alt="image-20240915210449670"></p><p>输入层即为我们输入样本的地方(<code>features</code>), 输出层即为产生结果的地方(<code>labels</code>)。</p><p>这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。因此，这个多层感知机中的层数为2。注意，这两个层都是<strong>全连接的（即相邻层之间的任意两个神经元互相连接）</strong>。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。</p><p><em>注：全连接的开销比较大，设一层有<code>p</code>个神经元，一层有<code>q</code>个神经元，则全连接的开销就是<code>O(pq)</code></em></p><h2 id="从线性到非线性">从线性到非线性</h2><p>和之前一样，我们仍使用一个矩阵**X~nd~**来表示n个样本的小批量。理论部分详见<a href="https://d2l.ai/chapter_multilayer-perceptrons/mlp.html#from-linear-to-nonlinear">5.1. Multilayer Perceptrons — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="激活函数">激活函数</h2><p>激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。下面介绍一些常见的激活函数。</p><h3 id="ReLU函数">ReLU函数</h3><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="built_in">$</span>ReLU(x) = max(x, 0)<span class="built_in">$</span></span><br></pre></td></tr></table></figure><p>通俗地说，ReLU函数通过将相应的活性值设为0，<strong>仅保留正元素并丢弃所有负元素</strong>。我们可视化一下</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>():</span><br><span class="line">    x = torch.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, <span class="number">0.1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y = torch.relu(x)  <span class="comment"># ReLU激活函数</span></span><br><span class="line">    <span class="comment"># .detach()方法用于创建一个新的Tensor，该Tensor从当前计算图中分离出来，但仍指向相同的数据</span></span><br><span class="line">    d2l.plot(x.detach(), y.detach(), <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;relu(x)&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))  <span class="comment"># 绘制图像, x和y的数据通过.detach()方法从计算图中分离，避免梯度计算</span></span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/relu.png" alt="image-20240915222502339"></p><p>再看一下导数</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 接上</span></span><br><span class="line">   y.backward(torch.ones_like(x), retain_graph=<span class="literal">True</span>)</span><br><span class="line"> d2l.plot(x.detach(), x.grad, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;grad of relu&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br><span class="line">   d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/relu_grad.png" alt="image-20240915223107659"></p><h3 id="sigmoid函数">sigmoid函数</h3><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/sigmoid_math.png" alt="image-20240915223924739"></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>():</span><br><span class="line">    x = torch.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, <span class="number">0.1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y = torch.sigmoid(x)</span><br><span class="line">    d2l.plot(x.detach(), y.detach(), <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;sigmoid(x)&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br><span class="line">    d2l.plt.show()</span><br><span class="line">    <span class="comment"># 求导数</span></span><br><span class="line">    y.backward(torch.ones_like(x), retain_graph=<span class="literal">True</span>)</span><br><span class="line">    d2l.plot(x.detach(), x.grad, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;grad of sigmoid&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><p><code>sigmoid原函数</code></p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/sigmoid.png" alt="image-20240915223711050"></p><p><code>sigmoid导函数</code></p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/sigmoid_grad.png" alt="image-20240915223751736"></p><h3 id="tanh函数">tanh函数</h3><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/tanh_math.png" alt="image-20240915224048702"></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>():</span><br><span class="line">    x = torch.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, <span class="number">0.1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y = torch.tanh(x)</span><br><span class="line">    d2l.plot(x.detach(), y.detach(), <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;tanh(x)&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br><span class="line">    d2l.plt.show()</span><br><span class="line">    <span class="comment"># 求导</span></span><br><span class="line">    y.backward(torch.ones_like(x), retain_graph=<span class="literal">True</span>)</span><br><span class="line">    d2l.plot(x.detach(), x.grad, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;grad of tanh&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><p><code>tanh原函数</code></p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/tanh.png" alt="image-20240915224553761"></p><p><code>tanh导函数</code></p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/tanh_grad.png" alt="image-20240915224558576"></p><p><s>恭喜你已经了解了多层感知机的所有知识，现在自己动手实现一个吧！</s></p><h2 id="从0-开始实现多层感知机">从0?开始实现多层感知机</h2><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="string">&quot;&quot;&quot;从零开始写一个多层感知机&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Fashion-MNIST图像数据集的输入是28*28的灰度图像，输出是10个类别</span></span><br><span class="line"><span class="comment"># 不妨实现一个256个隐藏层节点的MLP</span></span><br><span class="line">number_inputs, number_outputs, number_hidden = <span class="number">28</span> * <span class="number">28</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;relu激活函数&quot;&quot;&quot;</span></span><br><span class="line">    zero = torch.zeros_like(x)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(x, zero)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    plt.switch_backend(<span class="string">&#x27;Agg&#x27;</span>)  <span class="comment"># 为显示图片</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 继续使用Fashion-MNIST图像数据集</span></span><br><span class="line">    batch_size = <span class="number">256</span>  <span class="comment"># 批量样本大小</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)  <span class="comment"># 加载数据集并划分为训练迭代器和测试迭代器</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化模型参数</span></span><br><span class="line">    <span class="comment"># 注意到我们的多层感知机一共需要两层的参数（参数是一个权重矩阵和一个偏移向量），两层分别是输入层到隐藏层和隐藏层到输出层</span></span><br><span class="line">    <span class="comment"># 初始化为小幅度随机数, shape=(num_inputs, num_hidden)，因为是input层到hidden层，所以形状是input * hidden</span></span><br><span class="line">    w1 = nn.Parameter(torch.randn(number_inputs, number_hidden, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 初始化为0, 长度为num_hidden</span></span><br><span class="line">    b1 = nn.Parameter(torch.zeros(number_hidden, requires_grad=<span class="literal">True</span>))</span><br><span class="line">    <span class="comment"># 初始化为小幅度随机数, shape=(num_hidden, num_outputs)，因为是hidden层到output层，所以形状是hidden * output</span></span><br><span class="line">    w2 = nn.Parameter(torch.randn(number_hidden, number_outputs, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 初始化为0, 长度为num_outputs</span></span><br><span class="line">    b2 = nn.Parameter(torch.zeros(number_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line">    <span class="comment"># 参数列表为 w1, b1, w2, b2</span></span><br><span class="line">    params = [w1, b1, w2, b2]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;定义模型&quot;&quot;&quot;</span></span><br><span class="line">        x = x.reshape((-<span class="number">1</span>, number_inputs))  <span class="comment"># 将输入x重塑为二维数组，形状为(-1, number_inputs)，-1表示自动计算样本数量, 第二维大小为number_inputs</span></span><br><span class="line">        hidden = relu(x @ w1 + b1)  <span class="comment"># 计算隐藏层输出：使用ReLU激活函数对 输入与权重w1矩阵做乘法后加上偏置b1的结果 进行激活</span></span><br><span class="line">        <span class="keyword">return</span> hidden @ w2 + b2  <span class="comment"># @表示矩阵乘法, 等价于torch.matmul(hidden, w2) + b2</span></span><br><span class="line">    <span class="comment"># 在线性回归一文从零实现softmax板块中已实现损失函数，此处直接调用现有库api</span></span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    num_epochs, lr = <span class="number">10</span>, <span class="number">0.1</span>  <span class="comment"># 迭代轮数，学习率</span></span><br><span class="line">    updator = torch.optim.SGD(params, lr=lr)</span><br><span class="line">    <span class="comment"># 模型的训练与softmax一致，因此此处直接调用现有库api</span></span><br><span class="line">    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updator)  <span class="comment"># 该函数用于训练模型，输入参数为网络模型、训练数据、测试数据、损失函数、迭代轮数、优化器</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测 / 检查预测结果</span></span><br><span class="line">    d2l.predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><p><em>环境原因尚未看到图片输出</em></p><p>可以先参考<a href="https://d2l.ai/chapter_multilayer-perceptrons/mlp-implementation.html#id2">5.2. Implementation of Multilayer Perceptrons — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h2 id="多层感知机的简洁实现">多层感知机的简洁实现</h2><p>直接使用现有框架实现多层感知机</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    初始化神经网络模型中的权重</span></span><br><span class="line"><span class="string">    :param m: 传入模块</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 检查传入模块是否为全连接</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="comment"># 如果是，就以均值为0，标准差为0.01的正态分布对权重进行初始化</span></span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    batch_size = <span class="number">256</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义使用的模型</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Flatten(),  <span class="comment"># 将输入展平为向量</span></span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),  <span class="comment"># 输入为28*28=784，隐藏层为256</span></span><br><span class="line">        nn.ReLU(),  <span class="comment"># 激活函数为ReLU</span></span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 隐藏层为256，输出为10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    num_epochs, lr = <span class="number">10</span>, <span class="number">0.1</span></span><br><span class="line">    <span class="comment"># 直接从pytorch的优化算法类获取优化器SGD，传入网络参数和学习率</span></span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=lr)  <span class="comment"># 用于训练神经网络net，设置学习率为lr，优化器将根据此学习率和反向传播计算的梯度来更新net的所有可训练参数</span></span><br><span class="line">    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p><em>环境原因尚未看到图片输出</em></p><h1>拟合过程中的问题</h1><h2 id="模型选择">模型选择</h2><p>字面意思，选择拟合效果最好的模型；但这又引出了新问题，该如何验证比较呢？</p><h3 id="验证集">验证集</h3><p>原则上，在我们确定所有的超参数之前，我们不希望用到测试集。如果我们<strong>在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险</strong>，那就麻烦大了。</p><p>然而，我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差；这时就需要一个验证集。</p><p>常见做法是将我们的数据分成三份，除了训练和测试数据集之外，还<strong>增加一个验证数据集，也叫验证集</strong>，不过现实中验证集和测试集的边界相当模糊，在我们的学习过程中，凡是涉及所谓<em>预测精确度</em>的地方，<strong>除非明确说明，使用的都是验证集</strong>（也就是我们划分的其实是训练集和验证集，并不提供测试集）</p><h3 id="K折交叉验证">K折交叉验证</h3><p>当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是<strong>采用K折交叉验证</strong>。这里，原始训练数据被分成K个<strong>不重叠的子集</strong>。然后<strong>执行K次模型训练和验证</strong>，每次<strong>在K − 1个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证</strong>。最后，通过<strong>对K次实验的结果取平均</strong>来估计训练和验证误差。</p><h2 id="欠拟合">欠拟合</h2><p>当<strong>模型过于简单，表达能力不足</strong>，学习到的特征过少以至于来了一个在某方面有相似的样本就被接纳了，这种现象称为<strong>欠拟合</strong>。</p><p><em>欠拟合的训练误差和验证误差都很大</em>，但它们两者本身差距不大</p><h2 id="过拟合">过拟合</h2><p>将模型<strong>在训练数据上拟合得比在潜在分布中更接近</strong>的现象称为过拟合，说人话就是模型在训练集上达到了近乎完美的水平，而在测试集上误差却比较大。</p><p>比如当模型复杂度过高时，它记住了<strong>过多的</strong>样本的<strong>特征</strong>，然而其中有些是不那么必要的，以至于只要不满足特征的样本就被排斥在外了。</p><p><em>过拟合的训练误差很小，但验证误差很大，即训练误差 &lt;&lt; 验证误差</em>，<strong>此时要小心过拟合</strong></p><h2 id="模型复杂性">模型复杂性</h2><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/relate.png" alt="image-20240916103836204"></p><h2 id="数据集大小">数据集大小</h2><p><strong>训练数据的样本越少，越容易发生过拟合</strong>；随着训练数据量提升，泛化误差通常会减小。给出更多的数据，我们会尝试拟合一个更复杂的模型，而<strong>当数据较少时，简单的模型可能更有效</strong>。需要认识到，<strong>只有当训练数据量达到数千时，深度学习才会优于线性模型。</strong></p><h2 id="多项式回归">多项式回归</h2><p>我们尝试拟合这样一个多项式（用标准式生成数据，再用数据拟合模型）</p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/poly.png" alt="image-20240916104044506"></p><p>通过这样一个例子来了解欠拟合和过拟合的实际情况</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上的模型损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 损失的综合，样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="comment"># 将输入数据X通过神经网络net进行前向传播，得到输出结果out</span></span><br><span class="line">        out = net(X)</span><br><span class="line">        <span class="comment"># 将标签y的形状重塑为模型输出out的形状，确保维度匹配</span></span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        <span class="comment"># 计算模型输出out与标签y之间的损失函数值，并求和</span></span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        <span class="comment"># 更新评估指标，累加损失总和及样本数量</span></span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), y.numel())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算并返回两个metric元素的除法结果</span></span><br><span class="line">    <span class="comment"># 此函数解释两个metric元素之间的比例关系，其中metric假设为一个包含两个元素的列表或元组</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features, test_features, train_labels, test_labels, num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param train_features:</span></span><br><span class="line"><span class="string">    :param test_features:</span></span><br><span class="line"><span class="string">    :param train_labels:</span></span><br><span class="line"><span class="string">    :param test_labels:</span></span><br><span class="line"><span class="string">    :param num_epochs:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 多项式中已有偏置，所以不必再设置</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, <span class="number">1</span>, bias=<span class="literal">False</span>))</span><br><span class="line">    <span class="comment"># 设置批量大小，训练迭代器，测试迭代器，训练器</span></span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels.reshape(-<span class="number">1</span>, <span class="number">1</span>)), batch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features, test_labels.reshape(-<span class="number">1</span>, <span class="number">1</span>)), batch_size, is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 初始化一个动画对象animation，用于绘制训练和测试损失曲线</span></span><br><span class="line">    animation = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">1e-3</span>, <span class="number">1e2</span>],</span><br><span class="line">                             legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)  <span class="comment"># 对每个epoch，调用d2l.train_epoch_ch3训练模型</span></span><br><span class="line">        <span class="comment"># 方便可视化</span></span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 若为首个epoch或当前epoch加1能被20整除，记录训练和测试损失并添加到动画中</span></span><br><span class="line">            animation.add(epoch + <span class="number">1</span>, (evaluate_loss(net, train_iter, loss), evaluate_loss(net, test_iter, loss)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出权重</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;weight:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(net[<span class="number">0</span>].weight.data.numpy())</span><br><span class="line">    <span class="keyword">return</span> net[<span class="number">0</span>].weight.data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 拟合时假定阶数为20(其实是19，还有0次项)</span></span><br><span class="line">    max_degree = <span class="number">20</span></span><br><span class="line">    <span class="comment"># 生成一个三阶多项式</span></span><br><span class="line">    n_train, n_test = <span class="number">100</span>, <span class="number">100</span></span><br><span class="line">    true_w = torch.zeros(max_degree)</span><br><span class="line">    true_w[<span class="number">0</span>:<span class="number">4</span>] = torch.tensor([<span class="number">5</span>, <span class="number">1.2</span>, -<span class="number">3.4</span>, <span class="number">5.6</span>])  <span class="comment"># 多项式的系数 x^0 x^1 x^2 x^3</span></span><br><span class="line">    <span class="comment"># print(true_w)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成初始数据x</span></span><br><span class="line">    features = torch.randn((n_train + n_test, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 随机打乱</span></span><br><span class="line">    random_indices = torch.randperm(n_train + n_test)</span><br><span class="line">    features = features[random_indices]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(features)</span></span><br><span class="line">    <span class="comment"># 生成x的幂次</span></span><br><span class="line">    poly_features = torch.<span class="built_in">pow</span>(features, torch.arange(max_degree).reshape(<span class="number">1</span>, -<span class="number">1</span>))  <span class="comment"># 构造多项式特征, 幂次从0开始</span></span><br><span class="line">    <span class="comment"># print(poly_features)</span></span><br><span class="line">    <span class="comment"># 消减梯度</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">        poly_features[:, i] /= math.gamma(i + <span class="number">1</span>)  <span class="comment"># gamma(i+1) = (i+1)!, 防止梯度增加过快</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算标签值，y = w_0 + w_1 * x + w_2 * x^2 + w_3 * x^3 + ... = w · x</span></span><br><span class="line">    <span class="comment"># 一共有多个样本，所以x的幂次样本poly_features是一个矩阵</span></span><br><span class="line">    labels = torch.mm(poly_features, true_w.reshape(-<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># true_w是一个行向量，所以用.reshape(-1,1)变成列向量</span></span><br><span class="line">    <span class="comment"># 加上噪声</span></span><br><span class="line">    labels += torch.normal(<span class="number">0</span>, <span class="number">0.1</span>, labels.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 看一眼数据</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data slices:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(features[:<span class="number">2</span>])</span><br><span class="line">    <span class="built_in">print</span>(poly_features[:<span class="number">2</span>, :])</span><br><span class="line">    <span class="built_in">print</span>(labels[:<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="comment"># 数据集前面是验证集，后面是训练集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 先看正常拟合</span></span><br><span class="line">    <span class="comment"># 取前四个特征，即w_0 + w_1 * x + w_2 * x^2 + w_3 * x^3，正好是目标的阶</span></span><br><span class="line">    predict_w = train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_test:, :<span class="number">4</span>], labels[:n_train], labels[n_test:])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;correct mistake:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict_w - true_w[:<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 欠拟合</span></span><br><span class="line">    <span class="comment"># 因为实际是一个三级多项式，当我们尝试用线性模型（即一次函数）去拟合时，会出问题</span></span><br><span class="line">    <span class="comment"># 只取特征的前两行，即w_0 + w_1 * x</span></span><br><span class="line">    predict_w = train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_test:, :<span class="number">2</span>], labels[:n_train], labels[n_test:])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;linear mistake:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict_w - true_w[:<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 过拟合</span></span><br><span class="line">    <span class="comment"># 当模型过于复杂时可能发生过拟合，比如我们取前8个特征</span></span><br><span class="line">    <span class="comment"># 即w_0 + w_1 * x + w_2 * x^2 + w_3 * x^3 + w_4 * x^4 + w_5 * x^5 + w_6 * x^6 + w_7 * x^7七阶多项式</span></span><br><span class="line">    predict_w = train(poly_features[:n_train, :<span class="number">8</span>], poly_features[n_test:, :<span class="number">8</span>], labels[:n_train], labels[n_test:])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;overfit mistake:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict_w - true_w[:<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取所有特征</span></span><br><span class="line">    predict_w = train(poly_features[:n_train, :], poly_features[n_test:, :], labels[:n_train], labels[n_test:])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;all mistake:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict_w - true_w)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><p><code>正常</code></p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/correct.png" alt="image-20240916111436755"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weight:</span><br><span class="line">[[ 5.010181   1.2250326 -3.421302   5.54533  ]]</span><br><span class="line">correct mistake:</span><br><span class="line">tensor([[ 0.0102,  0.0250, -0.0213, -0.0547]])</span><br></pre></td></tr></table></figure><p><code>欠拟合</code></p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/not.png" alt="image-20240916111522061"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weight:</span><br><span class="line">[[3.5309384 4.0524364]]</span><br><span class="line">linear mistake:</span><br><span class="line">tensor([[-1.4691,  2.8524]])</span><br></pre></td></tr></table></figure><p><code>两种过拟合</code></p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/over1.png" alt="image-20240916111540879"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weight:</span><br><span class="line">[[ 4.937372    1.3379664  -3.0077322   5.0077944  -1.2465584   1.0296985</span><br><span class="line">  -0.5775194  -0.10814942]]</span><br><span class="line">overfit mistake:</span><br><span class="line">tensor([[-0.0626,  0.1380,  0.3923, -0.5922, -1.2466,  1.0297, -0.5775, -0.1081]])</span><br></pre></td></tr></table></figure><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/over2.png" alt="image-20240916111554095"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weight:</span><br><span class="line">[[ 4.9291635   1.3562328  -2.958946    4.904419   -1.4154936   1.2838099</span><br><span class="line">  -0.36761254  0.3409793  -0.16273078 -0.04508495 -0.09691837  0.2131685</span><br><span class="line">   0.18586184 -0.18636724 -0.20589598 -0.01916531  0.06223011  0.0851168</span><br><span class="line">   0.1374422   0.13517812]]</span><br><span class="line">all mistake:</span><br><span class="line">tensor([[-0.0708,  0.1562,  0.4411, -0.6956, -1.4155,  1.2838, -0.3676,  0.3410,</span><br><span class="line">         -0.1627, -0.0451, -0.0969,  0.2132,  0.1859, -0.1864, -0.2059, -0.0192,</span><br><span class="line">          0.0622,  0.0851,  0.1374,  0.1352]])</span><br></pre></td></tr></table></figure><h1>缓解过拟合问题的方法</h1><h2 id="权重衰减">权重衰减</h2><p>为了解决过拟合问题，引入一些<strong>正则化</strong>模型的技术。</p><p>权重衰减是最广泛使用的正则化技术之一，也被称为<code>L2正则化</code>。这项技术通过衡量函数与零的距离来判断模型复杂度</p><h3 id="一个高维线性回归的例子">一个高维线性回归的例子</h3><p>我们尝试拟合这样一个函数</p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/highdim_math.png" alt="image-20240916130958343"></p><p>为了尽可能地体现出过拟合，假设样本共有200个特征（200维），数据集是只有20个样本的小样本</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">使用正则化技术缓解过拟合</span></span><br><span class="line"><span class="string">模型具有200维，使用只包含20个样本的小样本</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>(<span class="params">number_features</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    随机初始化模型参数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 随机化权重向量</span></span><br><span class="line">    w = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (number_features, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 随机化偏移量, 是一个形状为(1,)的零张量</span></span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># print(b.shape)</span></span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义L2范数惩罚</span></span><br><span class="line"><span class="string">    :param w: 权重向量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> w.<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>() / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">num_features, train_iter, test_iter, batch_size, regular=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param regular: 正则系数</span></span><br><span class="line"><span class="string">    :return: 拟合后的权重与偏移</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.03</span>  <span class="comment"># 训练轮数, 学习率</span></span><br><span class="line">    <span class="comment"># 初始化模型参数</span></span><br><span class="line">    w, b = init_params(num_features)</span><br><span class="line">    <span class="comment"># 选择模型，损失函数，优化器，学习率</span></span><br><span class="line">    net = <span class="keyword">lambda</span> x: d2l.linreg(x, w, b)  <span class="comment"># 定义一个匿名函数，需要参数x</span></span><br><span class="line">    loss = d2l.squared_loss  <span class="comment"># 平方损失</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>, xlim=[<span class="number">5</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> train_iter:  <span class="comment"># train_iter的结构: (feature, label)</span></span><br><span class="line">            <span class="comment"># 选择性添加L2惩罚项</span></span><br><span class="line">            l = loss(net(x), y) + regular * l2_penalty(w)  <span class="comment"># net(x) = predict, loss(predict, label)为损失, 即loss(net(x), y)</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size=batch_size)</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(w.data[:<span class="number">5</span>].numpy())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;bias:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(b.data.numpy())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;L2:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(torch.norm(w).item())</span><br><span class="line">    <span class="keyword">return</span> w.data, b.data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    n_train, n_test = <span class="number">20</span>, <span class="number">100</span></span><br><span class="line">    num_inputs = <span class="number">200</span>  <span class="comment"># 200维(200个变量x)</span></span><br><span class="line">    batch_size = <span class="number">5</span></span><br><span class="line">    <span class="comment"># 函数真实的权重和偏移</span></span><br><span class="line">    true_w = torch.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span></span><br><span class="line">    true_b = <span class="number">0.05</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;true_w:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(true_w[:<span class="number">10</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;true_b:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(true_b)</span><br><span class="line">    <span class="comment"># 先得到数据，再生成迭代器</span></span><br><span class="line">    <span class="comment"># synthetic_data函数以N(0, 0.01^2)的高斯噪声为背景噪声，生成数据时自动添加</span></span><br><span class="line">    train_data = d2l.synthetic_data(true_w, true_b, n_train)  <span class="comment"># synthetic_data函数生成数据, 传入权重，偏移和生成数量</span></span><br><span class="line">    train_iter = d2l.load_array(train_data, batch_size=batch_size)</span><br><span class="line">    test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class="line">    test_iter = d2l.load_array(test_data, batch_size=batch_size, is_train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练，分为开正则化和不开正则化</span></span><br><span class="line">    <span class="comment"># 先不开正则化</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;----------no regularization----------&#x27;</span>)</span><br><span class="line">    pred_w, pred_b = train(num_inputs, train_iter, test_iter, batch_size, regular=<span class="number">0</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;pred_w - true_w:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(pred_w[:<span class="number">5</span>] - true_w[:<span class="number">5</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;pred_b - true_b:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(pred_b - true_b)</span><br><span class="line">    d2l.plt.show()  <span class="comment"># 可视化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 再开正则化</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;----------with regularization----------&#x27;</span>)</span><br><span class="line">    pred_w, pred_b = train(num_inputs, train_iter, test_iter, batch_size, regular=<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;pred_w - true_w:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(pred_w[:<span class="number">5</span>] - true_w[:<span class="number">5</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;pred_b - true_b:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(pred_b - true_b)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">true_w:</span><br><span class="line">tensor([[0.0100],</span><br><span class="line">        [0.0100],</span><br><span class="line">        [0.0100],</span><br><span class="line">        [0.0100],</span><br><span class="line">        [0.0100],</span><br><span class="line">        [0.0100],</span><br><span class="line">        [0.0100],</span><br><span class="line">        [0.0100],</span><br><span class="line">        [0.0100],</span><br><span class="line">        [0.0100]])</span><br><span class="line">true_b:</span><br><span class="line">0.05</span><br><span class="line">----------no regularization----------</span><br><span class="line">weight:</span><br><span class="line">[[-0.3840736 ]</span><br><span class="line"> [ 1.1716033 ]</span><br><span class="line"> [ 0.5462728 ]</span><br><span class="line"> [ 0.55637556]</span><br><span class="line"> [ 0.4533415 ]]</span><br><span class="line">bias:</span><br><span class="line">[0.0539746]</span><br><span class="line">L2:</span><br><span class="line">13.647695541381836</span><br><span class="line">pred_w - true_w:</span><br><span class="line">tensor([[-0.3941],</span><br><span class="line">        [ 1.1616],</span><br><span class="line">        [ 0.5363],</span><br><span class="line">        [ 0.5464],</span><br><span class="line">        [ 0.4433]])</span><br><span class="line">pred_b - true_b:</span><br><span class="line">tensor([0.0040])</span><br><span class="line">----------with regularization----------</span><br><span class="line">weight:</span><br><span class="line">[[-2.0754803e-03]</span><br><span class="line"> [ 4.0748098e-04]</span><br><span class="line"> [ 3.2745302e-06]</span><br><span class="line"> [ 1.6372477e-03]</span><br><span class="line"> [ 4.4756951e-03]]</span><br><span class="line">bias:</span><br><span class="line">[0.03499107]</span><br><span class="line">L2:</span><br><span class="line">0.036297813057899475</span><br><span class="line">pred_w - true_w:</span><br><span class="line">tensor([[-0.0121],</span><br><span class="line">        [-0.0096],</span><br><span class="line">        [-0.0100],</span><br><span class="line">        [-0.0084],</span><br><span class="line">        [-0.0055]])</span><br><span class="line">pred_b - true_b:</span><br><span class="line">tensor([-0.0150])</span><br></pre></td></tr></table></figure><p><code>不开正则化</code></p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/noregular.png" alt="image-20240916143614324"></p><p><code>开正则化</code></p><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/withregular.png" alt="image-20240916143623727"></p><h2 id="暂退法">暂退法</h2><h3 id="偏差-方差平衡">偏差-方差平衡</h3><p>偏差指的是预测值与实际值之间的差距，方差是指多次预测结果之间的差距。<strong>两者可能同时大，但不太可能同时小</strong>。对于<strong>线性模型而言，拟合较好的会偏向偏差一侧，而方差较小</strong>，因为他们只能表示一小类函数，不容易考虑特征之间的相互作用，因而在不同的随机数据样本上可以得出相似的结果；<strong>神经网络则刚好相反，偏向于方差一侧，而偏差较小</strong>，它们不局限于查看单个特征，而是擅长挖掘特征之间潜在的联系，但这也导致了较高的过拟合风险，可能会依赖一些虚假关联。</p><p>当面对更多的特征而样本不足时，线性模型往往会过拟合；当给出更多样本而不是特征，通常线性模型不会过拟合，这是以牺牲学习特征之间的交互换来的。</p><p>不过不幸的是，即使我们有比特征多得多的样本，深度神经网络也有可能过拟合</p><h3 id="从0实现暂退法">从0实现暂退法</h3><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="string">&quot;&quot;&quot;从零实现暂退法&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X, drop_prob</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在隐藏层应用暂退法，用于神经网络训练中防止过拟合</span></span><br><span class="line"><span class="string">    该函数以dropout的概率丢弃张量输入X中的元素，重新缩放剩余部分即除以 1 - dropout</span></span><br><span class="line"><span class="string">    :param X: 张量输入</span></span><br><span class="line"><span class="string">    :param drop_prob: 概率</span></span><br><span class="line"><span class="string">    :return: 丢弃、放缩后的结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= drop_prob &lt;= <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> drop_prob == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    mask = (torch.rand(X.shape) &gt; drop_prob).<span class="built_in">float</span>()  <span class="comment"># 生成一个形状与X相同、元素大于drop_prob的随机数掩码mask, 不大于的位置为0</span></span><br><span class="line">    <span class="keyword">return</span> mask * X / (<span class="number">1</span> - drop_prob)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="comment"># 为每一层分别设置暂退概率</span></span><br><span class="line">drop_out1, drop_out2 = <span class="number">0.2</span>, <span class="number">0.5</span>  <span class="comment"># 将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;实现一个两层感知机&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2, is_training=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_inputs = num_inputs</span><br><span class="line">        <span class="variable language_">self</span>.training = is_training</span><br><span class="line">        <span class="variable language_">self</span>.lin1 = nn.Linear(num_inputs, num_hiddens1)  <span class="comment"># 第一个隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.lin2 = nn.Linear(num_hiddens1, num_hiddens2)  <span class="comment"># 第二个隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.lin3 = nn.Linear(num_hiddens2, num_outputs)  <span class="comment"># 输出层</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        H1 = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.lin1(X.reshape((-<span class="number">1</span>, <span class="variable language_">self</span>.num_inputs))))</span><br><span class="line">        <span class="comment"># 训练模型时启用dropout</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line">            H1 = dropout_layer(H1, drop_out1)</span><br><span class="line">        H2 = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.lin2(H1))</span><br><span class="line">        <span class="comment"># 同理，训练模型时启用dropout, 防止测试时也启用dropout</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line">            H2 = dropout_layer(H2, drop_out2)</span><br><span class="line">        output = <span class="variable language_">self</span>.lin3(H2)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 测试暂退函数</span></span><br><span class="line">    A = torch.arange(<span class="number">25</span>).reshape(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;before:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(A)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;after:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(dropout_layer(A, <span class="number">0</span>))</span><br><span class="line">    <span class="built_in">print</span>(dropout_layer(A, <span class="number">0.5</span>))</span><br><span class="line">    <span class="built_in">print</span>(dropout_layer(A, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型参数，依然使用Fashion-MNIST数据集</span></span><br><span class="line">    <span class="comment"># 输入层28*28个神经元，输出层10个神经元, 有两个隐藏层，每个隐藏层有256个神经元</span></span><br><span class="line">    num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型</span></span><br><span class="line">    net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练和测试</span></span><br><span class="line">    num_epochs, lr, batch_size = <span class="number">10</span>, <span class="number">0.5</span>, <span class="number">256</span>  <span class="comment"># 迭代轮数， 学习率， 批量大小</span></span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)  <span class="comment"># 损失函数</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)  <span class="comment"># 获取训练迭代器，测试迭代器</span></span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=lr)  <span class="comment"># 定义优化器</span></span><br><span class="line">    <span class="comment"># 传参顺序为：模型，训练集，测试集，损失函数，迭代轮数，优化器</span></span><br><span class="line">    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">before:</span><br><span class="line">tensor([[ 0,  1,  2,  3,  4],</span><br><span class="line">        [ 5,  6,  7,  8,  9],</span><br><span class="line">        [10, 11, 12, 13, 14],</span><br><span class="line">        [15, 16, 17, 18, 19],</span><br><span class="line">        [20, 21, 22, 23, 24]])</span><br><span class="line">after:</span><br><span class="line">tensor([[ 0.,  1.,  2.,  3.,  4.],</span><br><span class="line">        [ 5.,  6.,  7.,  8.,  9.],</span><br><span class="line">        [10., 11., 12., 13., 14.],</span><br><span class="line">        [15., 16., 17., 18., 19.],</span><br><span class="line">        [20., 21., 22., 23., 24.]])</span><br><span class="line">tensor([[ 0.,  2.,  4.,  0.,  0.],</span><br><span class="line">        [10.,  0., 14., 16., 18.],</span><br><span class="line">        [ 0.,  0.,  0.,  0., 28.],</span><br><span class="line">        [ 0., 32.,  0.,  0., 38.],</span><br><span class="line">        [ 0.,  0., 44.,  0.,  0.]])</span><br><span class="line">tensor([[0, 0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0, 0]])</span><br></pre></td></tr></table></figure><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/dropout.png" alt="image-20240916200547990"></p><h3 id="利用现有框架复现暂退法">利用现有框架复现暂退法</h3><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="string">&quot;&quot;&quot;利用深度学习框架高级api实现暂退法，请先阅读dropout_self.py&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    初始化权重，针对此例</span></span><br><span class="line"><span class="string">    :param model: 传入模型</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(model) == nn.Linear:</span><br><span class="line">        nn.init.normal_(model.weight, <span class="number">0</span>, <span class="number">0.01</span>)  <span class="comment"># 等价于 nn.init.normal_(model.weight, std=0.01) mean默认0，std默认1.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 初始化二层感知机的参数，使用Fashion-MNIST数据集</span></span><br><span class="line">    num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line">    drop_out1, drop_out2 = <span class="number">0.2</span>, <span class="number">0.5</span>  <span class="comment"># 将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        <span class="comment"># 该函数(nn.Flatten())将多维输入张量展平为一维，常用于神经网络中连接卷积层与全连接层</span></span><br><span class="line">        nn.Flatten(),  <span class="comment"># 将输入展平</span></span><br><span class="line">        <span class="comment"># 第一层隐藏层</span></span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_out1),</span><br><span class="line">        <span class="comment"># 第二层隐藏层</span></span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_out2),</span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    net.apply(init_weights)  <span class="comment"># 初始化权重</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练和测试</span></span><br><span class="line">    num_epochs, lr, batch_size = <span class="number">10</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br><span class="line">    d2l.plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><p><img src="%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/dropout_lib.png" alt="image-20240916204520798"></p><h1>前向传播、反向传播和计算图</h1><p>详见<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.14_backprop">3.14 正向传播、反向传播和计算图 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><p>或<a href="https://d2l.ai/chapter_multilayer-perceptrons/backprop.html#forward-propagation-backward-propagation-and-computational-graphs">5.3. Forward Propagation, Backward Propagation, and Computational Graphs — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h1>数值稳定性和模型初始化</h1><p>详见<a href="https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#sec-numerical-stability">4.8. 数值稳定性和模型初始化 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p><h1>实操：预测房价</h1><p>TBD</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性神经网络</title>
      <link href="/ymhui.github.io/2024/09/15/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/ymhui.github.io/2024/09/15/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1>线性神经网络</h1><p><strong>source code</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning">NJU-ymhui/DeepLearning: Deep Learning with pytorch (github.com)</a></p><p><strong>use git to clone</strong>: <a href="https://github.com/NJU-ymhui/DeepLearning.git">https://github.com/NJU-ymhui/DeepLearning.git</a></p><p><code>/Linear</code></p><blockquote><p>vectorization_acceleration.py<a href="http://timer.py">timer.py</a><a href="http://normal.py">normal.py</a>linear_regression_self.pylinear_regression_lib.py<a href="http://image.py">image.py</a>softmax_self.py</p></blockquote><h2 id="写在前面">写在前面</h2><blockquote><p>参考书籍</p></blockquote><p><a href="http://www.d2l.ai">Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola. <em>Dive into Deep Learning</em>. 2020.</a></p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">简介 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><h2 id="线性回归">线性回归</h2><h3 id="线性模型">线性模型</h3><p>线性模型的基础是<strong>线性假设</strong>，即因变量=各自变量的加权和+偏移。</p><h4 id="公式">公式</h4><blockquote><p><em>ŷ = w~1~x~1~ + w~2~x~2~ + … + w~n~x~n~ + b</em>=&gt;ŷ = <strong>w^T^x</strong> + b</p></blockquote><p>其中向量<strong>x</strong>对应的是一个样本数据，如果将一个数据集的全部样本纳入计算，将有：</p><blockquote><p><strong>ŷ</strong> = <strong>Xw</strong> + b</p></blockquote><p>给定训练数据特征X和对应的已知标签y，线性回归的目标是<strong>找到一组权重向量w和偏置</strong>b：当给定从X的同分布中取样的<strong>新样本特征时</strong>，这组权重向量和偏置能够<strong>使得新样本预测标签的误差尽可能小</strong>。</p><h4 id="噪声项">噪声项</h4><p>真实情况中，即便给定数据集预测结果的最佳模型是线性的，也<strong>一定会存在一些不可避免的观测误差</strong>，所以加入一个<strong>噪声项</strong>来考虑观测误差带来的影响。</p><p>即：ŷ = <strong>w^T^x</strong> + b + ϵ，<code>ϵ</code>为噪声项</p><h4 id="损失函数">损失函数</h4><p>我们需要一个指标来度量我们**对于数据集拟合的好坏，**损失函数(loss function)应运而生。损失函数可以量化实际值与预测值之间的差距，<strong>通常选择非负值作为损失，损失越小预测越准，为0时为完美预测</strong>。</p><p>回归问题中常用的损失函数是<strong>平方误差函数</strong></p><h5 id="平方误差函数">平方误差函数</h5><p><em>ŷ为预测值，y为实际值</em></p><blockquote><p>l(<strong>w</strong>, b) = 0.5 * (ŷ - y)^2^// 对于单个样本</p></blockquote><p>对于全体样本，我们还需要计算一个<strong>损失均值，即各样本损失求和的均值</strong></p><p>在训练模型时，我们期望找到的一组参数(<strong>w</strong>, b)能够<strong>最小化损失均值</strong>（不针对某个样本，而是针对全体数据集）</p><h4 id="解析解">解析解</h4><p>由于线性回归是一个相对比较简单的优化问题，因此它的解(<strong>w</strong>, b)可以直接被公式表达，称之为<strong>解析解</strong>；遗憾的是，大部分其他模型并不具备解析解。</p><h5 id="线性回归的解析解-w">线性回归的解析解 w^*^</h5><p>即最小化损失均值，也就是最小化总损失(<strong>y</strong> - <strong>ŷ</strong>)</p><p>其解析解<strong>w^<em>^**为: **w^</em>^</strong> = (<strong>X^T^X</strong>)^-1^ <strong>X^T^y</strong></p><h4 id="随机梯度下降">随机梯度下降</h4><p>这是一种优化算法，内容比较复杂，详见<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression?id=_3-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95">3.1 线性回归 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><h3 id="矢量化加速">矢量化加速</h3><p>对计算进行矢量化，可以利用线性代数库，而不用在Python中编写开销高昂的for循环。</p><p>首先定义一个计时器</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Timer</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;记录多次运行时间&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="variable language_">self</span>.times = []</span><br><span class="line">    <span class="variable language_">self</span>.start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">start</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;启动计时器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="variable language_">self</span>.tik = time.time()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stop</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;停止计时器并将时间记录在列表中&quot;&quot;&quot;</span></span><br><span class="line">    <span class="variable language_">self</span>.times.append(time.time() - <span class="variable language_">self</span>.tik)</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.times[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">avg</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回平均时间&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="variable language_">self</span>.times) / <span class="built_in">len</span>(<span class="variable language_">self</span>.times)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sum</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时间总和&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="variable language_">self</span>.times)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回累计时间&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.array(<span class="variable language_">self</span>.times).cumsum().tolist()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compare_forloop_tensor</span>():</span><br><span class="line">    <span class="comment"># 对比for循环和张量加</span></span><br><span class="line">    a = torch.arange(<span class="number">100000</span>)</span><br><span class="line">    b = torch.arange(<span class="number">100000</span>)</span><br><span class="line">    res = torch.zeros(<span class="number">100000</span>)</span><br><span class="line">    clock = Timer()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a)):</span><br><span class="line">        res[i] += a[i] + b[i]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;use for loop: <span class="subst">&#123;clock.stop(): <span class="number">.5</span>f&#125;</span> sec&#x27;</span>)</span><br><span class="line">    clock.start()</span><br><span class="line">    c = a + b</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;use tensor: <span class="subst">&#123;clock.stop(): <span class="number">.5</span>f&#125;</span> sec&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">use for loop:  1.15502 sec</span><br><span class="line">use tensor:  0.00100 sec</span><br></pre></td></tr></table></figure><h3 id="正态分布与平方损失">正态分布与平方损失</h3><p>正态分布与线性回归关系密切，其概率密度函数如下</p><p><img src="%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/GaussDistribution.png" alt="GaussDistribution"></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">x, mu, sigma</span>):</span><br><span class="line">    <span class="comment"># 正态分布公式</span></span><br><span class="line">    p = <span class="number">1</span> / math.sqrt(<span class="number">2</span> * math.pi * sigma ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> p * np.exp(-<span class="number">0.5</span> / sigma ** <span class="number">2</span> * (x - mu) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gauss_distribution</span>():</span><br><span class="line">    x = torch.arange(-<span class="number">7</span>, <span class="number">7</span>, <span class="number">0.01</span>)  <span class="comment"># 从-7 到 7，步长为0.01</span></span><br><span class="line">    mu_sigma = [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">1</span>)]  <span class="comment"># 三种均值和标准差组合</span></span><br><span class="line">    d2l.plot(x, [normal(x, mu, sigma) <span class="keyword">for</span> mu, sigma <span class="keyword">in</span> mu_sigma], figsize=(<span class="number">4.5</span>, <span class="number">2.5</span>), xlabel=<span class="string">&#x27;x&#x27;</span>, ylabel=<span class="string">&#x27;p(x)&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/normal.png" alt="image-20240915134206875"></p><p>之所以说正态分布和线性回归关系紧密，是因为我们<strong>可以假设噪声服从正态分布</strong>，这样就能把均方误差损失函数用于线性回归</p><p>详细解释可见<a href="https://d2l.ai/chapter_linear-regression/linear-regression.html#the-normal-distribution-and-squared-loss">3.1. Linear Regression — Dive into Deep Learning 1.0.3 documentation (d2l.ai)</a></p><h3 id="从0实现线性回归">从0实现线性回归</h3><p><em>线性回归是一个单层神经网络</em></p><p><strong>从此节开始，将不会再单独列出pytorch等的语法以及数学公式，具体代码和公式均会体现在<code>code</code>板块中，可以自行阅读查看。</strong></p><p>实现步骤：</p><ol><li>生成数据集</li><li>读取数据集</li><li>初始化模型参数</li><li>定义模型</li><li>定义损失函数</li><li>定义优化算法</li><li>训练模型</li></ol><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">从零开始实现一个线性回归模型</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">emit_data</span>(<span class="params">w, b, num_examples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    create dataset with noise</span></span><br><span class="line"><span class="string">    :w: params of weight</span></span><br><span class="line"><span class="string">    :b: offset</span></span><br><span class="line"><span class="string">    :num_examples: number of examples</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># according to y = Xw + b + epsilon, X is dataset, w is params, b is offset, epsilon is noise</span></span><br><span class="line">    shape = (num_examples, <span class="built_in">len</span>(w))  <span class="comment"># shape of dataset, examples.number * w.length (Xw need X.col == w.row)</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, shape)  <span class="comment"># dataset X from standard normal distribution</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    <span class="comment"># add noise epsilon</span></span><br><span class="line">    noise = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    y += noise  <span class="comment"># add noise</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, x, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    读取数据集，根据原数据集可以生成一些批量样本</span></span><br><span class="line"><span class="string">    :param batch_size:批量样本的大小</span></span><br><span class="line"><span class="string">    :param x:</span></span><br><span class="line"><span class="string">    :param y:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    number_examples = <span class="built_in">len</span>(x)  <span class="comment"># x is dataset features, length is number of examples</span></span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(number_examples))</span><br><span class="line">    random.shuffle(indices)  <span class="comment"># “洗牌”，对列表进行随机排序，使列表中的每个元素都有可能出现在任意位置</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, number_examples, batch_size):  <span class="comment"># 以指定的批量大小为步长，遍历列表，每次生成一个批量样本</span></span><br><span class="line">        batch_indices = torch.tensor(indices[i:<span class="built_in">min</span>(i + batch_size, number_examples)])  <span class="comment"># 剩下不足一个批量大小的数据生成到一起</span></span><br><span class="line">        <span class="keyword">yield</span> x[batch_indices], y[batch_indices]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear_model</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    define a linear model</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义均方损失函数</span></span><br><span class="line"><span class="string">    :param y_hat: predict</span></span><br><span class="line"><span class="string">    :param y: actual</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在此例中我们有两个权重（特征）</span></span><br><span class="line">    <span class="comment"># sum((y_pred - y_act) ^ 2) / 2</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y) ** <span class="number">2</span> * <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">param, lr, batch_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义优化算法：梯度下降</span></span><br><span class="line"><span class="string">    :param param 模型参数列表</span></span><br><span class="line"><span class="string">    :param lr 学习率，控制参数更新的幅度</span></span><br><span class="line"><span class="string">    :param batch_size 批量大小，用于计算梯度平均值</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 禁用自动梯度计算，因为参数更新不需要跟踪梯度</span></span><br><span class="line">        <span class="keyword">for</span> param_i <span class="keyword">in</span> param:</span><br><span class="line">            param_i.data -= lr * param_i.grad / batch_size  <span class="comment"># param_i.grad 是当前参数的梯度，除以 batch_size 以得到平均梯度</span></span><br><span class="line">            param_i.grad.zero_()  <span class="comment"># 梯度清零，为下一次迭代计算新的梯度做准备</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">k, b, x</span>):</span><br><span class="line">    <span class="keyword">return</span> k * x + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># actual params and offset</span></span><br><span class="line">    actual_w = torch.tensor([<span class="number">1.14</span>, <span class="number">5.14</span>])</span><br><span class="line">    actual_b = <span class="number">1.91981</span></span><br><span class="line">    data_size = <span class="number">1145</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># emit data</span></span><br><span class="line">    features, labels = emit_data(actual_w, actual_b, data_size)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;features:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(features)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;labels:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(labels)</span><br><span class="line">    <span class="comment"># data in vision</span></span><br><span class="line">    d2l.set_figsize()  <span class="comment"># 设置绘图的尺寸</span></span><br><span class="line">    d2l.plt.scatter(features[:, <span class="number">1</span>].detach().numpy(), labels.detach().numpy(), <span class="number">1</span>)  <span class="comment"># 绘制散点图</span></span><br><span class="line">    <span class="comment"># features[:, 1]表示特征的第二列数据, labels表示标签；.detach().numpy()用于从张量中提取数值。</span></span><br><span class="line">    plt.xlabel = <span class="string">&quot;features&quot;</span></span><br><span class="line">    plt.ylabel = <span class="string">&quot;labels&quot;</span></span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># read data</span></span><br><span class="line">    batch_size = <span class="number">10</span>  <span class="comment"># 批量大小为10</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;a batch of data:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        <span class="built_in">print</span>(X)</span><br><span class="line">        <span class="built_in">print</span>(y)</span><br><span class="line">        <span class="keyword">break</span>  <span class="comment"># 看一下就行了</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># init out model&#x27;s param</span></span><br><span class="line">    <span class="comment"># 创建了一个名为w的张量，形状为(2, 1)，值从均值为0、标准差为0.01的正态分布中随机生成，且要求计算梯度。</span></span><br><span class="line">    w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)  <span class="comment"># 作为模型初始权重</span></span><br><span class="line">    <span class="comment"># 创建了一个名为b的张量，形状为(1,)，初始值为0，同样要求计算梯度。这些操作通常用于初始化神经网络的权重和偏置项。</span></span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># 作为模型初始偏移</span></span><br><span class="line">    <span class="comment"># 初始化这些参数之后，我们需要不断更新它们，直到足够拟合数据</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># start training</span></span><br><span class="line">    <span class="comment"># 首先为模板赋上此例中的具体值</span></span><br><span class="line">    lr = <span class="number">0.03</span>  <span class="comment"># 学习率是一个超参数，暂时指定为0.03</span></span><br><span class="line">    num_epochs = <span class="number">3</span>  <span class="comment"># 训练轮数也是一个超参数</span></span><br><span class="line">    net = linear_model  <span class="comment"># 神经网络采用线性模型</span></span><br><span class="line">    loss = squared_loss  <span class="comment"># 损失函数采用均方损失函数</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">            l = loss(net(X, w, b), y)  <span class="comment"># 计算X和y的小批量损失</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()  <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">            sgd([w, b], lr, batch_size)  <span class="comment"># 更新参数</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            train_l = loss(net(features, w, b), labels)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;real:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(actual_w)</span><br><span class="line">    <span class="built_in">print</span>(actual_b)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predict:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(w)</span><br><span class="line">    <span class="built_in">print</span>(b)</span><br><span class="line">    <span class="comment"># in vision, real scatter is blue, predict linear is red</span></span><br><span class="line">    draw_x = features[:, <span class="number">1</span>]</span><br><span class="line">    draw_y = func(w[<span class="number">1</span>], b, draw_x)</span><br><span class="line">    d2l.plt.plot(draw_x, draw_y.detach().numpy(), color=<span class="string">&quot;red&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">features:</span><br><span class="line">tensor([[ 1.2594, -0.1781],</span><br><span class="line">        [ 0.0551,  0.0021],</span><br><span class="line">        [ 0.5613, -0.4662],</span><br><span class="line">        ...,</span><br><span class="line">        [-0.1275,  0.3930],</span><br><span class="line">        [-0.5470,  0.7300],</span><br><span class="line">        [-0.0886,  0.5512]])</span><br><span class="line">labels:</span><br><span class="line">tensor([[2.4401],</span><br><span class="line">        [2.0031],</span><br><span class="line">        [0.1762],</span><br><span class="line">        ...,</span><br><span class="line">        [3.7953],</span><br><span class="line">        [5.0566],</span><br><span class="line">        [4.6438]])</span><br><span class="line">a batch of data:</span><br><span class="line">tensor([[-0.4109,  0.5243],</span><br><span class="line">        [-1.6386, -0.1570],</span><br><span class="line">        [-0.8908,  0.8113],</span><br><span class="line">        [-0.3493,  0.1824],</span><br><span class="line">        [-0.1640, -0.5096],</span><br><span class="line">        [ 0.1518, -1.7703],</span><br><span class="line">        [-1.6326,  2.3128],</span><br><span class="line">        [-0.2308, -1.2880],</span><br><span class="line">        [-0.9320, -1.3867],</span><br><span class="line">        [ 0.6465, -0.2461]])</span><br><span class="line">tensor([[ 4.1418],</span><br><span class="line">        [-0.7664],</span><br><span class="line">        [ 5.0695],</span><br><span class="line">        [ 2.4722],</span><br><span class="line">        [-0.8736],</span><br><span class="line">        [-7.0107],</span><br><span class="line">        [11.9394],</span><br><span class="line">        [-4.9565],</span><br><span class="line">        [-6.2656],</span><br><span class="line">        [ 1.4107]])</span><br><span class="line">epoch 1, loss 0.018194</span><br><span class="line">epoch 2, loss 0.000079</span><br><span class="line">epoch 3, loss 0.000051</span><br><span class="line">real:</span><br><span class="line">tensor([1.1400, 5.1400])</span><br><span class="line">1.91981</span><br><span class="line">predict:</span><br><span class="line">tensor([[1.1412],</span><br><span class="line">        [5.1394]], requires_grad=True)</span><br><span class="line">tensor([1.9196], requires_grad=True)</span><br></pre></td></tr></table></figure><p><img src="%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/linear_output.png" alt="image-20240915154508074"></p><h3 id="使用已有框架实现线性回归">使用已有框架实现线性回归</h3><p>现有的许多模型已经被研究很多了，许多现成的库可以直接调用; 由于数据迭代器、损失函数、优化器和神经网络层很常用，现代深度学习库也为我们实现了这些组件</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  <span class="comment"># 导入神经网络</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">使用现有的库实现线性回归</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据可以直接使用现有的data框架里的api</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">data_tuple, batch_size, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;实现一个pytorch迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># *用于解包，将data_tuple解包为多个参数，并作为独立的参数传入TensorDataset</span></span><br><span class="line">    dataset = data.TensorDataset(*data_tuple)  <span class="comment"># 将输入的元组数据转换为TensorDataset</span></span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)  <span class="comment"># 创建DataLoader，设置批量大小为batch_size</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># actual params and offset</span></span><br><span class="line">    actual_w = torch.tensor([<span class="number">1.14</span>, <span class="number">5.14</span>])</span><br><span class="line">    actual_b = <span class="number">1.91981</span></span><br><span class="line">    data_size = <span class="number">1145</span></span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">10</span></span><br><span class="line">    features, labels = d2l.synthetic_data(actual_w, actual_b, data_size)  <span class="comment"># 生成数据集也使用现成库</span></span><br><span class="line">    data_iter = load_data((features, labels), batch_size)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter)))  <span class="comment"># 看一下效果，从可迭代对象data_iter中获取一个迭代器，并调用next()方法从该迭代器中取出下一个元素</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))  <span class="comment"># 选择现有库nn中的线性模型，输入维度为2，输出维度为1，即将两个输入特征映射成一个输出结果</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># init model</span></span><br><span class="line">    net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)  <span class="comment"># 初始化权重参数, 均值为0，方差为0.01</span></span><br><span class="line">    net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)  <span class="comment"># 初始化偏置参数，0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义损失函数</span></span><br><span class="line">    loss = nn.MSELoss()  <span class="comment"># 均方误差损失函数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义优化算法</span></span><br><span class="line">    opt = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)  <span class="comment"># 随机梯度下降，学习率为0.03</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    num_epochs = <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        l = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            l = loss(net(X), y)  <span class="comment"># 计算损失</span></span><br><span class="line">            opt.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">            l.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            opt.step()  <span class="comment"># 更新参数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(l):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;real:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(actual_w)</span><br><span class="line">    <span class="built_in">print</span>(actual_b)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predict:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(net[<span class="number">0</span>].weight.data)</span><br><span class="line">    <span class="built_in">print</span>(net[<span class="number">0</span>].bias.data)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[tensor([[-0.5939,  0.1061],</span><br><span class="line">        [-0.1793, -0.2387],</span><br><span class="line">        [ 0.1372, -0.0213],</span><br><span class="line">        [ 0.0400,  1.5084],</span><br><span class="line">        [ 0.7948, -0.4549],</span><br><span class="line">        [ 1.9140, -0.7444],</span><br><span class="line">        [-0.1117,  0.3958],</span><br><span class="line">        [-2.2069, -1.2470],</span><br><span class="line">        [-0.2498, -0.0193],</span><br><span class="line">        [ 0.0258,  1.3288]]), tensor([[ 1.7824],</span><br><span class="line">        [ 0.4828],</span><br><span class="line">        [ 1.9654],</span><br><span class="line">        [ 9.7057],</span><br><span class="line">        [ 0.4970],</span><br><span class="line">        [ 0.2660],</span><br><span class="line">        [ 3.8387],</span><br><span class="line">        [-7.0099],</span><br><span class="line">        [ 1.5277],</span><br><span class="line">        [ 8.7718]])]</span><br><span class="line">epoch 1, loss 0.000083</span><br><span class="line">epoch 2, loss 0.000095</span><br><span class="line">epoch 3, loss 0.000060</span><br><span class="line">real:</span><br><span class="line">tensor([1.1400, 5.1400])</span><br><span class="line">1.91981</span><br><span class="line">predict:</span><br><span class="line">tensor([[1.1394, 5.1387]])</span><br><span class="line">tensor([1.9191])</span><br></pre></td></tr></table></figure><h2 id="softmax回归">softmax回归</h2><p>softmax也是一个单层神经网络</p><h3 id="分类问题">分类问题</h3><p>即取值是离散值的问题，比如是猫还是狗，成年还是未成年等。</p><h3 id="softmax详细理论知识">softmax详细理论知识</h3><p>详见<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.4_softmax-regression?id=_342-softmax%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B">3.4 softmax回归 - Dive-into-DL-PyTorch (tangshusen.me)</a></p><h3 id="从零实现softmax回归">从零实现softmax回归</h3><p>和从0实现线性回归同理，此处将从零实现softmax回归，使用接下来会介绍的Fashion-MNIST数据集</p><p>步骤如下：</p><ol><li>初始化模型参数</li><li>定义softmax操作</li><li>定义模型</li><li>定义损失函数</li><li>分类精度</li><li>训练</li><li>预测</li></ol><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TBD</span><br></pre></td></tr></table></figure><h3 id="使用已有框架实现softmax回归">使用已有框架实现softmax回归</h3><p>TBD</p><h2 id="图像分类数据集">图像分类数据集</h2><p>MNIST数据集是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。此处将使用类似但更复杂的Fashion‐MNIST数据集</p><p>直接上代码</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TBD</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性代数</title>
      <link href="/ymhui.github.io/2024/09/14/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
      <url>/ymhui.github.io/2024/09/14/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1>在pytorch中使用线性代数处理数据</h1><p><strong>source code</strong>: <a href="https://github.com/NJU-ymhui/DataOperations">NJU-ymhui/DataOperations: Use pytorch for data operations (github.com)</a></p><p><strong>use git to clone</strong>: <a href="https://github.com/NJU-ymhui/DataOperations.git">https://github.com/NJU-ymhui/DataOperations.git</a></p><blockquote><p>linear_algebra.pytensor_operation.py</p></blockquote><h2 id="基本概念">基本概念</h2><h3 id="轴">轴</h3><p>**张量的每一个轴对应数据的一个维度。**轴的编号从0开始；轴的编号指明了张量的某个方向。</p><p>示例：一个二维张量可以视作一个矩阵，它有两个轴：行(轴-0)与列(轴-1)</p><h3 id="维度">维度</h3><p><strong>此处的维度描述的是张量这个宏观概念。</strong></p><p>维度是张量的一个属性，指的是张量的数据结构在各个方向上的扩展性。例如，一个二维张量（矩阵）有两个维度，三维张量有三个维度，以此类推。维度可以理解为张量的形状的一部分，描述了张量在每个方向上的大小。例如，形状为 <code>(2, 3, 4)</code> 的三维张量有 3 个维度。</p><p><em>注意区分<strong>轴</strong>与<strong>维度</strong>的概念：维度是描述张量形状的，而轴是进行张量操作时用来指定方向的。可以把维度看作是描述张量结构的属性，而轴是对这些属性进行操作的工具。</em></p><h3 id="形状">形状</h3><p>**张量的形状是一个表示每个轴大小的元组。**例如，一个形状为 <code>(3, 4)</code> 的二维张量有 3 行和 4 列。</p><h2 id="标量">标量</h2><p>在<code>pytorch</code>中，标量由只有一个元素的张量表示，当然也可以进行代数运算。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scalar</span>():</span><br><span class="line">    x = torch.tensor(<span class="number">3</span>)  <span class="comment"># 生成一个标量3</span></span><br><span class="line">    y = torch.tensor(<span class="number">2.5</span>)  <span class="comment"># 生成一个标量2.5</span></span><br><span class="line">    z = torch.tensor(<span class="number">1.</span>)</span><br><span class="line">    <span class="built_in">print</span>(x, y, z)</span><br><span class="line">    <span class="comment"># calculate</span></span><br><span class="line">    <span class="built_in">print</span>(x + y + z, x * y / z, x ** y, x % z)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># differ</span></span><br><span class="line">    x = torch.tensor([<span class="number">3</span>])  <span class="comment"># 注：生成的不是标量，而是一个长度为 1 的一维张量</span></span><br><span class="line">    <span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor(3) tensor(2.5000) tensor(1.)</span><br><span class="line">tensor(6.5000) tensor(7.5000) tensor(15.5885) tensor(0.)</span><br><span class="line">tensor([3])</span><br></pre></td></tr></table></figure><p><em>注：x = torch.tensor(3)与x = torch.tensor([3])表达不同的语义，前者生成的是一个标量（虽然是以张量的形式表示），而后者生成的的的确确是一个只有一个元素的一维张量</em></p><h2 id="向量">向量</h2><p>向量可以被视作由标量组成的列表，在<code>pytorch</code>中，向量由一维张量表示。</p><p>和原生<code>python</code>一样可以用下标索引访问向量中的元素（在<strong>数据操作</strong>一文中有介绍）</p><h3 id="长度、维度">长度、维度</h3><p><code>pytorch</code>中的张量提供了size()和shape属性来获取向量(一维张量)的长度、维度（因为向量一定只有一个</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vector</span>():</span><br><span class="line">    vec = torch.arange(<span class="number">4</span>)  <span class="comment"># 生成一个长度为 4 的一维张量表示向量，arange 函数生成一个从 0 到 3 的序列</span></span><br><span class="line">    <span class="built_in">print</span>(vec)</span><br><span class="line">    <span class="comment"># visit element in vec</span></span><br><span class="line">    <span class="built_in">print</span>(vec[<span class="number">0</span>], vec[<span class="number">1</span>], vec[<span class="number">2</span>], vec[<span class="number">3</span>])</span><br><span class="line">    <span class="comment"># change element in vec</span></span><br><span class="line">    vec[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(vec[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(vec)</span><br><span class="line">    <span class="comment"># show the length, dim of vec</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;size:&quot;</span>, vec.size(), <span class="string">&quot;shape:&quot;</span>, vec.shape)  <span class="comment"># 因为向量vec是一维张量，所以它的shape就是这个向量的维数</span></span><br><span class="line">    <span class="comment"># use len()</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;size by len():&quot;</span>, <span class="built_in">len</span>(vec))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([0, 1, 2, 3])</span><br><span class="line">tensor(0) tensor(1) tensor(2) tensor(3)</span><br><span class="line">tensor(1)</span><br><span class="line">tensor([1, 1, 2, 3])</span><br><span class="line">size: torch.Size([4]) shape: torch.Size([4])</span><br><span class="line">size by len(): 4</span><br></pre></td></tr></table></figure><h2 id="矩阵">矩阵</h2><p>正如向量是标量的推广，矩阵是向量的推广，<strong>可以看作是向量的组合</strong>。</p><p>通过<code>reshape()</code>方法可以将一个一维张量重塑成多维张量，自然可以形成矩阵。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">matrix</span>():</span><br><span class="line">    mat = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)  <span class="comment"># 生成一个普通矩阵5行4列，0-19</span></span><br><span class="line">    <span class="built_in">print</span>(mat)</span><br><span class="line">    <span class="comment"># 通过matrix的T属性访问其转置矩阵</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;transpose of matrix:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat.T)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11],</span><br><span class="line">        [12, 13, 14, 15],</span><br><span class="line">        [16, 17, 18, 19]])</span><br><span class="line">tensor([[ 0,  4,  8, 12, 16],</span><br><span class="line">        [ 1,  5,  9, 13, 17],</span><br><span class="line">        [ 2,  6, 10, 14, 18],</span><br><span class="line">        [ 3,  7, 11, 15, 19]])</span><br></pre></td></tr></table></figure><h2 id="张量">张量</h2><p>现在我们要正式介绍张量的概念了。正如向量是标量的推广，矩阵是向量的推广，<strong>张量可以看作是对矩阵的推广，将数据送上更高的维度，拥有更多的轴。</strong></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tensor</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;3 * 4 * 5:&quot;</span>)</span><br><span class="line">    tens = torch.arange(<span class="number">60</span>).reshape(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)  <span class="comment"># 生成一个三维张量，4行5列的矩阵有3个</span></span><br><span class="line">    <span class="built_in">print</span>(tens)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;2 x 3 x 4 x 5:&quot;</span>)</span><br><span class="line">    tens2 = torch.arange(<span class="number">120</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)  <span class="comment"># 生成一个四维张量，4行5列的矩阵有3个, 这样的组合还有两个</span></span><br><span class="line">    <span class="built_in">print</span>(tens2)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">3 * 4 * 5:</span><br><span class="line">tensor([[[ 0,  1,  2,  3,  4],</span><br><span class="line">         [ 5,  6,  7,  8,  9],</span><br><span class="line">         [10, 11, 12, 13, 14],</span><br><span class="line">         [15, 16, 17, 18, 19]],</span><br><span class="line"></span><br><span class="line">        [[20, 21, 22, 23, 24],</span><br><span class="line">         [25, 26, 27, 28, 29],</span><br><span class="line">         [30, 31, 32, 33, 34],</span><br><span class="line">         [35, 36, 37, 38, 39]],</span><br><span class="line"></span><br><span class="line">        [[40, 41, 42, 43, 44],</span><br><span class="line">         [45, 46, 47, 48, 49],</span><br><span class="line">         [50, 51, 52, 53, 54],</span><br><span class="line">         [55, 56, 57, 58, 59]]])</span><br><span class="line">2 x 3 x 4 x 5:</span><br><span class="line">tensor([[[[  0,   1,   2,   3,   4],</span><br><span class="line">          [  5,   6,   7,   8,   9],</span><br><span class="line">          [ 10,  11,  12,  13,  14],</span><br><span class="line">          [ 15,  16,  17,  18,  19]],</span><br><span class="line"></span><br><span class="line">         [[ 20,  21,  22,  23,  24],</span><br><span class="line">          [ 25,  26,  27,  28,  29],</span><br><span class="line">          [ 30,  31,  32,  33,  34],</span><br><span class="line">          [ 35,  36,  37,  38,  39]],</span><br><span class="line"></span><br><span class="line">         [[ 40,  41,  42,  43,  44],</span><br><span class="line">          [ 45,  46,  47,  48,  49],</span><br><span class="line">          [ 50,  51,  52,  53,  54],</span><br><span class="line">          [ 55,  56,  57,  58,  59]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 60,  61,  62,  63,  64],</span><br><span class="line">          [ 65,  66,  67,  68,  69],</span><br><span class="line">          [ 70,  71,  72,  73,  74],</span><br><span class="line">          [ 75,  76,  77,  78,  79]],</span><br><span class="line"></span><br><span class="line">         [[ 80,  81,  82,  83,  84],</span><br><span class="line">          [ 85,  86,  87,  88,  89],</span><br><span class="line">          [ 90,  91,  92,  93,  94],</span><br><span class="line">          [ 95,  96,  97,  98,  99]],</span><br><span class="line"></span><br><span class="line">         [[100, 101, 102, 103, 104],</span><br><span class="line">          [105, 106, 107, 108, 109],</span><br><span class="line">          [110, 111, 112, 113, 114],</span><br><span class="line">          [115, 116, 117, 118, 119]]]])</span><br></pre></td></tr></table></figure><h2 id="pytorch中张量算法的性质">pytorch中张量算法的性质</h2><h3 id="clone-方法">clone()方法</h3><p>张量的<code>clone()</code>方法会分配一个新内存，<strong>并完全拷贝被克隆张量的内容</strong></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">clone</span>():</span><br><span class="line">    a = torch.arange(<span class="number">20</span>).reshape(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">    b = a.clone()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;memory same?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">id</span>(a) == <span class="built_in">id</span>(b))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;context same?&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(a == b)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;context:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;a:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(a)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;a + a.clone():&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(a + b)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">memory same?</span><br><span class="line">False</span><br><span class="line">context same?</span><br><span class="line">tensor([[True, True, True, True, True],</span><br><span class="line">        [True, True, True, True, True],</span><br><span class="line">        [True, True, True, True, True],</span><br><span class="line">        [True, True, True, True, True]])</span><br><span class="line">context:</span><br><span class="line">a:</span><br><span class="line">tensor([[ 0,  1,  2,  3,  4],</span><br><span class="line">        [ 5,  6,  7,  8,  9],</span><br><span class="line">        [10, 11, 12, 13, 14],</span><br><span class="line">        [15, 16, 17, 18, 19]])</span><br><span class="line">a + a.clone():</span><br><span class="line">tensor([[ 0,  2,  4,  6,  8],</span><br><span class="line">        [10, 12, 14, 16, 18],</span><br><span class="line">        [20, 22, 24, 26, 28],</span><br><span class="line">        [30, 32, 34, 36, 38]])</span><br></pre></td></tr></table></figure><h3 id="张量乘">张量乘</h3><p>张量的乘法<code>*</code>是按元素乘（<strong>数据操作</strong>里有讲）</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>():</span><br><span class="line">    a = torch.arange(<span class="number">4</span>).reshape(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    b = torch.arange(<span class="number">4</span>).reshape(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(a)</span><br><span class="line">    <span class="built_in">print</span>(b)</span><br><span class="line">    <span class="built_in">print</span>(a * b)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[0, 1],</span><br><span class="line">        [2, 3]])</span><br><span class="line">tensor([[0, 1],</span><br><span class="line">        [2, 3]])</span><br><span class="line">tensor([[0, 1],</span><br><span class="line">        [4, 9]])</span><br></pre></td></tr></table></figure><h3 id="与标量运算">与标量运算</h3><p>一个张量与一个标量运算，效果为<strong>张量的每个元素都与标量做一次运算</strong></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_with_scalar</span>():</span><br><span class="line">    mat = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># 生成一个2 * 3的矩阵</span></span><br><span class="line">    sca = torch.tensor(<span class="number">2</span>)  <span class="comment"># 生成一个标量2</span></span><br><span class="line">    <span class="built_in">print</span>(mat)</span><br><span class="line">    <span class="built_in">print</span>(sca)</span><br><span class="line">    <span class="built_in">print</span>(mat * sca)</span><br><span class="line">    <span class="built_in">print</span>(mat + sca)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[0, 1, 2],</span><br><span class="line">        [3, 4, 5]])</span><br><span class="line">tensor(2)</span><br><span class="line">tensor([[ 0,  2,  4],</span><br><span class="line">        [ 6,  8, 10]])</span><br><span class="line">tensor([[2, 3, 4],</span><br><span class="line">        [5, 6, 7]])</span><br></pre></td></tr></table></figure><h2 id="降维">降维</h2><h3 id="求和">求和</h3><p>可以通过<code>var.sum(...)</code>方法求张量中所有元素的和</p><p>默认情况下求和会把张量中所有元素都加在一起，使其退化为一个标量，但也<strong>可以指定张量沿哪一个轴来求和降维</strong>，通过<code>axis</code>参数。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sum_reduce_dim</span>():</span><br><span class="line">    mat = torch.arange(<span class="number">20</span>).reshape(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">    vec = torch.arange(<span class="number">4</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat)</span><br><span class="line">    <span class="built_in">print</span>(vec)</span><br><span class="line">    <span class="comment"># 默认求所有元素和，求完后变成标量</span></span><br><span class="line">    <span class="built_in">print</span>(mat.<span class="built_in">sum</span>())</span><br><span class="line">    <span class="built_in">print</span>(vec.<span class="built_in">sum</span>())</span><br><span class="line">    <span class="comment"># 指定轴降维求和</span></span><br><span class="line">    <span class="comment"># 以矩阵为例</span></span><br><span class="line">    <span class="built_in">print</span>(mat.<span class="built_in">sum</span>(axis=<span class="number">0</span>))  <span class="comment"># 沿行（轴-0）求和，即对于第i列，将它的每一行元素求和，即求每列的和，求完后变成向量</span></span><br><span class="line">    <span class="built_in">print</span>(mat.<span class="built_in">sum</span>(axis=<span class="number">1</span>))  <span class="comment"># 沿列（轴-1）求和，即对于第i行，将它的每一列元素求和，即求每行的和</span></span><br><span class="line">    <span class="built_in">print</span>(mat.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>]))  <span class="comment"># 沿所有轴降维，即求全体和</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[ 0,  1,  2,  3,  4],</span><br><span class="line">        [ 5,  6,  7,  8,  9],</span><br><span class="line">        [10, 11, 12, 13, 14],</span><br><span class="line">        [15, 16, 17, 18, 19]])</span><br><span class="line">tensor([0, 1, 2, 3])</span><br><span class="line">tensor(190)</span><br><span class="line">tensor(6)</span><br><span class="line">tensor([30, 34, 38, 42, 46])</span><br><span class="line">tensor([10, 35, 60, 85])</span><br><span class="line">tensor(190)</span><br></pre></td></tr></table></figure><h3 id="求平均">求平均</h3><p>可以通过<code>var.mean()</code>方法求所有元素的平均值，与<code>sum</code>同理，<strong>也可以指定沿哪个轴求均值</strong>。</p><p><em>注：求均值的张量元素期望为浮点或复数类型</em></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean</span>():</span><br><span class="line">    mat = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat)</span><br><span class="line">    <span class="built_in">print</span>(mat.mean())  <span class="comment"># 求完后变成标量</span></span><br><span class="line">    <span class="built_in">print</span>(mat.<span class="built_in">sum</span>() / mat.numel())</span><br><span class="line">    <span class="comment"># 指定轴</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;mean of column:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat.mean(axis=<span class="number">0</span>))  <span class="comment"># 求每列的平均值，求完后变成向量</span></span><br><span class="line">    <span class="built_in">print</span>(mat.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / mat.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;mean of row:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat.mean(axis=<span class="number">1</span>))  <span class="comment"># 求每行的平均值</span></span><br><span class="line">    <span class="built_in">print</span>(mat.<span class="built_in">sum</span>(axis=<span class="number">1</span>) / mat.shape[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  2.,  3.,  4.],</span><br><span class="line">        [ 5.,  6.,  7.,  8.,  9.],</span><br><span class="line">        [10., 11., 12., 13., 14.],</span><br><span class="line">        [15., 16., 17., 18., 19.]])</span><br><span class="line">tensor(9.5000)</span><br><span class="line">tensor(9.5000)</span><br><span class="line">mean of column:</span><br><span class="line">tensor([ 7.5000,  8.5000,  9.5000, 10.5000, 11.5000])</span><br><span class="line">tensor([ 7.5000,  8.5000,  9.5000, 10.5000, 11.5000])</span><br><span class="line">mean of row:</span><br><span class="line">tensor([ 2.,  7., 12., 17.])</span><br><span class="line">tensor([ 2.,  7., 12., 17.])</span><br></pre></td></tr></table></figure><h2 id="非降维求和">非降维求和</h2><p>有时我们希望求完和或均值后的张量<strong>保持轴数</strong>，可以使用<code>keepdims</code>参数来实现。通过这种方法得到的按某个轴求得的和或均值轴数不变，就可以利用广播机制与原张量进行运算。</p><p>如果我们想沿某个轴计算张量中元素的累积总和，比如<code>axis=0</code>（按行计算），可以调用<code>cumsum()</code>函数。此函数不会沿任何轴降低输入张量的维度。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">keep_dims</span>():</span><br><span class="line">    mat = torch.arange(<span class="number">20</span>).reshape(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;mat:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sum of column:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat.<span class="built_in">sum</span>(axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))  <span class="comment"># 依然求每列的和，但求完和后仍体现为一个二维张量</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sum of row:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">    <span class="comment"># 结果仍是两个轴，可以与原张量运算</span></span><br><span class="line">    tmp = mat.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)  <span class="comment"># tmp为每行求和结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;tmp:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(tmp)</span><br><span class="line">    <span class="comment"># 利用广播机制求原矩阵除以按行求和的结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;broadcast for mat / tmp:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat / tmp)</span><br><span class="line">    <span class="comment"># 沿行（轴-0）计算矩阵中元素的累计值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;cumulative value of mat by row:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat.cumsum(axis=<span class="number">0</span>))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mat:</span><br><span class="line">tensor([[ 0,  1,  2,  3,  4],</span><br><span class="line">        [ 5,  6,  7,  8,  9],</span><br><span class="line">        [10, 11, 12, 13, 14],</span><br><span class="line">        [15, 16, 17, 18, 19]])</span><br><span class="line">sum of column:</span><br><span class="line">tensor([[30, 34, 38, 42, 46]])</span><br><span class="line">sum of row:</span><br><span class="line">tensor([[10],</span><br><span class="line">        [35],</span><br><span class="line">        [60],</span><br><span class="line">        [85]])</span><br><span class="line">tmp:</span><br><span class="line">tensor([[10],</span><br><span class="line">        [35],</span><br><span class="line">        [60],</span><br><span class="line">        [85]])</span><br><span class="line">broadcast for mat / tmp:</span><br><span class="line">tensor([[0.0000, 0.1000, 0.2000, 0.3000, 0.4000],</span><br><span class="line">        [0.1429, 0.1714, 0.2000, 0.2286, 0.2571],</span><br><span class="line">        [0.1667, 0.1833, 0.2000, 0.2167, 0.2333],</span><br><span class="line">        [0.1765, 0.1882, 0.2000, 0.2118, 0.2235]])</span><br><span class="line">cumulative value of mat by row:</span><br><span class="line">tensor([[ 0,  1,  2,  3,  4],</span><br><span class="line">        [ 5,  7,  9, 11, 13],</span><br><span class="line">        [15, 18, 21, 24, 27],</span><br><span class="line">        [30, 34, 38, 42, 46]])</span><br></pre></td></tr></table></figure><h2 id="点积">点积</h2><p>两个向量之间可以进行点积，使用<code>torch.dot(...)</code>方法可以在向量间使用点积；根据点积的定义也可以先对向量做按元素乘法，然后再计算总和。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dot_product</span>():</span><br><span class="line">    a = torch.arange(<span class="number">4</span>)</span><br><span class="line">    b = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">9</span>])</span><br><span class="line">    <span class="built_in">print</span>(a)</span><br><span class="line">    <span class="built_in">print</span>(b)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;dot product:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(torch.dot(a, b))</span><br><span class="line">    <span class="built_in">print</span>(torch.<span class="built_in">sum</span>(a * b))</span><br><span class="line">    <span class="built_in">print</span>((a * b).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([0, 1, 2, 3])</span><br><span class="line">tensor([1, 2, 5, 9])</span><br><span class="line">dot product:</span><br><span class="line">tensor(39)</span><br><span class="line">tensor(39)</span><br><span class="line">tensor(39)</span><br></pre></td></tr></table></figure><h2 id="矩阵乘法">矩阵乘法</h2><h3 id="矩阵-向量积">矩阵-向量积</h3><p>即用一个矩阵<strong>矩阵乘</strong>一个向量：<strong>A~mn~x~n~</strong>，注意矩阵的列（轴-1）要与向量的维数一致</p><p><code>torch.mv(...)</code>：实现矩阵乘向量</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_mul_vector</span>():</span><br><span class="line">    mat = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)  <span class="comment"># 生成一个5 * 4的矩阵</span></span><br><span class="line">    vec = torch.arange(<span class="number">4</span>)  <span class="comment"># 生成一个4维向量</span></span><br><span class="line">    <span class="built_in">print</span>(mat)</span><br><span class="line">    <span class="built_in">print</span>(mat.shape)</span><br><span class="line">    <span class="built_in">print</span>(vec)</span><br><span class="line">    <span class="built_in">print</span>(vec.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;mat * vec:&quot;</span>)</span><br><span class="line">    res = torch.mv(mat, vec)  <span class="comment"># 矩阵乘向量</span></span><br><span class="line">    <span class="built_in">print</span>(res)</span><br><span class="line">    <span class="built_in">print</span>(res.shape)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11],</span><br><span class="line">        [12, 13, 14, 15],</span><br><span class="line">        [16, 17, 18, 19]])</span><br><span class="line">torch.Size([5, 4])</span><br><span class="line">tensor([0, 1, 2, 3])</span><br><span class="line">torch.Size([4])</span><br><span class="line">mat * vec:</span><br><span class="line">tensor([ 14,  38,  62,  86, 110])</span><br><span class="line">torch.Size([5])</span><br></pre></td></tr></table></figure><h3 id="矩阵-矩阵积">矩阵-矩阵积</h3><p><strong>A~mn~B~nk~</strong>，注意左矩阵的列（轴-1）要和右矩阵的行（轴-0）一致</p><p><code>torch.mm(...)</code>：实现矩阵乘矩阵（仅二维张量也就是矩阵）</p><p><code>torch.matmul(...)</code>：实现任意维度张量乘法</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_mul_matrix</span>():</span><br><span class="line">    mat1 = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)  <span class="comment"># 生成一个5 * 4的矩阵</span></span><br><span class="line">    mat2 = torch.arange(<span class="number">12</span>).reshape(<span class="number">4</span>, <span class="number">3</span>)  <span class="comment"># 生成一个4 * 3的矩阵</span></span><br><span class="line">    <span class="built_in">print</span>(mat1)</span><br><span class="line">    <span class="built_in">print</span>(mat1.shape)</span><br><span class="line">    <span class="built_in">print</span>(mat2)</span><br><span class="line">    <span class="built_in">print</span>(mat2.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;mat1 * mat2:&quot;</span>)</span><br><span class="line">    res = torch.mm(mat1, mat2)</span><br><span class="line">    <span class="built_in">print</span>(res)</span><br><span class="line">    <span class="built_in">print</span>(res.shape)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11],</span><br><span class="line">        [12, 13, 14, 15],</span><br><span class="line">        [16, 17, 18, 19]])</span><br><span class="line">torch.Size([5, 4])</span><br><span class="line">tensor([[ 0,  1,  2],</span><br><span class="line">        [ 3,  4,  5],</span><br><span class="line">        [ 6,  7,  8],</span><br><span class="line">        [ 9, 10, 11]])</span><br><span class="line">torch.Size([4, 3])</span><br><span class="line">mat1 * mat2:</span><br><span class="line">tensor([[ 42,  48,  54],</span><br><span class="line">        [114, 136, 158],</span><br><span class="line">        [186, 224, 262],</span><br><span class="line">        [258, 312, 366],</span><br><span class="line">        [330, 400, 470]])</span><br><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure><h2 id="范数">范数</h2><p>通俗的来讲，<strong>范数反映出一个向量的大小</strong>。常见的范数计算方法有<code>L1</code>范数和<code>L2</code>范数两种。</p><h3 id="L-1">L~1~</h3><p>向量元素的绝对值和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">\sum_{i=1}^{n}|x_i|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.104em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L1</span>():</span><br><span class="line">    vec = torch.tensor([<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>, -<span class="number">4</span>])</span><br><span class="line">    <span class="built_in">print</span>(vec)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;L1:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(torch.<span class="built_in">abs</span>(vec).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([ 1, -2,  3, -4])</span><br><span class="line">L1:</span><br><span class="line">tensor(10)</span><br></pre></td></tr></table></figure><h3 id="L-2">L~2~</h3><p>向量与线性空间原点的距离（有点类似距离公式）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mo stretchy="false">(</mo></msqrt><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>x</mi><mi>i</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sqrt(\sum_{i=1}^{n}x_i^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.24em;vertical-align:-0.305em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.935em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mopen" style="padding-left:1em;">(</span></span><span style="top:-2.895em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.28em" viewBox="0 0 400000 1296" preserveAspectRatio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120c340,-704.7,510.7,-1060.3,512,-1067l0 -0c4.7,-7.3,11,-11,19,-11H40000v40H1012.3s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60zM1001 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.305em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，深度学习中更常使用L~2~范数</p><p><code>torch.norm(...)</code>：计算L~2~范数，<strong>注意张量元素要是浮点或复数类型</strong></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L2</span>():</span><br><span class="line">    vec = torch.tensor([<span class="number">1.</span>, -<span class="number">2</span>, <span class="number">3</span>, -<span class="number">4</span>])</span><br><span class="line">    <span class="built_in">print</span>(vec)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;L2:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(torch.norm(vec))</span><br><span class="line">    <span class="built_in">print</span>(torch.sqrt(torch.<span class="built_in">pow</span>(vec, <span class="number">2</span>).<span class="built_in">sum</span>()))  <span class="comment"># 自己套公式</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([ 1., -2.,  3., -4.])</span><br><span class="line">L2:</span><br><span class="line">tensor(5.4772)</span><br><span class="line">tensor(5.4772)</span><br></pre></td></tr></table></figure><h3 id="L-p-范数">L~p~范数</h3><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>V</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mi>p</mi></msub><mo>=</mo><mo stretchy="false">(</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><msup><mi mathvariant="normal">∣</mi><mi>p</mi></msup><msup><mo stretchy="false">)</mo><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">||Vector||_{p}=(\sum_{i=1}^{n}|x_i|^p)^{1/p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord">∣∣</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord mathnormal">ec</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1877em;vertical-align:-0.2997em;"></span><span class="mopen">(</span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1/</span><span class="mord mathnormal mtight">p</span></span></span></span></span></span></span></span></span></span></span></span></p><h3 id="矩阵的L-2-范数">矩阵的L~2~范数</h3><p>矩阵的<em>Frobenius范数</em>定义为<strong>矩阵所有元素的平方和的平方根。</strong></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>M</mi><mi>a</mi><mi>t</mi><mi>r</mi><mi>i</mi><msub><mi>x</mi><mrow><mi>m</mi><mi>n</mi></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mi>F</mi></msub><mo>=</mo><msqrt><mo stretchy="false">(</mo></msqrt><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">||Matrix_{mn}||_F=\sqrt(\sum_{i=1}^{m}\sum_{j=1}^{n}x_{ij}^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣∣</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">mn</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3708em;vertical-align:-0.4358em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.935em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mopen" style="padding-left:1em;">(</span></span><span style="top:-2.895em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.28em" viewBox="0 0 400000 1296" preserveAspectRatio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120c340,-704.7,510.7,-1060.3,512,-1067l0 -0c4.7,-7.3,11,-11,19,-11H40000v40H1012.3s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60zM1001 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.305em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_l2</span>():</span><br><span class="line">    mat = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(mat)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;mat&#x27;s L2 norm&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(torch.norm(mat))</span><br><span class="line">    <span class="built_in">print</span>(torch.sqrt(torch.<span class="built_in">pow</span>(mat, <span class="number">2</span>).<span class="built_in">sum</span>()))  <span class="comment"># 自己套公式</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  2.],</span><br><span class="line">        [ 3.,  4.,  5.],</span><br><span class="line">        [ 6.,  7.,  8.],</span><br><span class="line">        [ 9., 10., 11.]])</span><br><span class="line">mat&#x27;s L2 norm</span><br><span class="line">tensor(22.4944)</span><br><span class="line">tensor(22.4944)</span><br></pre></td></tr></table></figure><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据操作</title>
      <link href="/ymhui.github.io/2024/09/14/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/"/>
      <url>/ymhui.github.io/2024/09/14/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h1>使用pytorch进行数据处理</h1><p>source code: <a href="https://github.com/NJU-ymhui/DataOperations">NJU-ymhui/DataOperations: Use pytorch for data operations (github.com)</a></p><p>use git to clone: <a href="https://github.com/NJU-ymhui/DataOperations.git">https://github.com/NJU-ymhui/DataOperations.git</a></p><blockquote><p><a href="http://start.py">start.py</a><a href="http://dataprepare.py">dataprepare.py</a></p></blockquote><h2 id="入门">入门</h2><p><code>pytorch</code>中的数组被称为张量(<code>Tensor</code>)，与<code>numpy</code>中的<code>ndarray</code>类似，但<code>ndarray</code>仅支持CPU运算，而<code>Tensor</code>同时可以很好地支持<strong>GPU加速运算</strong>，并且Tensor类支持<strong>自动微分</strong>。</p><p>导入pytorch库</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br></pre></td></tr></table></figure><h3 id="张量的生成">张量的生成</h3><p><code>torch.arange(...)</code>：创建一个行向量</p><p><code>var.shape</code>：查看张量的形状</p><p><code>var.numel()</code>：检查张量中元素总数</p><p><code>var.reshape(...)</code>：改变一个张量的形状而不改变元素数量和元素值</p><p><code>torch.zeros(...)</code>：生成一个张量并以<code>0</code>覆盖，张量形状由参数指定</p><p><code>torch.ones(...)</code>：生成一个张量并以<code>1</code>覆盖，张量形状由参数指定</p><p><code>torch.randn(...)</code>：生成一个张量，元素值随机采样自标准正态分布，张量形状由参数指定</p><p><code>torch.tensor(...)</code>：指定初始化一个张量</p><p><code>torch.zeros_like(...)</code>：创建一个和传入的张量形状相同的张量，并填入<code>0</code></p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">torch_tensor</span>():</span><br><span class="line">    x = torch.arange(<span class="number">12</span>)</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(x.numel())</span><br><span class="line">    <span class="built_in">print</span>(x.reshape(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="built_in">print</span>(torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))  <span class="comment"># 1个张量tensor，3 * 4 的零矩阵有2个</span></span><br><span class="line">    <span class="built_in">print</span>(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))  <span class="comment"># 同上，不过填入1</span></span><br><span class="line">    <span class="built_in">print</span>(torch.randn(<span class="number">3</span>, <span class="number">4</span>))  <span class="comment"># 采样自标准正态分布的随机数，3 * 4的矩阵</span></span><br><span class="line">    <span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>]]))  <span class="comment"># 手动初始化一个矩阵</span></span><br><span class="line">    <span class="built_in">print</span>(torch.zeros_like(x))  <span class="comment"># 复制x, 并填0</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><br><span class="line">torch.Size([12])</span><br><span class="line">12</span><br><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11]])</span><br><span class="line">tensor([[[0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.]],</span><br><span class="line"></span><br><span class="line">        [[0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.]]])</span><br><span class="line">tensor([[[1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.]],</span><br><span class="line"></span><br><span class="line">        [[1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.]]])</span><br><span class="line">tensor([[ 1.6022, -0.8597,  0.0841,  0.7659],</span><br><span class="line">        [-1.3949, -0.0424, -0.3197, -0.6832],</span><br><span class="line">        [ 1.1753, -1.5020,  0.5873, -0.2480]])</span><br><span class="line">tensor([[1, 2, 3, 4],</span><br><span class="line">        [6, 5, 7, 8],</span><br><span class="line">        [2, 3, 4, 1]])</span><br><span class="line">tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</span><br></pre></td></tr></table></figure><h3 id="张量的运算">张量的运算</h3><p>对具有相同形状的张量可以进行数值运算（类似<code>matlab</code>），语义与原生python运算一致</p><p><code>torch.exp(...)</code>：e指数运算</p><p><code>var.sum()</code>：对张量所有元素求和</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_operation</span>():</span><br><span class="line">    x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3.6</span>])</span><br><span class="line">    y = torch.tensor([<span class="number">0.5</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">0.1</span>])</span><br><span class="line">    <span class="built_in">print</span>(x * y)</span><br><span class="line">    <span class="built_in">print</span>(x / y)</span><br><span class="line">    <span class="built_in">print</span>(x + y)</span><br><span class="line">    <span class="built_in">print</span>(x - y)</span><br><span class="line">    <span class="built_in">print</span>(x ** y)</span><br><span class="line">    <span class="built_in">print</span>(torch.exp(x))</span><br><span class="line">    <span class="built_in">print</span>(x == y)</span><br><span class="line">    <span class="built_in">print</span>(x.<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([0.5000, 8.0000, 6.0000, 0.3600])</span><br><span class="line">tensor([ 2.0000,  0.5000,  1.5000, 36.0000])</span><br><span class="line">tensor([1.5000, 6.0000, 5.0000, 3.7000])</span><br><span class="line">tensor([ 0.5000, -2.0000,  1.0000,  3.5000])</span><br><span class="line">tensor([ 1.0000, 16.0000,  9.0000,  1.1367])</span><br><span class="line">tensor([ 2.7183,  7.3891, 20.0855, 36.5982])</span><br><span class="line">tensor([False, False, False, False])</span><br><span class="line">tensor(9.6000)</span><br></pre></td></tr></table></figure><h3 id="张量的连接">张量的连接</h3><p><code>tensor.cat(...)</code>：连接多个张量，第一个参数指定连接哪些，第二个参数<code>dim</code>指定按第几维（轴）连接</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_concat</span>():</span><br><span class="line">    x = (torch.arange(<span class="number">12</span>, dtype=torch.float32)).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">    y = (torch.arange(<span class="number">12</span>)).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 按行(轴-0)连接</span></span><br><span class="line">    <span class="built_in">print</span>(torch.cat((x, y), dim=<span class="number">0</span>))</span><br><span class="line">    <span class="comment"># 按列(轴-1)连接</span></span><br><span class="line">    <span class="built_in">print</span>(torch.cat((x, y), dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.],</span><br><span class="line">        [ 0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.]])</span><br><span class="line">tensor([[ 0.,  1.,  2.,  3.,  0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  5.,  6.,  7.,  4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.,  8.,  9., 10., 11.]])</span><br></pre></td></tr></table></figure><p>注意到按行（轴-0）连接的张量列数不变，行合并；按列（轴-1）连接的张量行数不变，列合并</p><h3 id="广播机制">广播机制</h3><p>如果我们尝试对形状不同的张量进行运算，会发生什么？答案是会<strong>将两个张量分别进行适当的复制，变得形状相同</strong>，然后运算（和<code>matlab</code>机制一样）</p><p>在大多数情况下，我们将<strong>沿着数组中长度为1的轴进行广播</strong></p><blockquote><p>demo</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">a = tensor([[0],</span><br><span class="line">   [1],</span><br><span class="line">   [2]])</span><br><span class="line">b = tensor([[0, 1]])</span><br><span class="line">if a + b:</span><br><span class="line">first a =&gt; a&#x27; = tensor([0, 0],</span><br><span class="line">   [1, 1],</span><br><span class="line">   [2, 2])</span><br><span class="line">  b =&gt; b&#x27; = tensor([0, 1],</span><br><span class="line">     [0, 1],</span><br><span class="line">     [0, 1])</span><br><span class="line">then a&#x27; + b&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_broadcast</span>():</span><br><span class="line">    x = torch.arange(<span class="number">3</span>).reshape(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    y = torch.arange(<span class="number">2</span>).reshape(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line">    <span class="built_in">print</span>(y)</span><br><span class="line">    <span class="comment"># add x and y</span></span><br><span class="line">    <span class="built_in">print</span>(x + y)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[0],</span><br><span class="line">        [1],</span><br><span class="line">        [2]])</span><br><span class="line">tensor([[0, 1]])</span><br><span class="line">tensor([[0, 1],</span><br><span class="line">        [1, 2],</span><br><span class="line">        [2, 3]])</span><br></pre></td></tr></table></figure><h3 id="索引和切片">索引和切片</h3><p>与原生python完全一致！第一个元素的索引是0，最后一个元素索引是‐1；可以指定范围以包含第一个元素和最后一个之前的元素；访问张量指定位置元素可以用matlab式访问<code>arr[i, j]</code>或C式访问<code>arr[i][j]</code></p><p><code>i:j</code>: [i, j)左闭右开</p><p><code>[:, i]</code>：取行所有元素（轴-0）与第i列，即第i列的所有元素</p><p><code>[i, :]</code>：取列（轴-1）与第i行，即第i行所有元素</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">index_slice</span>():</span><br><span class="line">    x = torch.arange(<span class="number">12</span>).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;initial:\n&quot;</span>, x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;last:\n&quot;</span>, x[-<span class="number">1</span>])  <span class="comment"># last line</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;row 1 ~ 2:\n&quot;</span>, x[<span class="number">1</span>:<span class="number">3</span>])  <span class="comment"># 第 1 ~ 2 行</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;row 0 ~ 1 and column 1 ~ 2:\n&quot;</span>, x[<span class="number">0</span>:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>])  <span class="comment"># 第 0 ~ 1 行，第 1 ~ 2 列</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;column 2:\n&quot;</span>, x[:, <span class="number">2</span>])  <span class="comment"># 第 2 列</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;row 1:\n&quot;</span>, x[<span class="number">1</span>, :])  <span class="comment"># 第 1 行</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;row 1 and col 2:\n&quot;</span>, x[<span class="number">1</span>, <span class="number">2</span>], x[<span class="number">1</span>][<span class="number">2</span>])  <span class="comment"># 第 1 行，第 2 列</span></span><br><span class="line">    x[<span class="number">1</span>, <span class="number">2</span>] = <span class="number">114</span>  <span class="comment"># 修改</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;modify:\n&quot;</span>, x)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">initial:</span><br><span class="line"> tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11]])</span><br><span class="line">last:</span><br><span class="line"> tensor([ 8,  9, 10, 11])</span><br><span class="line">row 1 ~ 2:</span><br><span class="line"> tensor([[ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11]])</span><br><span class="line">row 0 ~ 1 and column 1 ~ 2:</span><br><span class="line"> tensor([[1, 2],</span><br><span class="line">        [5, 6]])</span><br><span class="line">column 2:</span><br><span class="line"> tensor([ 2,  6, 10])</span><br><span class="line">row 1:</span><br><span class="line"> tensor([4, 5, 6, 7])</span><br><span class="line">row 1 and col 2:</span><br><span class="line"> tensor(6) tensor(6)</span><br><span class="line">modify:</span><br><span class="line"> tensor([[  0,   1,   2,   3],</span><br><span class="line">        [  4,   5, 114,   7],</span><br><span class="line">        [  8,   9,  10,  11]])</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="节省内存">节省内存</h3><p>例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_memory</span>():</span><br><span class="line">    x = torch.arange(<span class="number">4</span>)</span><br><span class="line">    y = torch.arange(<span class="number">4</span>)</span><br><span class="line">    old = <span class="built_in">id</span>(y)</span><br><span class="line">    y = x + y</span><br><span class="line">    <span class="built_in">print</span>(old == <span class="built_in">id</span>(y))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><p>***这是致命的！***在深度学习中我们可能有百兆级的数据，频繁地分配新内存会造成极大的浪费</p><p>幸运的是，复用内存还是比较简单的，只需要使用一下切片操作或<strong>使用<code>op=</code>简化运算符</strong>来进行值覆盖，就可以继续使用旧内存, 即将<code>y = x + y</code> =&gt; <code>y[:] = x + y</code>或<code>y += x</code>。</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_memory</span>():</span><br><span class="line">    x = torch.arange(<span class="number">4</span>)</span><br><span class="line">    y = torch.arange(<span class="number">4</span>)</span><br><span class="line">    old = <span class="built_in">id</span>(y)</span><br><span class="line">    y = x + y</span><br><span class="line">    <span class="built_in">print</span>(old == <span class="built_in">id</span>(y))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;slice operation for reusing memory&quot;</span>)</span><br><span class="line">    <span class="comment"># slice operation for reusing memory</span></span><br><span class="line">    old = <span class="built_in">id</span>(x)</span><br><span class="line">    x[:] = x + y</span><br><span class="line">    <span class="built_in">print</span>(old == <span class="built_in">id</span>(x))</span><br><span class="line">    x += y</span><br><span class="line">    <span class="built_in">print</span>(old == <span class="built_in">id</span>(x))</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">False</span><br><span class="line">slice operation for reusing memory</span><br><span class="line">True</span><br><span class="line">True</span><br></pre></td></tr></table></figure><h3 id="与python对象转换">与python对象转换</h3><p>如题</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_transform</span>():</span><br><span class="line">    x = torch.arange(<span class="number">4</span>)</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line">    y = x.numpy()</span><br><span class="line">    <span class="built_in">print</span>(y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(x), <span class="built_in">type</span>(y))</span><br><span class="line">    <span class="comment"># 大小为 1 的张量可以转化为 python标量</span></span><br><span class="line">    z = torch.tensor([<span class="number">1.14</span>])</span><br><span class="line">    <span class="built_in">print</span>(z, z.item(), <span class="built_in">float</span>(z), <span class="built_in">int</span>(z))  <span class="comment"># 转化成标量可以使用 item()，也可以使用python内置函数 float() 和 int()</span></span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([0, 1, 2, 3])</span><br><span class="line">[0 1 2 3]</span><br><span class="line">&lt;class &#x27;torch.Tensor&#x27;&gt; &lt;class &#x27;numpy.ndarray&#x27;&gt;</span><br><span class="line">tensor([1.1400]) 1.1399999856948853 1.1399999856948853 1</span><br></pre></td></tr></table></figure><h2 id="数据预处理">数据预处理</h2><p>为了能用深度学习解决现实问题，第一步就是要<strong>对原始数据进行预处理</strong>而不是始于已经准备好的张量格式数据，此处介绍如何使用<code>pandas库</code>预处理原始数据，并将原始数据转化为张量格式。<code>pandas</code>可以与张量兼容。</p><p>导入<code>pandas</code>库</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><h3 id="读取数据">读取数据</h3><p><code>pandas.read_csv(...)</code>：读取.csv格式的数据</p><p><code>pandas.read_excel(...)</code>：读取.xls或.xlsx格式的数据</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_data_read</span>():</span><br><span class="line">    os.makedirs(os.path.join(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">    data_file = os.path.join(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line">    <span class="comment"># create data</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment">#        房屋数量  巷子类型 房屋价格</span></span><br><span class="line">        f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)  <span class="comment"># 列名</span></span><br><span class="line">        f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)  <span class="comment"># 每行表示一个数据样本</span></span><br><span class="line">        f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)  <span class="comment"># NA为缺失值</span></span><br><span class="line">        f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">        f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br><span class="line">    <span class="comment"># use pandas read data</span></span><br><span class="line">    df = pd.read_csv(data_file)</span><br><span class="line">    <span class="built_in">print</span>(df)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   NumRooms Alley   Price</span><br><span class="line">0       NaN  Pave  127500</span><br><span class="line">1       2.0   NaN  106000</span><br><span class="line">2       4.0   NaN  178100</span><br><span class="line">3       NaN   NaN  140000</span><br></pre></td></tr></table></figure><h3 id="缺失值处理">缺失值处理</h3><p>两种方法：<code>插值法</code>和<code>删除法</code>，此处介绍插值法</p><h4 id="插值法">插值法</h4><p>即用一个替代值弥补缺失值</p><ol><li>连续值：对于一列/行的缺失值，可以用该行/列的<strong>均值</strong>来替代</li></ol><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">insert_missing</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="comment"># data来自create_data_read</span></span><br><span class="line">    inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, <span class="number">2</span>]  <span class="comment"># inputs 取前两列，outputs取第三列 Price(暂时用不到)</span></span><br><span class="line">    <span class="built_in">print</span>(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为连续值的缺失值插值</span></span><br><span class="line">    inputs = inputs.fillna(inputs.mean())  <span class="comment"># 以均值替换NaN</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;after inserting for continuous variable:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   NumRooms Alley   Price</span><br><span class="line">0       NaN  Pave  127500</span><br><span class="line">1       2.0   NaN  106000</span><br><span class="line">2       4.0   NaN  178100</span><br><span class="line">3       NaN   NaN  140000</span><br><span class="line">   NumRooms Alley</span><br><span class="line">0       NaN  Pave</span><br><span class="line">1       2.0   NaN</span><br><span class="line">2       4.0   NaN</span><br><span class="line">3       NaN   NaN</span><br><span class="line">after inserting for continuous variable:</span><br><span class="line">   NumRooms Alley</span><br><span class="line">0       3.0  Pave</span><br><span class="line">1       2.0   NaN</span><br><span class="line">2       4.0   NaN</span><br><span class="line">3       3.0   NaN</span><br></pre></td></tr></table></figure><ol start="2"><li>离散值：对于一行/列的缺失值，可以将NaN也视作一个类别（离散值）。在此例中，“巷子类型”只有<code>Pave</code> 与<code>NaN</code>两种类型，因此pandas可以自动将Alley列转化为<code>Alley_Pave</code>和<code>Alley_nan</code>两列，Alley列为<code>Pave</code>的<code>Alley_Pave=1</code>, <code>Alley_nan=0</code>, 为<code>NaN</code>的反之。使用pandas中的get_dummies方法</li></ol><p><code>pandas.get_dummies(...)</code>：将传入数据的分类变量转化为虚拟变量(<code>one-hot编码</code>)，<code>dummy_na</code>参数决定是否为缺失值额外创建一个虚拟列，<code>True</code>为创建。</p><blockquote><p>coed</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">insert_missing</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="comment"># data来自create_data_read</span></span><br><span class="line">    inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, <span class="number">2</span>]  <span class="comment"># inputs 取前两列，outputs取第三列 Price(暂时用不到)</span></span><br><span class="line">    <span class="built_in">print</span>(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为连续值的缺失值插值</span></span><br><span class="line">    inputs = inputs.fillna(inputs.mean())  <span class="comment"># 以均值替换NaN</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;after inserting for continuous variable:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为离散值的缺失值插值</span></span><br><span class="line">    inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)  <span class="comment"># dummy_na=True, 表示为缺失值创建一个新特征</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;after inserting for discrete variable:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(inputs)</span><br><span class="line">    <span class="keyword">return</span> inputs, outputs</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   NumRooms Alley   Price</span><br><span class="line">0       NaN  Pave  127500</span><br><span class="line">1       2.0   NaN  106000</span><br><span class="line">2       4.0   NaN  178100</span><br><span class="line">3       NaN   NaN  140000</span><br><span class="line">   NumRooms Alley</span><br><span class="line">0       NaN  Pave</span><br><span class="line">1       2.0   NaN</span><br><span class="line">2       4.0   NaN</span><br><span class="line">3       NaN   NaN</span><br><span class="line">after inserting for continuous variable:</span><br><span class="line">   NumRooms Alley</span><br><span class="line">0       3.0  Pave</span><br><span class="line">1       2.0   NaN</span><br><span class="line">2       4.0   NaN</span><br><span class="line">3       3.0   NaN</span><br><span class="line">after inserting for discrete variable:</span><br><span class="line">   NumRooms  Alley_Pave  Alley_nan</span><br><span class="line">0       3.0           1          0</span><br><span class="line">1       2.0           0          1</span><br><span class="line">2       4.0           0          1</span><br><span class="line">3       3.0           0          1</span><br></pre></td></tr></table></figure><h4 id="删除法">删除法</h4><p>直接忽略缺失值</p><h3 id="转换为张量格式">转换为张量格式</h3><p>经过<code>读取数据</code>和<code>缺失值处理</code>后得到的所有条目都是数值类型的，它们可以被转化为张量格式，以方便调用<code>pytorch</code>中的张量函数便捷地处理数据。</p><p><code>pandas.read_csv()</code>得到的数据是<code>DataFrame</code>格式的，先调用其<code>to_numpy</code>方法转化为numpy数组，再利用<code>torch.tensor()</code>方法将numpy数组转化为张量格式。</p><p><code>DataFrame.to_numpy(dtype=...)</code>：将DataFrame数据转化为numpy数组，<code>dtype</code>指定numpy数组的元素类型</p><blockquote><p>code</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transfer_tensor</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="comment"># data来自insert_missing</span></span><br><span class="line">    inputs, outputs = data</span><br><span class="line">    x = torch.tensor(inputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">    y = torch.tensor(outputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line">    <span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><blockquote><p>output</p></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[3., 1., 0.],</span><br><span class="line">        [2., 0., 1.],</span><br><span class="line">        [4., 0., 1.],</span><br><span class="line">        [3., 0., 1.]], dtype=torch.float64)</span><br><span class="line">tensor([127500., 106000., 178100., 140000.], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 数据处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>回溯</title>
      <link href="/ymhui.github.io/2024/09/13/%E5%9B%9E%E6%BA%AF/"/>
      <url>/ymhui.github.io/2024/09/13/%E5%9B%9E%E6%BA%AF/</url>
      
        <content type="html"><![CDATA[<h1>回溯</h1><p>回溯法是<strong>递归</strong>的子集，通过将原问题分割成可迭代的子问题，将当前元素与更小的结果进行组合（就像回溯一样，回去和子结果组合）进行求解</p><h2 id="例题">例题</h2><h3 id="全排列">全排列</h3><h4 id="题目">题目</h4><p>给定一个不含重复数字的数组 <code>nums</code> ，返回其 <em>所有可能的全排列</em> 。你可以 <strong>按任意顺序</strong> 返回答案。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [1,2,3]</span><br><span class="line">输出：[[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]]</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [0,1]</span><br><span class="line">输出：[[0,1],[1,0]]</span><br></pre></td></tr></table></figure><p><strong>示例 3：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [1]</span><br><span class="line">输出：[[1]]</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>1 &lt;= nums.length &lt;= 6</code></li><li><code>-10 &lt;= nums[i] &lt;= 10</code></li><li><code>nums</code> 中的所有整数 <strong>互不相同</strong></li></ul><h4 id="分析">分析</h4><p>全排列问题是回溯法的经典题型。<code>[a, b, c, ...]</code>的全排列<code>tot(n)</code><strong>可以视作第一个元素和之后所有元素的全排列进行组合</strong>，第一个“主导”元素可以由所有元素轮流担任，因此将该问题分解为<code>n * tot(n - 1)</code>，后者可以继续迭代分解（很像全排列公式）；<strong>当数组中只有一个元素时，全排列就是自己，直接返回</strong>（基本情况）。</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; <span class="title function_">permute</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length == <span class="number">1</span>) &#123;</span><br><span class="line">            List&lt;List&lt;Integer&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">            List&lt;Integer&gt; l = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">            l.add(nums[<span class="number">0</span>]);</span><br><span class="line">            list.add(l);</span><br><span class="line">            <span class="keyword">return</span> list;</span><br><span class="line">        &#125;</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            <span class="comment">// 第0个位置让每个数都当一下</span></span><br><span class="line">            <span class="comment">// swap</span></span><br><span class="line">            swap(nums, <span class="number">0</span>, i);</span><br><span class="line">            <span class="type">int</span>[] subArr = Arrays.copyOfRange(nums, <span class="number">1</span>, nums.length);</span><br><span class="line">            List&lt;List&lt;Integer&gt;&gt; tmpList = permute(subArr);</span><br><span class="line">            <span class="comment">// 向该list加入首元素</span></span><br><span class="line">            <span class="keyword">for</span> (List&lt;Integer&gt; l : tmpList) &#123;</span><br><span class="line">                l.add(<span class="number">0</span>, nums[<span class="number">0</span>]); <span class="comment">// add first element</span></span><br><span class="line">                list.add(l);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// recover swap</span></span><br><span class="line">            swap(nums, <span class="number">0</span>, i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">swap</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> i, <span class="type">int</span> j)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">tmp</span> <span class="operator">=</span> nums[i];</span><br><span class="line">        nums[i] = nums[j];</span><br><span class="line">        nums[j] = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="子集">子集</h3><h4 id="题目-2">题目</h4><p>给你一个整数数组 <code>nums</code> ，数组中的元素 <strong>互不相同</strong> 。返回该数组所有可能的</p><p>子集（幂集）。</p><p>解集 <strong>不能</strong> 包含重复的子集。你可以按 <strong>任意顺序</strong> 返回解集。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [1,2,3]</span><br><span class="line">输出：[[],[1],[2],[1,2],[3],[1,3],[2,3],[1,2,3]]</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [0]</span><br><span class="line">输出：[[],[0]]</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>1 &lt;= nums.length &lt;= 10</code></li><li><code>-10 &lt;= nums[i] &lt;= 10</code></li><li><code>nums</code> 中的所有元素 <strong>互不相同</strong></li></ul><h4 id="分析-2">分析</h4><p>依然采用迭代分解（分治法）。<code>[a, b, c, ...]</code>的幂集<code>P(n)</code><strong>可以看作首元素<code>a</code>与之后的子数组的幂集进行组合</strong>，可以加入<code>a</code>，也可以不加入<code>a</code>，即<code>2 * P(n - 1)</code>（正是幂集公式）。</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; <span class="title function_">subsets</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length == <span class="number">0</span>) &#123;</span><br><span class="line">            List&lt;List&lt;Integer&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">            List&lt;Integer&gt; l = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">            list.add(l);</span><br><span class="line">            <span class="keyword">return</span> list;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 子集可以视作除掉首元素的所有子集加或不加首元素得到的所有集合</span></span><br><span class="line">        <span class="type">int</span>[] subArr = Arrays.copyOfRange(nums, <span class="number">1</span>, nums.length);</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; tmp = subsets(subArr);</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (List&lt;Integer&gt; l : tmp) &#123;</span><br><span class="line">            list.add(l);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (List&lt;Integer&gt; l : tmp) &#123;</span><br><span class="line">            <span class="comment">// 要拷贝一个新l，不然会把之前的那个一起改了（Java特性）</span></span><br><span class="line">            l = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(l);</span><br><span class="line">            l.add(<span class="number">0</span>, nums[<span class="number">0</span>]);</span><br><span class="line">            list.add(l);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="电话号码的字母组合">电话号码的字母组合</h3><h4 id="题目-3">题目</h4><p>给定一个仅包含数字 <code>2-9</code> 的字符串，返回所有它能表示的字母组合。答案可以按 <strong>任意顺序</strong> 返回。</p><p>给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。</p><p><img src="%E5%9B%9E%E6%BA%AF/200px-telephone-keypad2svg.png" alt="img"></p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：digits = &quot;23&quot;</span><br><span class="line">输出：[&quot;ad&quot;,&quot;ae&quot;,&quot;af&quot;,&quot;bd&quot;,&quot;be&quot;,&quot;bf&quot;,&quot;cd&quot;,&quot;ce&quot;,&quot;cf&quot;]</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：digits = &quot;&quot;</span><br><span class="line">输出：[]</span><br></pre></td></tr></table></figure><p><strong>示例 3：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：digits = &quot;2&quot;</span><br><span class="line">输出：[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>0 &lt;= digits.length &lt;= 4</code></li><li><code>digits[i]</code> 是范围 <code>['2', '9']</code> 的一个数字。</li></ul><h4 id="分析-3">分析</h4><p>敲数字d~1~d~2~d~3~…得到的字符串集合<code>S(n)</code>可以视作<strong>首数字<code>d1</code>对应的所有字母和去掉首数字后的数字串对应的字符串集合<code>S(n - 1)</code>的所有组合</strong>，因此得到递归求解的公式<code>S(n) = [one of map(d1)] appends S(n - 1)</code>, <strong>当数字串为空时，返回一个空集合</strong>（基础情况）。</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    String[] alphabets = &#123;<span class="string">&quot;&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="string">&quot;abc&quot;</span>, <span class="string">&quot;def&quot;</span>, <span class="string">&quot;ghi&quot;</span>, <span class="string">&quot;jkl&quot;</span>, <span class="string">&quot;mno&quot;</span>, <span class="string">&quot;pqrs&quot;</span>, <span class="string">&quot;tuv&quot;</span>, <span class="string">&quot;wxyz&quot;</span>&#125;;</span><br><span class="line">    <span class="keyword">public</span> List&lt;String&gt; <span class="title function_">letterCombinations</span><span class="params">(String digits)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (digits.length() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 扣数字的结果可以视作首数字对应的字母和后面的数字串得到的结果的组合</span></span><br><span class="line">        List&lt;String&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        List&lt;String&gt; tmp = letterCombinations(digits.substring(<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">if</span> (tmp.isEmpty()) &#123;</span><br><span class="line">            tmp.add(<span class="string">&quot;&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (String s : tmp) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">char</span> c : alphabets[digits.charAt(<span class="number">0</span>) - <span class="string">&#x27;0&#x27;</span>].toCharArray()) &#123;</span><br><span class="line">                <span class="type">String</span> <span class="variable">t</span> <span class="operator">=</span> s;</span><br><span class="line">                t = c + t;</span><br><span class="line">                list.add(t);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        list.remove(<span class="string">&quot;&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="组合总和">组合总和</h3><h4 id="题目-4">题目</h4><p>给你一个 <strong>无重复元素</strong> 的整数数组 <code>candidates</code> 和一个目标整数 <code>target</code> ，找出 <code>candidates</code> 中可以使数字和为目标数 <code>target</code> 的 所有 <strong>不同组合</strong> ，并以列表形式返回。你可以按 <strong>任意顺序</strong> 返回这些组合。</p><p><code>candidates</code> 中的 <strong>同一个</strong> 数字可以 <strong>无限制重复被选取</strong> 。如果至少一个数字的被选数量不同，则两种组合是不同的。</p><p>对于给定的输入，保证和为 <code>target</code> 的不同组合数少于 <code>150</code> 个。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：candidates = [2,3,6,7], target = 7</span><br><span class="line">输出：[[2,2,3],[7]]</span><br><span class="line">解释：</span><br><span class="line">2 和 3 可以形成一组候选，2 + 2 + 3 = 7 。注意 2 可以使用多次。</span><br><span class="line">7 也是一个候选， 7 = 7 。</span><br><span class="line">仅有这两种组合。</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: candidates = [2,3,5], target = 8</span><br><span class="line">输出: [[2,2,2,2],[2,3,3],[3,5]]</span><br></pre></td></tr></table></figure><p><strong>示例 3：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: candidates = [2], target = 1</span><br><span class="line">输出: [] </span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>1 &lt;= candidates.length &lt;= 30</code></li><li><code>2 &lt;= candidates[i] &lt;= 40</code></li><li><code>candidates</code> 的所有元素 <strong>互不相同</strong></li><li><code>1 &lt;= target &lt;= 40</code></li></ul><h4 id="分析-4">分析</h4><p>candidates是无重复的，这大大减轻了我们的工作量。对于一组可能的组合，比如<code>2 + 2 + 3 = 7</code>, 我们发现<code>2 + 3 = 7 - 2</code>, <code>3 = 7 - 2 - 2</code>, 3就在candidates里（于是找到了一组）！而每次减的元素<code>2</code>也是数组中的元素！所以想到了一种回溯的方式：<strong>只要<code>target</code>仍然不小于<code>2</code>，我们就先判断<code>target</code>是否直接在candidates中存在，然后枚举candidates中的元素<code>can[i]</code>，用同样的方式（递归调用）判断<code>target - can[i]</code>的情况，其结果（不为空）与<code>can[i]</code>的组合就是<code>target</code>情况时的结果</strong>；当上层判断完<code>target - can[i]</code>的情况时，<strong>需将<code>can[i]</code>从candidates中去除，防止计算重复情况</strong>。</p><p>几点优化：</p><ul><li>可以先把<code>candidates</code>排序方便后续处理</li><li>当<code>can[i] &gt;= target</code>时就可以退出了</li></ul><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">     <span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; <span class="title function_">combinationSum</span><span class="params">(<span class="type">int</span>[] candidates, <span class="type">int</span> target)</span> &#123;</span><br><span class="line">        Arrays.sort(candidates);</span><br><span class="line">        <span class="keyword">return</span> combine(candidates, target);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    List&lt;List&lt;Integer&gt;&gt; <span class="title function_">combine</span><span class="params">(<span class="type">int</span>[] candidates, <span class="type">int</span> target)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (target &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        &#125;</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span> (find(candidates, target)) &#123;</span><br><span class="line">            List&lt;Integer&gt; l = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">            l.add(target);</span><br><span class="line">            list.add(l);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (candidates.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">cur</span> <span class="operator">=</span> candidates[<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">if</span> (cur &gt;= target) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            List&lt;List&lt;Integer&gt;&gt; tmpList = combine(candidates, target - cur);</span><br><span class="line">            <span class="keyword">if</span> (!tmpList.isEmpty()) &#123;</span><br><span class="line">                <span class="comment">// 当非空时说明有匹配，所有情况加上主导元素cur</span></span><br><span class="line">                <span class="keyword">for</span> (List&lt;Integer&gt; tmp : tmpList) &#123;</span><br><span class="line">                    tmp.add(<span class="number">0</span>, cur);</span><br><span class="line">                    list.add(tmp);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            candidates = Arrays.copyOfRange(candidates, <span class="number">1</span>, candidates.length); <span class="comment">// 寻找下一个元素主导时排除第一个元素，防止与第一个元素主导时的情况重复</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">find</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> key)</span> &#123;</span><br><span class="line">        <span class="comment">// 二分查找是否存在某个元素key</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>, right = nums.length - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> (left + right) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (nums[mid] == key) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &gt; key) &#123;</span><br><span class="line">                right = mid - <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="括号生成">括号生成</h3><h4 id="题目-5">题目</h4><p>数字 <code>n</code> 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 <strong>有效的</strong> 括号组合。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：n = 3</span><br><span class="line">输出：[&quot;((()))&quot;,&quot;(()())&quot;,&quot;(())()&quot;,&quot;()(())&quot;,&quot;()()()&quot;]</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：n = 1</span><br><span class="line">输出：[&quot;()&quot;]</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>1 &lt;= n &lt;= 8</code></li></ul><h4 id="分析-5">分析</h4><p>一般的分治思想无法解决本题，比如认为<code>n</code>的所有情况可以视作<code>n - 1</code>的所有情况中的每个字符串<code>s</code>与括号的任意组合<code>s() / ()s / (s)</code>，但是这种方法无法生成<code>(())(())</code>。所以我们不能从括号对的角度出发（粒度太大），而<strong>应该从单括号的角度出发，每次只填入一个括号</strong>。具体来说，我们可以<strong>用一个参数记录当前字符串的填写进度，再记录当前还可以填入的左括号以及尚未被匹配的左括号</strong>（右括号不必记录，思考为什么）；若当前还有<code>(</code>可以填入：则可以尝试填入一个<code>(</code>，若当前还有尚未被匹配的<code>(</code>，则也可以尝试填入<code>)</code>；否则：只能填入<code>)</code>。<strong>当没有左括号可以填入且没有尚未被匹配的<code>(</code>时，当前记录的字符串就是一个结果</strong>且不会重复（思考为什么）。</p><p><em>右括号不必记录的原因：右括号不能先填入，只有存在未匹配的左括号时才可以填入右括号，只要左括号还能填，右括号就一定还有剩余，而两个参数记录了左括号的所有信息，所以右括号可以被左括号完全映射。</em></p><p><em>不会重复：从来没有回过头，当前位置填的不是<code>(</code>就是<code>)</code>，当填了<code>(</code>时就一定和填了<code>)</code>的情况不同，不会出现重复。</em></p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;String&gt; <span class="title function_">generateParenthesis</span><span class="params">(<span class="type">int</span> n)</span> &#123;</span><br><span class="line">        <span class="comment">// 以下做法有问题，即以成对括号进行讨论是不可行的</span></span><br><span class="line">        <span class="comment">// 可能有三种情况 ()s / s() / (s), 但是一和二有可能相同</span></span><br><span class="line">        <span class="comment">// 生成不了(())(())，因为不符合上述任何一种情况，它在s3的基础上从中间加入括号</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 因此要单括号枚举</span></span><br><span class="line">        generate(n, <span class="number">0</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; ans = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/***</span></span><br><span class="line"><span class="comment">     * </span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> left 剩余左括号数量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> nowLeft 没被匹配的左括号数量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> current 当前组合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">generate</span><span class="params">(<span class="type">int</span> left, <span class="type">int</span> nowLeft, String current)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (left == <span class="number">0</span> &amp;&amp; nowLeft == <span class="number">0</span>) &#123;</span><br><span class="line">            ans.add(current);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        List&lt;String&gt; ans = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span> (left &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// 还有’(‘可填</span></span><br><span class="line">            generate(left - <span class="number">1</span>, nowLeft + <span class="number">1</span>, current + <span class="string">&quot;(&quot;</span>);</span><br><span class="line">            <span class="keyword">if</span> (nowLeft &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                generate(left,  nowLeft - <span class="number">1</span>, current + <span class="string">&quot;)&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            generate(<span class="number">0</span>, nowLeft - <span class="number">1</span>, current + <span class="string">&quot;)&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="单词搜索">单词搜索</h3><h4 id="题目-6">题目</h4><p>给定一个 <code>m x n</code> 二维字符网格 <code>board</code> 和一个字符串单词 <code>word</code> 。如果 <code>word</code> 存在于网格中，返回 <code>true</code> ；否则，返回 <code>false</code> 。</p><p>单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。</p><p><strong>示例 1：</strong></p><p><img src="%E5%9B%9E%E6%BA%AF/ABCCED.jpg" alt="img"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：board = [[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;E&quot;],[&quot;S&quot;,&quot;F&quot;,&quot;C&quot;,&quot;S&quot;],[&quot;A&quot;,&quot;D&quot;,&quot;E&quot;,&quot;E&quot;]], word = &quot;ABCCED&quot;</span><br><span class="line">输出：true</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><p><img src="%E5%9B%9E%E6%BA%AF/SEE.jpg" alt="img"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：board = [[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;E&quot;],[&quot;S&quot;,&quot;F&quot;,&quot;C&quot;,&quot;S&quot;],[&quot;A&quot;,&quot;D&quot;,&quot;E&quot;,&quot;E&quot;]], word = &quot;SEE&quot;</span><br><span class="line">输出：true</span><br></pre></td></tr></table></figure><p><strong>示例 3：</strong></p><p><img src="%E5%9B%9E%E6%BA%AF/ABCB.jpg" alt="img"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：board = [[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;E&quot;],[&quot;S&quot;,&quot;F&quot;,&quot;C&quot;,&quot;S&quot;],[&quot;A&quot;,&quot;D&quot;,&quot;E&quot;,&quot;E&quot;]], word = &quot;ABCB&quot;</span><br><span class="line">输出：false</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>m == board.length</code></li><li><code>n = board[i].length</code></li><li><code>1 &lt;= m, n &lt;= 6</code></li><li><code>1 &lt;= word.length &lt;= 15</code></li><li><code>board</code> 和 <code>word</code> 仅由大小写英文字母组成</li></ul><h4 id="分析-6">分析</h4><p>回溯法的经典案例，采用DFS，先尝试往一个方向搜索到底。**字串<code>&quot;ABCCED&quot;</code>的匹配过程为：尝试站在<code>A</code>上，匹配<code>&quot;BCCED&quot;</code>，尝试站在<code>B</code>上，匹配<code>&quot;CCED&quot;</code>…（不能走回头路）**每次匹配的过程都是一致的，因此可以递归调用，<strong>当某一次没有任何邻居能匹配上字串首字符时，匹配失败就会返回到上一个位置</strong>（发生回溯），以此类推。<strong>当字串为空时证明匹配成功。</strong></p><p><em>注：匹配过程走不能回头路！</em></p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">exist</span><span class="params">(<span class="type">char</span>[][] board, String word)</span> &#123;</span><br><span class="line">        <span class="type">char</span> <span class="variable">start</span> <span class="operator">=</span> word.charAt(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; board.length; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; board[<span class="number">0</span>].length; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (board[i][j] == start) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (search(board, i, j, word.substring(<span class="number">1</span>))) &#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">search</span><span class="params">(<span class="type">char</span>[][] board, <span class="type">int</span> i, <span class="type">int</span> j, String target)</span> &#123;</span><br><span class="line">        <span class="comment">// 只能上下左右搜</span></span><br><span class="line">        <span class="keyword">if</span> (target.length() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">char</span> <span class="variable">start</span> <span class="operator">=</span> target.charAt(<span class="number">0</span>);</span><br><span class="line">        board[i][j] ^= <span class="number">0xFFFFFFFF</span>; <span class="comment">// 不允许回头</span></span><br><span class="line">        <span class="keyword">if</span> (i - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; board[i - <span class="number">1</span>][j] == start &amp;&amp; search(board, i - <span class="number">1</span>, j, target.substring(<span class="number">1</span>))) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (i + <span class="number">1</span> &lt; board.length &amp;&amp; board[i + <span class="number">1</span>][j] == start &amp;&amp; search(board, i + <span class="number">1</span>, j, target.substring(<span class="number">1</span>))) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (j + <span class="number">1</span> &lt; board[<span class="number">0</span>].length &amp;&amp; board[i][j + <span class="number">1</span>] == start &amp;&amp; search(board, i, j + <span class="number">1</span>, target.substring(<span class="number">1</span>))) &#123;</span><br><span class="line">            <span class="comment">// 注意j是列，长度由第二维度决定</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">boolean</span> <span class="variable">tmp</span> <span class="operator">=</span> j - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; board[i][j - <span class="number">1</span>] == start &amp;&amp; search(board, i, j - <span class="number">1</span>, target.substring(<span class="number">1</span>));</span><br><span class="line">            board[i][j] ^= <span class="number">0xFFFFFFFF</span>; <span class="comment">// recover</span></span><br><span class="line">            <span class="keyword">return</span> tmp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="分割回文串">分割回文串</h3><h4 id="题目-7">题目</h4><p>给你一个字符串 <code>s</code>，请你将 <code>s</code> 分割成一些子串，使每个子串都是<strong>回文串</strong>。返回 <code>s</code> 所有可能的分割方案。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：s = &quot;aab&quot;</span><br><span class="line">输出：[[&quot;a&quot;,&quot;a&quot;,&quot;b&quot;],[&quot;aa&quot;,&quot;b&quot;]]</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：s = &quot;a&quot;</span><br><span class="line">输出：[[&quot;a&quot;]]</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>1 &lt;= s.length &lt;= 16</code></li><li><code>s</code> 仅由小写英文字母组成</li></ul><h4 id="分析-7">分析</h4><p>TBD</p>]]></content>
      
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>compare函数</title>
      <link href="/ymhui.github.io/2024/09/13/compare%E5%87%BD%E6%95%B0/"/>
      <url>/ymhui.github.io/2024/09/13/compare%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1>比较函数</h1><p>许多语言通过定义<strong>compare函数</strong>来定义排序的标准（升序 or 降序），比如C的<code>qsort</code>，Java的优先队列（堆）<code>PriorityQueue</code></p><p>对于<code>compare</code>函数，当<code>升序</code>时</p><ul><li>第一个数小于第二个数，返回负数</li><li>第一个数等于第二个数，返回零</li><li>第一个数大于第二个数，返回正数</li></ul><p>当<code>降序</code>时</p><ul><li>第一个数大于第二个数，返回负数</li><li>第一个数等于第二个数，返回零</li><li>第一个数小于第二个数，返回正数</li></ul><p>以Java<code>降序</code>为例</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="title class_">Comparator</span>&lt;Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(<span class="type">int</span> o1, <span class="type">int</span> o2)</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> o2 - o1; <span class="comment">// 降序如果前面大于后面返回负数</span></span><br><span class="line">            &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 算法基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>子串</title>
      <link href="/ymhui.github.io/2024/09/12/%E5%AD%90%E4%B8%B2/"/>
      <url>/ymhui.github.io/2024/09/12/%E5%AD%90%E4%B8%B2/</url>
      
        <content type="html"><![CDATA[<h1>子串</h1><p>子串指的是一个串中一部分连续元素的集合，是原串的一个子集</p><h2 id="例题">例题</h2><h3 id="和为k的子数组">和为k的子数组</h3><h4 id="题目">题目</h4><p>给你一个整数数组 <code>nums</code> 和一个整数 <code>k</code> ，请你统计并返回 <em>该数组中和为 <code>k</code> 的子数组的个数</em> 。</p><p>子数组是数组中元素的连续非空序列。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [1,1,1], k = 2</span><br><span class="line">输出：2</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [1,2,3], k = 3</span><br><span class="line">输出：2</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>1 &lt;= nums.length &lt;= 2 * 104</code></li><li><code>-1000 &lt;= nums[i] &lt;= 1000</code></li><li><code>-107 &lt;= k &lt;= 107</code></li></ul><h4 id="分析">分析</h4><p>子数组可以被看作两个前缀作差，即<code>arr = longPrefix - shortPrefix</code>。因此目标值<code>k</code>可以通过<strong>前缀和的差</strong>求得，即<code>k = sum[long] - sum[short]</code>，移项有<code>sum[long] = sum[short] + k</code>. 同时注意到<code>sum[long]</code>的求得总是在<code>sum[short]</code>右侧发生，因此我们可以<strong>一边求前缀和并把加上<code>k</code>的结果（<code>short + k</code>）存入一张表，一边验证当前的前缀和<code>long</code>是否等于之前的某个存好的结果</strong>；这种情况每出现一次，就说明我们找到了至少一个（可能多个比如有两种short，每出现一次long就会产生两个）子数组。</p><p>综上，我们维护一张哈希表，<strong>以前缀和加k<code>short + k</code>为键，以这个结果值出现的次数为值</strong>，一边求和一边维护的行为保证了<strong>当前计算的前缀和一定是之前就算好并保存的前缀和的后继</strong>，即一定是<code>long</code>。</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"> <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">subarraySum</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> k)</span> &#123;</span><br><span class="line">        <span class="comment">// 加入哈希的优化</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        * 上述前缀和作差得到的一个子串可以视作 preLong - preShort = k</span></span><br><span class="line"><span class="comment">        * 即 preLong = preShort + k</span></span><br><span class="line"><span class="comment">        * 所以先遍历一遍生成一次前缀和，将该前缀和(preShort)加上k的结果作为key，查找这个key是否存在一个前缀和(preLong)与之相等</span></span><br><span class="line"><span class="comment">        * */</span></span><br><span class="line">        Map&lt;Integer, Integer&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;(); <span class="comment">// key: 前缀和  value: 对应前缀和出现的次数</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>, ans = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 注意需要保证必须是 long - k = short，即靠后的前缀和减去k的前缀和应当对于靠前的前缀和，即先加入map的前缀和</span></span><br><span class="line">        map.put(sum, <span class="number">1</span>); <span class="comment">// 前缀和0出现一次</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            sum += nums[i];</span><br><span class="line">            <span class="keyword">if</span> (map.containsKey(sum - k)) &#123;</span><br><span class="line">                ans += map.get(sum - k);</span><br><span class="line">            &#125;</span><br><span class="line">            map.put(sum, map.getOrDefault(sum, <span class="number">0</span>) + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="滑动窗口最大值">滑动窗口最大值</h3><h4 id="题目-2">题目</h4><p>给你一个整数数组 <code>nums</code>，有一个大小为 <code>k</code> 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 <code>k</code> 个数字。滑动窗口每次只向右移动一位。</p><p>返回 <em>滑动窗口中的最大值</em> 。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [1,3,-1,-3,5,3,6,7], k = 3</span><br><span class="line">输出：[3,3,5,5,6,7]</span><br><span class="line">解释：</span><br><span class="line">滑动窗口的位置                最大值</span><br><span class="line">---------------               -----</span><br><span class="line">[1  3  -1] -3  5  3  6  7       3</span><br><span class="line"> 1 [3  -1  -3] 5  3  6  7       3</span><br><span class="line"> 1  3 [-1  -3  5] 3  6  7       5</span><br><span class="line"> 1  3  -1 [-3  5  3] 6  7       5</span><br><span class="line"> 1  3  -1  -3 [5  3  6] 7       6</span><br><span class="line"> 1  3  -1  -3  5 [3  6  7]      7</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [1], k = 1</span><br><span class="line">输出：[1]</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>1 &lt;= nums.length &lt;= 105</code></li><li><code>-104 &lt;= nums[i] &lt;= 104</code></li><li><code>1 &lt;= k &lt;= nums.length</code></li></ul><h4 id="分析-2">分析</h4><p><em>只维护一个一般的线性滑动窗口移动并更新其中最大值会超时。</em></p><p>解法一：使用最大堆。每个位置的数据有两个指标，<strong>下标和数值</strong>。想象这样一个情景，最初始的窗口在移动，我们每次移动窗口就往窗口中加入一个值，每次获取其中的最大值，<strong>只要这个最大值的下标仍在实际的滑动窗口范围内，我们仍然可以继续使用它，否则选拔一个新的最大值，以此类推</strong>。为此我们想到可以使用<code>最大堆 </code>，堆顶元素即是最大值，**每次只需要确认堆顶元素是否还在滑动窗口内即可，如果不在，就一直出堆直到堆顶元素在滑动窗口内；滑动窗口移动，就是让新元素入堆。**在一开始初始化一个初始堆（初始窗口）并维护它即可。</p><p>堆中元素采用一个数对，即<code>&#123;在数组中的下标，数值&#125;</code></p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span>[] maxSlidingWindow(<span class="type">int</span>[] nums, <span class="type">int</span> k) &#123;</span><br><span class="line">        <span class="comment">// &#123;index, value&#125;</span></span><br><span class="line">        PriorityQueue&lt;<span class="type">int</span>[]&gt; heap = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;&gt;((o1, o2) -&gt; &#123;</span><br><span class="line">            <span class="keyword">return</span> o2[<span class="number">1</span>] - o1[<span class="number">1</span>]; <span class="comment">// 降序如果前面大于后面返回负数</span></span><br><span class="line">        &#125;); <span class="comment">// 使用堆</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">            heap.offer(<span class="keyword">new</span> <span class="title class_">int</span>[]&#123;i, nums[i]&#125;); <span class="comment">// offer保证还是堆</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span>[] ans = <span class="keyword">new</span> <span class="title class_">int</span>[nums.length - k + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">if</span> (heap.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        &#125;</span><br><span class="line">        ans[<span class="number">0</span>] = heap.peek()[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= nums.length - k; i++) &#123;</span><br><span class="line">            heap.offer(<span class="keyword">new</span> <span class="title class_">int</span>[] &#123;i + k - <span class="number">1</span>, nums[i + k - <span class="number">1</span>]&#125;); </span><br><span class="line">            <span class="type">int</span>[] tmp = heap.peek();</span><br><span class="line">            <span class="keyword">while</span> (tmp[<span class="number">0</span>] &lt; i || tmp[<span class="number">0</span>] &gt; i + k - <span class="number">1</span>) &#123;</span><br><span class="line">                heap.poll();</span><br><span class="line">                tmp = heap.peek();</span><br><span class="line">            &#125;</span><br><span class="line">            ans[i] = tmp[<span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解法二：使用单调队列。 TBD</p><h3 id="最小覆盖子串">最小覆盖子串</h3><h4 id="题目-3">题目</h4><p>给你一个字符串 <code>s</code> 、一个字符串 <code>t</code> 。返回 <code>s</code> 中涵盖 <code>t</code> 所有字符的最小子串。如果 <code>s</code> 中不存在涵盖 <code>t</code> 所有字符的子串，则返回空字符串 <code>&quot;&quot;</code> 。</p><p><strong>注意：</strong></p><ul><li>对于 <code>t</code> 中重复字符，我们寻找的子字符串中该字符数量必须不少于 <code>t</code> 中该字符数量。</li><li>如果 <code>s</code> 中存在这样的子串，我们保证它是唯一的答案。</li></ul><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：s = &quot;ADOBECODEBANC&quot;, t = &quot;ABC&quot;</span><br><span class="line">输出：&quot;BANC&quot;</span><br><span class="line">解释：最小覆盖子串 &quot;BANC&quot; 包含来自字符串 t 的 &#x27;A&#x27;、&#x27;B&#x27; 和 &#x27;C&#x27;。</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：s = &quot;a&quot;, t = &quot;a&quot;</span><br><span class="line">输出：&quot;a&quot;</span><br><span class="line">解释：整个字符串 s 是最小覆盖子串。</span><br></pre></td></tr></table></figure><p><strong>示例 3:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: s = &quot;a&quot;, t = &quot;aa&quot;</span><br><span class="line">输出: &quot;&quot;</span><br><span class="line">解释: t 中两个字符 &#x27;a&#x27; 均应包含在 s 的子串中，</span><br><span class="line">因此没有符合条件的子字符串，返回空字符串。</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>m == s.length</code></li><li><code>n == t.length</code></li><li><code>1 &lt;= m, n &lt;= 105</code></li><li><code>s</code> 和 <code>t</code> 由英文字母组成</li></ul><h4 id="分析-3">分析</h4><p>本题可以采用<strong>滑动窗口</strong>算法。维护一个滑动窗口，不断向右扩张，<strong>如果左边界的字符超出了<code>t</code>中的所有字符，就收缩左边界直到窗口清零或左边界字符仍在<code>t</code>的字符范围内</strong>，每次窗口扩张前都要判断一下是否已经完成覆盖，记录覆盖结果中出现的最小子串。</p><p>tip：判断字符数量有没有到达要求可以再<strong>维护一个<code>count</code>数组记录<code>t</code>中字符出现的次数（也代指窗口中还需要出现的次数）</strong>，<code>0</code>即为没有(窗口中不需要)出现；之后在维护窗口时，<strong>右边界纳入的字符即代表窗口覆盖到了它，因此<code>count</code>数组对应位置自减</strong>，如果一个位置的计数为负数，说明这个字符在子串中是多余的，左边界可以收缩将它踢出窗口（记得更新<code>count</code>）。如果<code>count</code>数组没有正数，说明当前窗口覆盖了<code>t</code>。</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">   <span class="keyword">public</span> String <span class="title function_">minWindow</span><span class="params">(String s, String t)</span> &#123;</span><br><span class="line">        <span class="type">char</span>[] sArr = s.toCharArray();</span><br><span class="line">        <span class="type">int</span>[] count = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="string">&#x27;z&#x27;</span> - <span class="string">&#x27;A&#x27;</span> + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">char</span> c : t.toCharArray()) &#123;</span><br><span class="line">            count[c - <span class="string">&#x27;A&#x27;</span>]++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>, min = Integer.MAX_VALUE;</span><br><span class="line">        <span class="type">int</span> <span class="variable">minStart</span> <span class="operator">=</span> <span class="number">0</span>, minEnd = <span class="number">0</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">sub</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">right</span> <span class="operator">=</span> <span class="number">0</span>; right &lt; sArr.length; right++) &#123;</span><br><span class="line">            <span class="type">char</span> <span class="variable">l</span> <span class="operator">=</span> sArr[left], r = sArr[right];</span><br><span class="line">            count[r - <span class="string">&#x27;A&#x27;</span>]--;</span><br><span class="line">            <span class="keyword">while</span> (left &lt; right &amp;&amp; count[l - <span class="string">&#x27;A&#x27;</span>] &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                left++;</span><br><span class="line">                count[l - <span class="string">&#x27;A&#x27;</span>]++;</span><br><span class="line">                l = sArr[left];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (match(count)) &#123;</span><br><span class="line">                <span class="comment">// find a substring</span></span><br><span class="line">                <span class="keyword">if</span> (min &gt; right - left + <span class="number">1</span>) &#123;</span><br><span class="line">                    min = right - left + <span class="number">1</span>;</span><br><span class="line">                    minStart = left;</span><br><span class="line">                    minEnd = right;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// renew sub</span></span><br><span class="line">        <span class="keyword">if</span> (min &lt; Integer.MAX_VALUE) &#123;</span><br><span class="line">            sub = s.substring(minStart, minEnd + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> sub;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">match</span><span class="params">(<span class="type">int</span>[] count)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> n : count) &#123;</span><br><span class="line">            <span class="keyword">if</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>哈希</title>
      <link href="/ymhui.github.io/2024/09/11/%E5%93%88%E5%B8%8C/"/>
      <url>/ymhui.github.io/2024/09/11/%E5%93%88%E5%B8%8C/</url>
      
        <content type="html"><![CDATA[<h1>哈希</h1><p>哈希是一种非常重要的算法思想，通过利用某些数据相同的特征对其进行分组，从而大大提高查询时的效率</p><p>关键：发现特征，构造函数，避免/减少冲突</p><h2 id="例题">例题</h2><h3 id="两数之和">两数之和</h3><h4 id="题目">题目</h4><p>给定一个整数数组 <code>nums</code> 和一个整数目标值 <code>target</code>，请你在该数组中找出 <strong>和为目标值</strong> <em><code>target</code></em> 的那 <strong>两个</strong> 整数，并返回它们的数组下标。</p><p>你可以假设每种输入只会对应一个答案，并且你不能使用两次相同的元素。</p><p>你可以按任意顺序返回答案。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [2,7,11,15], target = 9</span><br><span class="line">输出：[0,1]</span><br><span class="line">解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [3,2,4], target = 6</span><br><span class="line">输出：[1,2]</span><br></pre></td></tr></table></figure><p><strong>示例 3：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [3,3], target = 6</span><br><span class="line">输出：[0,1]</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>2 &lt;= nums.length &lt;= 104</code></li><li><code>-109 &lt;= nums[i] &lt;= 109</code></li><li><code>-109 &lt;= target &lt;= 109</code></li><li><strong>只会存在一个有效答案</strong></li></ul><h4 id="分析">分析</h4><p>由于题目限定了只存在一个有效答案，即<strong>只存在一种组合(x, target - x)</strong>，所以考虑使用<strong>数+补数</strong>的思路进行搜索，即如果一个数x的补数target - x不在数组中，那它一定不在答案中。</p><p>考虑到原有的数组中可能存在重复元素，且有可能出现3 + 3 = 6这种组合，所以不能简单粗暴地将原数组转为Java中的<code>HashSet</code>。因此我们选择先新建一个<strong>空set</strong>，然后遍历原数组，每次拿到一个数时，先查看一下它的补数在不在<code>set</code>中，如果在，算法结束；否则将此数加入<code>set</code>。如果没有数重复，此算法一定可行, 即使存在重复数，如果不是恰好<code>x + x = target</code>的形式，这个数最多有一个就够用了（可能成为其他数的补数），直接加入<code>set</code>去重即可; 如果恰好为<code>x + x = target</code>形式，那么这个数第一次被访问到时会加入<code>set</code>, 再次访问到时它的补数（即自身）已经在<code>set</code>里了，所以找到结果算法结束，因此算法仍然可行。</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">   <span class="keyword">public</span> <span class="type">int</span>[] twoSum(<span class="type">int</span>[] nums, <span class="type">int</span> target) &#123;</span><br><span class="line">        HashMap&lt;Integer, Integer&gt; hashMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        <span class="comment">// 先不初始化map，直接对数做查询操作</span></span><br><span class="line">        <span class="comment">// key存数值，value存索引，假定有重复数，只有当这两个数正好加起来等于target才会被选中，否则无意义，覆盖也没关系，只要有一个还在map中（备选）即可</span></span><br><span class="line">        <span class="comment">// 而当两个相同的数刚好加起来等于target时，他们被选中时一定一个在map中一个在手中，所有不会被覆盖，直接return了</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">fill</span> <span class="operator">=</span> target - nums[i];</span><br><span class="line">            <span class="keyword">if</span> (hashMap.containsKey(fill)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;i, hashMap.get(fill)&#125;;</span><br><span class="line">            &#125;</span><br><span class="line">            hashMap.put(nums[i], i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">0</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字母异位词分组">字母异位词分组</h3><h4 id="题目-2">题目</h4><p>给你一个字符串数组，请你将 <strong>字母异位词</strong> 组合在一起。可以按任意顺序返回结果列表。</p><p><strong>字母异位词</strong> 是由重新排列源单词的所有字母得到的一个新单词。</p><p><strong>示例 1:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: strs = [&quot;eat&quot;, &quot;tea&quot;, &quot;tan&quot;, &quot;ate&quot;, &quot;nat&quot;, &quot;bat&quot;]</span><br><span class="line">输出: [[&quot;bat&quot;],[&quot;nat&quot;,&quot;tan&quot;],[&quot;ate&quot;,&quot;eat&quot;,&quot;tea&quot;]]</span><br></pre></td></tr></table></figure><p><strong>示例 2:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: strs = [&quot;&quot;]</span><br><span class="line">输出: [[&quot;&quot;]]</span><br></pre></td></tr></table></figure><p><strong>示例 3:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: strs = [&quot;a&quot;]</span><br><span class="line">输出: [[&quot;a&quot;]]</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>1 &lt;= strs.length &lt;= 104</code></li><li><code>0 &lt;= strs[i].length &lt;= 100</code></li><li><code>strs[i]</code> 仅包含小写字母</li></ul><h4 id="分析-2">分析</h4><p>字母异位词的相同特征是拥有完全相同的字符，因此他们<strong>按字典序排列自己的字符结果一定一样</strong>。所以可以建立一个哈希表，<strong>key是字符按字典序排列的字符串，value是key的字母异位词。</strong></p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;List&lt;String&gt;&gt; <span class="title function_">groupAnagrams</span><span class="params">(String[] strs)</span> &#123;</span><br><span class="line">        <span class="comment">// 本题关键：找到合适的key</span></span><br><span class="line">        <span class="comment">// 参考解法：将字符串转为字符数组后对数组排序再转为字符串，同一组的字符串相同</span></span><br><span class="line">        HashMap&lt;String, List&lt;String&gt;&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String str : strs) &#123;</span><br><span class="line">            <span class="type">char</span>[] tmp = str.toCharArray();</span><br><span class="line">            Arrays.sort(tmp); <span class="comment">// 字符串按字典序</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">ranked</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(tmp); </span><br><span class="line">            <span class="keyword">if</span> (map.containsKey(ranked)) &#123;</span><br><span class="line">                <span class="comment">// 之前已经有同类</span></span><br><span class="line">                List&lt;String&gt; list = map.get(ranked);</span><br><span class="line">                list.add(str);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 自己是第一个</span></span><br><span class="line">                List&lt;String&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                list.add(str);</span><br><span class="line">                map.put(ranked, list);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// map构建完毕        </span></span><br><span class="line">        <span class="comment">// 转化为答案</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(map.values());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="最长连续序列">最长连续序列</h3><h4 id="题目-3">题目</h4><p>给定一个未排序的整数数组 <code>nums</code> ，找出数字连续的最长序列（不要求序列元素在原数组中连续）的长度。</p><p>请你设计并实现时间复杂度为 <code>O(n)</code> 的算法解决此问题。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [100,4,200,1,3,2]</span><br><span class="line">输出：4</span><br><span class="line">解释：最长数字连续序列是 [1, 2, 3, 4]。它的长度为 4。</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [0,3,7,2,5,8,4,6,0,1]</span><br><span class="line">输出：9</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>0 &lt;= nums.length &lt;= 105</code></li><li><code>-109 &lt;= nums[i] &lt;= 109</code></li></ul><h4 id="分析-3">分析</h4><p>一个连续序列如果是最长的，那么它的开头必然是这个序列组中最小的，<strong>即它在数组中不应该有直接前驱</strong>。换言之，如果一个数组中的数，它的<strong>直接前驱在数组中，那么它一定不是一个最长连续序列的开头</strong>，就<strong>不需要考虑以它为始的情况</strong>了。</p><p>那么该如何寻找一个序列的伙伴呢？只需要<strong>不断查询当前数的后继是否在数组中</strong>即可，若在，则长度加1，继续循环；反之退出，当前长度就是这个序列的极限长度，找出这些极限长度中的最大值即可。</p><p><em>注：本题不能排序，至少我不知道有什么复杂度为O(n)的排序算法，查询采用 Java的 HashSet（数组没有查询的轮子），因为重复数字对于连续序列是无意义的</em></p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">longestConsecutive</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="comment">// 依然采取HashSet进行去重，不过在数值选择上有所改进</span></span><br><span class="line">        <span class="comment">// 最多使用一个set，多用会增加查询开销</span></span><br><span class="line">        Set&lt;Integer&gt; set = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> n : nums) &#123;</span><br><span class="line">            set.add(n);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> <span class="variable">max</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 开始查询</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> n : set) &#123;</span><br><span class="line">            <span class="keyword">if</span> (set.contains(n - <span class="number">1</span>)) &#123;</span><br><span class="line">                <span class="comment">// 如果有直接前驱，必然不可能是一个最长的连续序列的开头，直接跳过即可</span></span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 到这里这个数没有直接前驱，可能是开头</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">curLen</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">while</span> (set.contains(n + <span class="number">1</span>)) &#123;</span><br><span class="line">                curLen++; <span class="comment">// 只要直接后继在里面，就说明这个连续序列可以加长 1</span></span><br><span class="line">                n++;</span><br><span class="line">            &#125;</span><br><span class="line">            max = Math.max(max, curLen);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="复杂度分析">复杂度分析</h4><p>直观上这里存在双重循环，看起来复杂度是O(n)。然而我们从n个数的角度来看，如果一个数有直接前驱，那么它在一开始就被跳过了，只有在作为直接前驱的直接后继时才会被访问一次；**换言之，开头的数只会作为开头被访问一次（不是任何数的直接后继），其他数都只会作为直接后继仅被访问一次。**因此n个数每个都只访问一次，复杂度是O(n)</p><h3 id="缺失的第一个正数">缺失的第一个正数</h3><h4 id="题目-4">题目</h4><p>给你一个未排序的整数数组 <code>nums</code> ，请你找出其中没有出现的最小的正整数。</p><p>请你实现时间复杂度为 <code>O(n)</code> 并且只使用常数级别额外空间的解决方案。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [1,2,0]</span><br><span class="line">输出：3</span><br><span class="line">解释：范围 [1,2] 中的数字都在数组中。</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [3,4,-1,1]</span><br><span class="line">输出：2</span><br><span class="line">解释：1 在数组中，但 2 没有。</span><br></pre></td></tr></table></figure><p><strong>示例 3：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [7,8,9,11,12]</span><br><span class="line">输出：1</span><br><span class="line">解释：最小的正数 1 没有出现。</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>1 &lt;= nums.length &lt;= 105</code></li><li><code>-231 &lt;= nums[i] &lt;= 231 - 1</code></li></ul><h4 id="分析-4">分析</h4><p>要满足题目的复杂度需求，本题使用<strong>原地哈希</strong>。因为一个数组可以被看作是一张哈希表，所以把输入的nums看作哈希表，对它进行更新。</p><p>因为本题找的是最小的缺失正数，<strong>根据鸽巢原理</strong>，这个数一定位于**[1, n + 1]**中，问题的关键在于我们怎么记录这个区间中的一个数是否出现过，一个想法是将位于[1, n + 1]中的数放到对应下标的位置，结束后如果某个位置的数和下标(或减1)不一致则说明这个数没有出现，最小的这种数就是答案。因此我们考虑遍历整个数组，将读到的正数放到对应下标位置（如果可以），遍历完后再遍历一遍找答案。</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">firstMissingPositive</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="comment">// 遍历数组，将遇到的正数放入其值作为下标的位置，如果超出就不动</span></span><br><span class="line">        <span class="comment">// 之后再按顺序遍历一遍数组，若某个位置的下标和元素值不同，则说明他没有出现，第一个就是最小的</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i != nums[i] - <span class="number">1</span> &amp;&amp; nums[i] &gt; <span class="number">0</span> &amp;&amp; nums[i] &lt;= nums.length) &#123;</span><br><span class="line">                <span class="type">int</span> <span class="variable">tmp</span> <span class="operator">=</span> nums[nums[i] - <span class="number">1</span>];</span><br><span class="line">                <span class="keyword">if</span> (nums[i] == tmp) &#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                nums[nums[i] - <span class="number">1</span>] = nums[i];</span><br><span class="line">                nums[i] = tmp;</span><br><span class="line">                i--; <span class="comment">// 这个换过来的数可能还没归位，我们保证一下</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] - <span class="number">1</span> != i) &#123;</span><br><span class="line">                <span class="keyword">return</span> i + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> nums.length + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="考场就座">考场就座</h3><h4 id="题目-5">题目</h4><p>在考场里，有 <code>n</code> 个座位排成一行，编号为 <code>0</code> 到 <code>n - 1</code>。</p><p>当学生进入考场后，他必须坐在离最近的人最远的座位上。如果有多个这样的座位，他会坐在编号最小的座位上。(另外，如果考场里没有人，那么学生就坐在 <code>0</code> 号座位上。)</p><p>设计一个模拟所述考场的类。</p><p>实现 <code>ExamRoom</code> 类：</p><ul><li><code>ExamRoom(int n)</code> 用座位的数量 <code>n</code> 初始化考场对象。</li><li><code>int seat()</code> 返回下一个学生将会入座的座位编号。</li><li><code>void leave(int p)</code> 指定坐在座位 <code>p</code> 的学生将离开教室。保证座位 <code>p</code> 上会有一位学生。</li></ul><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line">[&quot;ExamRoom&quot;, &quot;seat&quot;, &quot;seat&quot;, &quot;seat&quot;, &quot;seat&quot;, &quot;leave&quot;, &quot;seat&quot;]</span><br><span class="line">[[10], [], [], [], [], [4], []]</span><br><span class="line">输出：</span><br><span class="line">[null, 0, 9, 4, 2, null, 5]</span><br><span class="line">解释：</span><br><span class="line">ExamRoom examRoom = new ExamRoom(10);</span><br><span class="line">examRoom.seat(); // 返回 0，房间里没有人，学生坐在 0 号座位。</span><br><span class="line">examRoom.seat(); // 返回 9，学生最后坐在 9 号座位。</span><br><span class="line">examRoom.seat(); // 返回 4，学生最后坐在 4 号座位。</span><br><span class="line">examRoom.seat(); // 返回 2，学生最后坐在 2 号座位。</span><br><span class="line">examRoom.leave(4);</span><br><span class="line">examRoom.seat(); // 返回 5，学生最后坐在 5 号座位。</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ol><li><code>1 &lt;= n &lt;= 109</code></li><li>保证有学生正坐在座位 <code>p</code> 上。</li><li><code>seat</code> 和 <code>leave</code> 最多被调用 <code>104</code> 次。</li></ol><h4 id="分析-5">分析</h4><p>这题的关键是降低复杂度，超过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>复杂度的应该都会超时。那怎么降低复杂度？其实还是<strong>寻找题目特征，建模，转化问题</strong>。这道题的关键在于怎么快速找到距离最近的人最远的座位。距离最近的人可以看成是到两边有人的位置的距离中较短的那一个，换位思考一下就是<strong>相邻</strong>两个坐了人的位置中间夹着的区间的中点，则区间越长，区间中点的座位距离最近的人越远，相同距离则选择靠前的区间（座位编号小的）。<strong>因此我们只需要用一个哈希表维护座位上有人的区间端点即可</strong>，<code>seat()</code>就是找到合适的位置坐下，并把该位置作为一个坐了人的端点加入哈希表中，<code>leave(p)</code>就是将区间端点从哈希表中移除。</p><p>代码如下：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ExamRoom</span> &#123;</span><br><span class="line">    <span class="comment">// 整理, 题目很不错</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; hash_set; <span class="comment">// 哈希表按顺序记录已经坐了的位置，每个数字代表一个区间端点</span></span><br><span class="line">    <span class="type">int</span> num;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">ExamRoom</span>(<span class="type">int</span> n) &#123;</span><br><span class="line">        num = n;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">seat</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 特判两种特殊情况</span></span><br><span class="line">        <span class="keyword">if</span> (hash_set.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="comment">// 没有人左就坐0</span></span><br><span class="line">            hash_set.<span class="built_in">push_back</span>(<span class="number">0</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;        </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">-1</span>, right = <span class="number">-1</span>, mmax = <span class="number">0</span>; </span><br><span class="line">        <span class="keyword">if</span> (hash_set[<span class="number">0</span>] != <span class="number">0</span>) &#123;</span><br><span class="line">            mmax = hash_set[<span class="number">0</span>] * <span class="number">2</span>;</span><br><span class="line">            left = <span class="number">0</span>;</span><br><span class="line">            right = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (hash_set.<span class="built_in">back</span>() != num - <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (mmax &lt; <span class="number">2</span> * (num - <span class="number">1</span> - hash_set.<span class="built_in">back</span>())) &#123;</span><br><span class="line">                mmax = <span class="number">2</span> * (num - <span class="number">1</span> - hash_set.<span class="built_in">back</span>());</span><br><span class="line">                left = right = num - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 常规情况：</span></span><br><span class="line">        <span class="comment">// 遍历哈希表找到最长的区间，选中点坐下, 存端点时保证升序</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; hash_set.<span class="built_in">size</span>() - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (mmax / <span class="number">2</span> &lt;= (hash_set[i + <span class="number">1</span>] - hash_set[i]) / <span class="number">2</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (left != num - <span class="number">1</span> &amp;&amp; mmax / <span class="number">2</span> == (hash_set[i + <span class="number">1</span>] - hash_set[i]) / <span class="number">2</span>) &#123;</span><br><span class="line">                    <span class="keyword">continue</span>; <span class="comment">// 防止前面出现比最后一个位置更合适的座位，因为选最小的</span></span><br><span class="line">                &#125;</span><br><span class="line">                left = hash_set[i], right = hash_set[i + <span class="number">1</span>];</span><br><span class="line">                mmax = hash_set[i + <span class="number">1</span>] - hash_set[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 端点之间的点肯定没有被坐，不然就在哈希表里了</span></span><br><span class="line">        <span class="type">int</span> pos = (left + right) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">auto</span> it = <span class="built_in">upper_bound</span>(hash_set.<span class="built_in">begin</span>(), hash_set.<span class="built_in">end</span>(), pos);</span><br><span class="line">        hash_set.<span class="built_in">insert</span>(it, pos);</span><br><span class="line">        <span class="keyword">return</span> pos;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">leave</span><span class="params">(<span class="type">int</span> p)</span> </span>&#123;</span><br><span class="line">        hash_set.<span class="built_in">erase</span>(<span class="built_in">remove</span>(hash_set.<span class="built_in">begin</span>(), hash_set.<span class="built_in">end</span>(), p));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 算法 </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>滑动窗口</title>
      <link href="/ymhui.github.io/2024/09/11/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"/>
      <url>/ymhui.github.io/2024/09/11/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/</url>
      
        <content type="html"><![CDATA[<h1>滑动窗口</h1><p>滑动窗口往往用于求特定类型的最长字串，通过在正确的时机对窗口进行平移或扩张来获得最大值。</p><h2 id="例题">例题</h2><h3 id="无重复字符的最长字串">无重复字符的最长字串</h3><h4 id="题目">题目</h4><p>给定一个字符串 <code>s</code> ，请你找出其中不含有重复字符的 <strong>最长</strong> <strong>子串</strong> 的长度。</p><p><strong>示例 1:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: s = &quot;abcabcbb&quot;</span><br><span class="line">输出: 3 </span><br><span class="line">解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。</span><br></pre></td></tr></table></figure><p><strong>示例 2:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: s = &quot;bbbbb&quot;</span><br><span class="line">输出: 1</span><br><span class="line">解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。</span><br></pre></td></tr></table></figure><p><strong>示例 3:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: s = &quot;pwwkew&quot;</span><br><span class="line">输出: 3</span><br><span class="line">解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。</span><br><span class="line">     请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>0 &lt;= s.length &lt;= 5 * 104</code></li><li><code>s</code> 由英文字母、数字、符号和空格组成</li></ul><h4 id="分析">分析</h4><p>题目要求不能有重复字符，所以我们想象现在有一个窗口，透过窗口你可以看到字符串的一部分，现在要求这个窗口内的子串不能有重复字符。我们假设当前的窗口内没有重复字符，那么<strong>可以尝试将窗口向右扩张一格</strong>，如果出现了重复字符，就<strong>从左侧开始收缩整个窗口，直到之前出现过的这个字符移到了窗口外侧</strong>，然后继续向右扩展这个窗口。在这个过程中窗口的最大值即是最长字串的长度（因此每次更新窗口大小的时候都要记录以确定最大值）</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">lengthOfLongestSubstring</span><span class="params">(String s)</span> &#123;</span><br><span class="line">        <span class="type">boolean</span>[] happen = <span class="keyword">new</span> <span class="title class_">boolean</span>[<span class="number">256</span>];</span><br><span class="line">        <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>, max = <span class="number">0</span>; <span class="comment">// 左边界, 最大值</span></span><br><span class="line">        <span class="type">char</span>[] tmp = s.toCharArray();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">right</span> <span class="operator">=</span> <span class="number">0</span>; right &lt; tmp.length; right++) &#123;</span><br><span class="line">            <span class="type">char</span> <span class="variable">c</span> <span class="operator">=</span> tmp[right];</span><br><span class="line">            <span class="keyword">if</span> (happen[c]) &#123;</span><br><span class="line">                <span class="comment">// 重复, 收缩左边界, 直到c被排除</span></span><br><span class="line">                <span class="keyword">while</span> (tmp[left] != c) &#123;</span><br><span class="line">                    <span class="type">char</span> <span class="variable">real</span> <span class="operator">=</span> tmp[left++];</span><br><span class="line">                    happen[real] = <span class="literal">false</span>; <span class="comment">// 收缩时把那些出现一次的字符的标记（happen）置为false（窗口里一定只有仅出现一次的字符）</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 现在left指向前一个c, 去除</span></span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">            happen[c] = <span class="literal">true</span>;</span><br><span class="line">            <span class="comment">// 记录并更新可能的最大值</span></span><br><span class="line">            max = Math.max(max, right - left + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="找到字符串中所有字母异位词">找到字符串中所有字母异位词</h3><h4 id="题目-2">题目</h4><p>给定两个字符串 <code>s</code> 和 <code>p</code>，找到 <code>s</code> 中所有 <code>p</code> 的 <strong>异位词</strong> 的子串，返回这些子串的起始索引。不考虑答案输出的顺序。</p><p><strong>异位词</strong> 指由相同字母重排列形成的字符串（包括相同的字符串）。</p><p><strong>示例 1:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: s = &quot;cbaebabacd&quot;, p = &quot;abc&quot;</span><br><span class="line">输出: [0,6]</span><br><span class="line">解释:</span><br><span class="line">起始索引等于 0 的子串是 &quot;cba&quot;, 它是 &quot;abc&quot; 的异位词。</span><br><span class="line">起始索引等于 6 的子串是 &quot;bac&quot;, 它是 &quot;abc&quot; 的异位词。</span><br></pre></td></tr></table></figure><p><strong>示例 2:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: s = &quot;abab&quot;, p = &quot;ab&quot;</span><br><span class="line">输出: [0,1,2]</span><br><span class="line">解释:</span><br><span class="line">起始索引等于 0 的子串是 &quot;ab&quot;, 它是 &quot;ab&quot; 的异位词。</span><br><span class="line">起始索引等于 1 的子串是 &quot;ba&quot;, 它是 &quot;ab&quot; 的异位词。</span><br><span class="line">起始索引等于 2 的子串是 &quot;ab&quot;, 它是 &quot;ab&quot; 的异位词。</span><br></pre></td></tr></table></figure><p><strong>提示:</strong></p><ul><li><code>1 &lt;= s.length, p.length &lt;= 3 * 104</code></li><li><code>s</code> 和 <code>p</code> 仅包含小写字母</li></ul><h4 id="分析-2">分析</h4><p>首先根据异位词的定义，我们只需要<strong>保证这个子串和目标字符串的构成字符一致</strong>即可，即维护一个窗口，保证窗口中的所有字符和串 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span> 的所有字符一致即可。串 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span> 的特征可以用一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>26</mn></mrow><annotation encoding="application/x-tex">26</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">26</span></span></span></span>位长的数组来表示，即每个字符出现的次数，之后维护窗口时动态的更新另一个用于记录窗口字符信息的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>26</mn></mrow><annotation encoding="application/x-tex">26</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">26</span></span></span></span>位数组，当这两个数组的内容一致时，我们认为找到了这个重排形成的字符串，即窗口框住的串。</p><p>需要注意一些细节来优化性能：</p><ul><li>当<strong>某个字符出现的次数大于目标串中该字符的出现次数时</strong>，该窗口的内容一定不可能满足，此时要<strong>收缩窗口左边界直到该字符满足出现次数要求</strong></li><li>当<strong>某个字符出现的次数小于或等于目标串中该字符的出现次数时</strong>，如果还没匹配上，则窗口需要继续向右扩张</li></ul><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;Integer&gt; <span class="title function_">findAnagrams</span><span class="params">(String s, String p)</span> &#123;</span><br><span class="line">        List&lt;Integer&gt; ans = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">char</span>[] sArr = s.toCharArray(), pArr = p.toCharArray();</span><br><span class="line">        <span class="type">int</span>[] count = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">26</span>];</span><br><span class="line">        <span class="type">int</span>[] countTmp = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">26</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">char</span> c : pArr) &#123;</span><br><span class="line">            count[c - <span class="string">&#x27;a&#x27;</span>]++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; sArr.length; i++) &#123;</span><br><span class="line">            countTmp[sArr[i] - <span class="string">&#x27;a&#x27;</span>]++;</span><br><span class="line">            <span class="keyword">if</span> (countTmp[sArr[i] - <span class="string">&#x27;a&#x27;</span>] &gt; count[sArr[i] - <span class="string">&#x27;a&#x27;</span>]) &#123;</span><br><span class="line">                <span class="keyword">while</span> (left &lt; i &amp;&amp; sArr[left] != sArr[i]) &#123;</span><br><span class="line">                    countTmp[sArr[left] - <span class="string">&#x27;a&#x27;</span>]--;</span><br><span class="line">                    left++;</span><br><span class="line">                &#125;</span><br><span class="line">                countTmp[sArr[left] - <span class="string">&#x27;a&#x27;</span>]--;</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (checkTwoArr(countTmp, count)) &#123;</span><br><span class="line">                ans.add(left);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">checkTwoArr</span><span class="params">(<span class="type">int</span>[] a, <span class="type">int</span>[] b)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (a.length == b.length) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; a.length; i++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (a[i] != b[i]) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双指针</title>
      <link href="/ymhui.github.io/2024/09/11/%E5%8F%8C%E6%8C%87%E9%92%88/"/>
      <url>/ymhui.github.io/2024/09/11/%E5%8F%8C%E6%8C%87%E9%92%88/</url>
      
        <content type="html"><![CDATA[<h1>双指针</h1><p>双指针是一种常用且简洁的数据结构，通过维护两个指针/下标/列表，动态地存储原数据的关键信息，在需要时进行快速的调取；往往可以降低空间复杂度</p><h2 id="例题">例题</h2><h3 id="移动零">移动零</h3><h4 id="题目">题目</h4><p>给定一个数组 <code>nums</code>，编写一个函数将所有 <code>0</code> 移动到数组的末尾，同时保持非零元素的相对顺序。</p><p><strong>请注意</strong> ，必须在不复制数组的情况下原地对数组进行操作。</p><p><strong>示例 1:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: nums = [0,1,0,3,12]</span><br><span class="line">输出: [1,3,12,0,0]</span><br></pre></td></tr></table></figure><p><strong>示例 2:</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: nums = [0]</span><br><span class="line">输出: [0]</span><br></pre></td></tr></table></figure><p><strong>提示</strong>:</p><ul><li><code>1 &lt;= nums.length &lt;= 104</code></li><li><code>-231 &lt;= nums[i] &lt;= 231 - 1</code></li></ul><h4 id="分析">分析</h4><p>解法一：采用双指针。维护两个指针，其中左指针指向要处理的数，右指针指向已处理完成的数组尾部；一开始两个指针均在数组开始，左指针不断右移，<strong>每当遇到非零数，就交换左右指针的数值，同时右指针右移一位，遇到零则跳过。<strong>这样总可以保证右指针要么与左指针一同移动——<em>无事发生</em>，<strong>要么停在零上，即指向要放到尾部的元素</strong>，这样每次和左指针交换时总倾向于把0放到之后的位置，当左指针遍历到数组尾部，右指针一定已经将最后一个0换到尾部；而交换非零数的时候总是</strong>先访问者先交换</strong>，因此在<strong>前面的元素（非0）一定先被交换到前面，在后面的元素被交换后仍在之前就在他前面元素的后面</strong>，这样就保证了非0数相对顺序。</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">moveZeroes</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="comment">// 使用双指针</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">right</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>; left &lt; nums.length; left++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[left] != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// swap</span></span><br><span class="line">                <span class="type">int</span> <span class="variable">tmp</span> <span class="operator">=</span> nums[left];</span><br><span class="line">                nums[left] = nums[right];</span><br><span class="line">                nums[right] = tmp;</span><br><span class="line">                right++; <span class="comment">// 更新右指针</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解法二：本题双指针不是必须的，不用双指针也有很便捷的方法。注意到题目将要放到数组尾部的元素总是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span>（确定值），因此我们完全可以任意覆盖该元素，<strong>最后将空缺出来的位置填上0即可</strong>。换句话说，我们只需要维护一个指针 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 和一个计数器 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">cnt</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span></span>，<strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">cnt</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span></span>记录非零值的个数（应当填入的数组位置）</strong>；指针 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 一开始位于头部，之后不断右移，每次<strong>遇到零值就跳过，遇到非零值就将它放入计数器指示的位置</strong>。由于 <strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>n</mi><mi>t</mi><mo>&lt;</mo><mo>=</mo><mi>i</mi></mrow><annotation encoding="application/x-tex">cnt &lt;= i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6542em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>恒成立</strong>，因此在一个<strong>非零值被覆盖前（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">cnt</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span></span>占用）一定会先访问到（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mo>=</mo><mi>c</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">i &gt;= cnt</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6986em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span></span>）</strong>，除了被跳过的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span>，而0是一个确定值，只需要在所有非零值都处理完后将空缺位置填入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span>即可。按顺序遍历也保证了相对顺序。</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">moveZeroes</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">cnt</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            <span class="comment">// 非0元素前移，剩余位置全部填0</span></span><br><span class="line">            <span class="comment">// 因为填入的是固定值，所以直接覆盖也没有关系，而非零值的当前下标一定小于等于最终的下标，所以非零值一定不会被提前覆盖</span></span><br><span class="line">            <span class="keyword">if</span> (nums[i] != <span class="number">0</span>) &#123;</span><br><span class="line">                nums[cnt++] = nums[i]; <span class="comment">// cnt &lt;= i, 因此再覆盖一个元素前一定会先访问，这样非零元素总会被安排到正确的位置</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> cnt; i &lt; nums.length; i++) &#123;</span><br><span class="line">            nums[i] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="盛最多水的容器">盛最多水的容器</h3><h4 id="题目-2">题目</h4><p>给定一个长度为 <code>n</code> 的整数数组 <code>height</code> 。有 <code>n</code> 条垂线，第 <code>i</code> 条线的两个端点是 <code>(i, 0)</code> 和 <code>(i, height[i])</code> 。</p><p>找出其中的两条线，使得它们与 <code>x</code> 轴共同构成的容器可以容纳最多的水。</p><p>返回容器可以储存的最大水量。</p><p>**说明：**你不能倾斜容器。</p><p><strong>示例 1：</strong></p><p><img src="%E5%8F%8C%E6%8C%87%E9%92%88/question_11.jpg" alt="img"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：[1,8,6,2,5,4,8,3,7]</span><br><span class="line">输出：49 </span><br><span class="line">解释：图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：height = [1,1]</span><br><span class="line">输出：1</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>n == height.length</code></li><li><code>2 &lt;= n &lt;= 105</code></li><li><code>0 &lt;= height[i] &lt;= 104</code></li></ul><h4 id="分析-2">分析</h4><p>我们考虑夹逼法：从两端开始不断向中间收缩。不难发现<strong>收缩时容器的宽度一定会减小</strong>，所以只有容器的**有效高度（两边器壁高度的较小者）**变大，容器的容量才有可能变大，<strong>即让两侧高度中较小的一侧向内收缩</strong>，这样才有机会取得更大值，如果选择让较大者收缩，那之后的高度一定不会高于较小一侧的高度（目前的高度），再加上宽度一定减小，因此容量一定变小，这是不可行的。因此维护两个指针 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">left</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">right</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span></span></span></span> ，一开始 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">left</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span></span></span></span> 指向最左侧 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">right</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span></span></span></span> 指向最右侧，每次让高度较小一侧的指针向内收缩（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo>+</mo><mo>+</mo></mrow><annotation encoding="application/x-tex">left++</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">+</span></span></span></span> or <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>−</mo><mo>−</mo></mrow><annotation encoding="application/x-tex">right--</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">−</span></span></span></span>），尝试取到更大的容量。当两个指针相遇，即宽度减为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span>时，算法结束。</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">maxArea</span><span class="params">(<span class="type">int</span>[] height)</span> &#123;</span><br><span class="line">        <span class="comment">// 装水遵循木桶原则，即短板是上限</span></span><br><span class="line">        <span class="comment">// 利用左右两个指针从边界开始夹逼</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 每次总是固定较长的那个柱子，移动较短的那个柱子 -- 因为向中间收缩时，宽度一定降低，只有提高高度才有可能变大</span></span><br><span class="line"><span class="comment">           如果移动较长的柱子，那么后面的宽度一定不会大于较短的柱子，再加上宽度一定减小，所以容量一定变小</span></span><br><span class="line"><span class="comment">           所以只能移动较短的柱子，这样之后的高度仍是不确定的，可能超过当前容量（在宽度减小的情况下）</span></span><br><span class="line"><span class="comment">           当两根柱子相遇，结束算法 */</span></span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> <span class="variable">max</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">right</span> <span class="operator">=</span> height.length - <span class="number">1</span>, left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (right &gt; left) &#123;</span><br><span class="line">            max = Math.max(Math.min(height[right], height[left]) * (right - left), max);</span><br><span class="line">            <span class="keyword">if</span> (height[left] &gt; height[right]) &#123;</span><br><span class="line">                <span class="comment">// 左边高，移动右边</span></span><br><span class="line">                right--;;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="三数之和">三数之和</h3><h4 id="题目-3">题目</h4><p>给你一个整数数组 <code>nums</code> ，判断是否存在三元组 <code>[nums[i], nums[j], nums[k]]</code> 满足 <code>i != j</code>、<code>i != k</code> 且 <code>j != k</code> ，同时还满足 <code>nums[i] + nums[j] + nums[k] == 0</code> 。请你返回所有和为 <code>0</code> 且不重复的三元组。</p><p>**注意：**答案中不可以包含重复的三元组。</p><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [-1,0,1,2,-1,-4]</span><br><span class="line">输出：[[-1,-1,2],[-1,0,1]]</span><br><span class="line">解释：</span><br><span class="line">nums[0] + nums[1] + nums[2] = (-1) + 0 + 1 = 0 。</span><br><span class="line">nums[1] + nums[2] + nums[4] = 0 + 1 + (-1) = 0 。</span><br><span class="line">nums[0] + nums[3] + nums[4] = (-1) + 2 + (-1) = 0 。</span><br><span class="line">不同的三元组是 [-1,0,1] 和 [-1,-1,2] 。</span><br><span class="line">注意，输出的顺序和三元组的顺序并不重要。</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [0,1,1]</span><br><span class="line">输出：[]</span><br><span class="line">解释：唯一可能的三元组和不为 0 。</span><br></pre></td></tr></table></figure><p><strong>示例 3：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [0,0,0]</span><br><span class="line">输出：[[0,0,0]]</span><br><span class="line">解释：唯一可能的三元组和为 0 。</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>3 &lt;= nums.length &lt;= 3000</code></li><li><code>-105 &lt;= nums[i] &lt;= 105</code></li></ul><h4 id="分析-3">分析</h4><p>本题关键在于去重。首先对数组进行排序（此处采用升序），之所以先进行排序，是因为可以让重复的元素连在一起，方便之后跳过；同时也可以利用大小有序、分离正负的排序效果去除一些必然不成立的情况。</p><p>因为三元组的特殊性，我们<strong>首先固定一个元素</strong>来减少变动元素的数量，这样就只需要考虑两个元素的和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>u</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">sum</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span></span></span></span>，<strong>每次这两个元素都从固定元素之后的数组中取</strong>（从之前取一定会重复，因为前面的已经固定过了）；又由于数组已经有序即从小到大排列，所以<strong>当固定的数值为正数时，后面的数必然都是正数，三数之和不可能为0，此时算法结束</strong>。当固定的数非正时，我们只需要剩下两个数的和为固定数的相反数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">target</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span></span></span></span> 即可；因此<strong>维护两个指针 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">left</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">right</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span></span></span></span>，<strong>一开始<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">left</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span></span></span></span> 指向固定数左侧数组的最左侧，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">right</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span></span></span></span>指向最右侧，不难发现当</strong>收缩 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">left</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span></span></span></span>时 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>u</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">sum</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span></span></span></span>会增大，反之收缩收缩 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">right</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span></span></span></span>时 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>u</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">sum</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span></span></span></span>会减小</strong>，利用这个特点，只要<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>&gt;</mo><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">right &gt; left</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span></span></span></span> （指针不相遇，这一趟固定扫描尚未结束） 我们就重复做</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mo>=</mo><mo>=</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">sum == target</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">==</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span></span></span></span> 获取一个三元组，<strong>同时左边界和右边界收缩直到跳过所有和刚才指向的元素相同的元素</strong>，注意保持<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>&gt;</mo><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">right &gt; left</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mo>&gt;</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">sum &gt; target</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span></span></span></span> 收缩右边界 即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>−</mo><mo>−</mo></mrow><annotation encoding="application/x-tex">right--</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">−</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mo>&lt;</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">sum &lt; target</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span></span></span></span> 收缩左边界 即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo>+</mo><mo>+</mo></mrow><annotation encoding="application/x-tex">left++</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">+</span></span></span></span></li></ul><p>每次循环最后要更新 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>u</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">sum</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span></span></span></span> 的值</p><p>当固定一个数之后的所有三元组获取结束后，要跳过之后所有和这个数相同的元素来防止重复。</p><p>总结：</p><ul><li>当固定的数值为正数时，<strong>算法结束</strong></li><li>固定完一个数，<strong>跳过之后所有相同的数</strong>之后再固定下一个数</li><li>获取完三元组后<strong>收缩左右边界跳过和刚才获取的三元组中相同的数</strong></li><li>固定一个数时的循环保证 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>&gt;</mo><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">right &gt; left</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span></span></span></span></li></ul><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; <span class="title function_">threeSum</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length &lt; <span class="number">3</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        &#125;</span><br><span class="line">        Arrays.sort(nums);</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; ans = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length - <span class="number">2</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>; <span class="comment">// 因为数组已经有序，所以若当前数字为正，后面不可能抵消出负数，不可能再有成立的情况; 算法结束</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (i &gt; <span class="number">0</span> &amp;&amp; nums[i] == nums[i - <span class="number">1</span>]) &#123; </span><br><span class="line">                <span class="comment">// 且若前面有一个和自己一样的，自己会重复，跳过</span></span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 先固定一个</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">target</span> <span class="operator">=</span> -nums[i];</span><br><span class="line">            <span class="comment">// 启动双指针，相向而行</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> i + <span class="number">1</span>, right = nums.length - <span class="number">1</span>;</span><br><span class="line">            <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> nums[left] + nums[right];</span><br><span class="line">            <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">                <span class="comment">// 由于数组有序，所以right--和会变小，left++和会变大</span></span><br><span class="line">                <span class="keyword">if</span> (sum &gt; target) &#123;</span><br><span class="line">                    <span class="comment">// sum需减小</span></span><br><span class="line">                    <span class="comment">// 要跳过所有一样的, 降低复杂度</span></span><br><span class="line">                    <span class="type">int</span> <span class="variable">old</span> <span class="operator">=</span> nums[right];</span><br><span class="line">                    <span class="keyword">while</span> (nums[right] == old &amp;&amp; left &lt; right) &#123;</span><br><span class="line">                        right--;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (sum &lt; target) &#123;</span><br><span class="line">                    <span class="type">int</span> <span class="variable">old</span> <span class="operator">=</span> nums[left];</span><br><span class="line">                    <span class="keyword">while</span> (nums[left] == old &amp;&amp; left &lt; right) &#123;</span><br><span class="line">                        left++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    ArrayList&lt;Integer&gt; group = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                    group.add(nums[i]);</span><br><span class="line">                    group.add(nums[left]);</span><br><span class="line">                    group.add(nums[right]);</span><br><span class="line">                    <span class="comment">// 找到了一个可能的三元组，只要不重复</span></span><br><span class="line">                    ans.add(group);</span><br><span class="line">                    <span class="comment">// 跳过之后/前所有还会重复的</span></span><br><span class="line">                    <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[left] == nums[left + <span class="number">1</span>]) &#123;</span><br><span class="line">                        left++;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[right] == nums[right - <span class="number">1</span>]) &#123;</span><br><span class="line">                        right--;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (left &lt; right) &#123;</span><br><span class="line">                        left++;</span><br><span class="line">                        right--;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                sum = nums[left] + nums[right];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="接雨水">接雨水</h3><p><em>注：本题主体应当属于动态规划，双指针只是用来优化空间复杂度，不必须</em></p><h4 id="题目-4">题目</h4><p>给定 <code>n</code> 个非负整数表示每个宽度为 <code>1</code> 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。</p><p><strong>示例 1：</strong></p><p><img src="%E5%8F%8C%E6%8C%87%E9%92%88/rainwatertrap.png" alt="img"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：height = [0,1,0,2,1,0,1,3,2,1,2,1]</span><br><span class="line">输出：6</span><br><span class="line">解释：上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 </span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：height = [4,2,0,3,2,5]</span><br><span class="line">输出：9</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li><code>n == height.length</code></li><li><code>1 &lt;= n &lt;= 2 * 104</code></li><li><code>0 &lt;= height[i] &lt;= 105</code></li></ul><h4 id="分析-4">分析</h4><p>我们将这个问题分解，整体存雨水的量等于所有坐标处存的雨水量之和，而<strong>每个坐标处的雨水量</strong>该怎么求呢？观察发现每个坐标处的宽度都是<code>1</code>，因此<strong>坐标处的雨水量就是该坐标处雨水能达到的高度</strong>。结合木桶效应，一个位置的水位能达到的最高处等于它两侧器壁中的较小者再减去此坐标处柱子的高度（柱子排水），如果小于0，就是0；因为此处柱子是连续的，所以一个位置<strong>左侧器壁终结于左边最长的一根柱子，右侧器壁终结于右侧最高的柱子。</strong></p><p><strong>由此该问题退化为：找每个位置的左侧最高柱子和右侧最高柱子，取小的最高水位，再减去自己的柱子长度的实际水位，如果小于0就没有水，所有位置加起来即最终结果。</strong></p><p>求每个位置的左/右侧最高柱子可用<strong>迭代法</strong>：每个位置的左/右侧最高柱子就两种情况</p><ul><li><strong>左/右边的那根柱子</strong></li><li><strong>左/右边那根柱子的左/右侧最高柱子</strong></li></ul><p>取两者中较大者即可</p><p>代码如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">trap</span><span class="params">(<span class="type">int</span>[] height)</span> &#123;</span><br><span class="line">        <span class="comment">// 先找到两侧最高柱子</span></span><br><span class="line">        <span class="comment">/* 用迭代法计算每个位置左/右侧柱子的最大值</span></span><br><span class="line"><span class="comment">              以左侧柱子为例，位置i的左侧柱子最大值为i - 1处柱子最大值和i - 1处高度中的较大者</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">ans</span> <span class="operator">=</span> <span class="number">0</span>; <span class="comment">// 最终用于求和</span></span><br><span class="line">        <span class="comment">// 迭代法求两侧最高柱子</span></span><br><span class="line">        <span class="type">int</span>[] left = <span class="keyword">new</span> <span class="title class_">int</span>[height.length], right = <span class="keyword">new</span> <span class="title class_">int</span>[height.length];</span><br><span class="line">        left[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt; height.length; i++) &#123;</span><br><span class="line">            left[i] = Math.max(height[i - <span class="number">1</span>], left[i - <span class="number">1</span>]); <span class="comment">// 左边或左边的左侧的最高柱子</span></span><br><span class="line">        &#125;</span><br><span class="line">        right[height.length - <span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> height.length - <span class="number">2</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            right[i] = Math.max(height[i + <span class="number">1</span>], right[i + <span class="number">1</span>]); <span class="comment">// 右边或右边的右侧最高柱子</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 计算每个位置的水量</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; height.length; i++) &#123;</span><br><span class="line">            <span class="comment">// each = max(0, min(left, right) - self_h);</span></span><br><span class="line">            ans += Math.max(<span class="number">0</span>, Math.min(left[i], right[i]) - height[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>优化：双指针</em></p><p>TBD</p><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-分类算法</title>
      <link href="/ymhui.github.io/2024/09/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
      <url>/ymhui.github.io/2024/09/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1>机器学习之分类算法</h1><h2 id="写在前面">写在前面</h2><p>请先阅读<strong>机器学习之特征工程</strong></p><h3 id="sklearn转换器和估计器">sklearn转换器和估计器</h3><p><em><strong>转换器(transformer): 特征工程的父类</strong></em></p><ol><li>实例化（实例化的是一个转换器类(Transformer)）</li><li>调用 <em>fit_transform()</em> 方法（对于文档建立分类词频矩阵，不能同时调用）</li></ol><blockquote><p>标准化：(x - mean) / std</p><blockquote><p>fit_transform()<br><strong>fit()</strong> 计算 每一行列的平均值、标准差<br><strong>transform()</strong> (x - mean) / std 进行最终的转换</p></blockquote></blockquote><hr><p><em><strong>估计器(estimator): sklearn机器算法的实现</strong></em></p><ol><li><p>实例化一个estimator</p></li><li><p>调用fit()</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimator.fit(x_train, y_train)  <span class="comment"># 计算 生成模型</span></span><br><span class="line"><span class="comment"># 调用完毕，模型生成</span></span><br></pre></td></tr></table></figure></li><li><p>模型评估：</p><p>i. 直接比对真实值和测试值</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line">y_test == y_predict  <span class="comment"># 生成一系列布尔值，Ture多则准确</span></span><br></pre></td></tr></table></figure><p>ii. 计算准确率</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accuracy = estimator.score(x_test, y_test)</span><br></pre></td></tr></table></figure></li></ol><h2 id="介绍">介绍</h2><p>分类算法用于将数据样本分配到不同的类别</p><h2 id="算法">算法</h2><h3 id="朴素贝叶斯">朴素贝叶斯</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">朴素贝叶斯算法：</span><br><span class="line">    朴素：假设特征与特征之间相互独立</span><br><span class="line">    应用场景：文本分类，单词作为特征</span><br><span class="line">    优点：分类效率稳定；对缺失数据不敏感，算法简单（贝叶斯公式），常用于文本分类；准确度高速度快</span><br><span class="line">    缺点：由于有“朴素”的假定，如果特征属性有关联时效果不好</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">贝叶斯公式: P(C | W) = P(W | C) * P(C) / P(W)</span><br></pre></td></tr></table></figure><h4 id="demo">demo</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> SelfTools <span class="keyword">import</span> Tool</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">MultinomialNB(alpha=1.0)</span></span><br><span class="line"><span class="string">朴素贝叶斯分类</span></span><br><span class="line"><span class="string">alpha: Laplace平滑系数</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">news_demo</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    朴素贝叶斯算法进行新闻分类</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    news = fetch_20newsgroups(subset=<span class="string">&quot;all&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=<span class="number">0.2</span>, random_state=<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征工程：文本特征值抽取</span></span><br><span class="line">    transfer = TfidfVectorizer()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 朴素贝叶斯预估器流程</span></span><br><span class="line">    estimator = MultinomialNB()</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型评估</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y_predict:\n&quot;</span>, y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y_predict == y_test:\n&quot;</span>, y_predict == y_test)</span><br><span class="line">    Tool.separate()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;score = &quot;</span>, estimator.score(x_test, y_test))</span><br><span class="line">    Tool.separate()</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    news_demo()</span><br></pre></td></tr></table></figure><h3 id="a-name-knn-knn-a"><a name="knn">knn</a></h3><p>k-临近算法（k-Nearest Neighbour）：<strong>近朱者赤，近墨者黑</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">如果一个样本在特征空间中的 k 个最相似（即特征空间中最邻近）的样本中的大多数都属于某个类别，则该样本也属于这个类别</span><br><span class="line"></span><br><span class="line">=&gt;第 1 近，第 2 近 ... 第 k 近</span><br></pre></td></tr></table></figure><p>缺点：k 取得太小，容易受到异常点影响；k 取得太大，容易受到样本不均衡影响; 懒惰算法，内存开销大</p><p><em>注意：knn算法计算距离时需要对数据进行标准化处理，防止大数据的支配效果</em></p><p>适合小数据场景</p><h4 id="demo-2">demo</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> SelfTools <span class="keyword">import</span> Tool</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">KNeighbourClassifier(n_neighbors=5, algorithm=&#x27;auto&#x27;)</span></span><br><span class="line"><span class="string">    n_neighbors: int, 可选（默认5），k_neighbors查询时默认使用的邻居数</span></span><br><span class="line"><span class="string">    algorithm: 选择算法，不必关注，使用&#x27;auto&#x27;自动选择最优即可</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 鸢尾花种类预测</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">iris_classify</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    knn算法对鸢尾花分类</span></span><br><span class="line"><span class="string">    数据集分为测试集和训练集</span></span><br><span class="line"><span class="string">    :return: class(Iris-Setosa, Iris-Versicolour, Iris-Virginica)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    iris = load_iris()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据集划分 训练集 + 测试集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class="number">6</span>)  <span class="comment"># 返回值固定形式</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征工程：数据标准化</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)  <span class="comment"># 对训练集标准化</span></span><br><span class="line">    x_test = transfer.transform(x_test)</span><br><span class="line">    <span class="comment"># 测试集要用和训练集一样的处理方式，即均值与方差均与训练集一致，因为transfer在fit_transform()时计算过了均值方差，所以test直接使用transform()</span></span><br><span class="line">    <span class="comment"># 如果test仍使用fit_transform()那相当于在用测试集的均值和方差</span></span><br><span class="line">    <span class="comment"># 原因是transfer是一个固定的实例化对象，它在第一次调用fit_transform方法时均值和方差的属性就被修改完毕(被修改为训练集的均值和方差) 测试集沿用</span></span><br><span class="line">    <span class="comment"># 再次执行又会被修改，测试集不希望发生修改</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># knn预估器流程</span></span><br><span class="line">    estimator = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    Tool.separate()</span><br><span class="line">    <span class="comment"># 模型评估</span></span><br><span class="line">    <span class="comment"># 法一：直接比对真实值和测试值</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y_predict:\n&quot;</span>, y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y_predict == y_test: \n&quot;</span>, y_test == y_predict)</span><br><span class="line">    Tool.separate()</span><br><span class="line">    <span class="comment"># 法二：计算准确率</span></span><br><span class="line">    score = estimator.score(x_test, y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;score = &quot;</span>, score)</span><br><span class="line">    Tool.separate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    iris_classify()</span><br></pre></td></tr></table></figure><h3 id="决策树">决策树</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">决策树</span><br><span class="line">信息：用来消除随机不定性的东西</span><br><span class="line">信息熵：衡量信息的依据</span><br><span class="line">    bit</span><br><span class="line">    信息熵 H(X) = - sum(P(xi) * log (b) P(xi)), i: [1, n]</span><br><span class="line">决策树的划分依据之一：信息增益</span><br><span class="line">    特征A对训练数据集D的信息增益 g(D, A) 定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵 H(D|A)之差</span><br><span class="line">    g(D, A) = H(D) - H(D|A)</span><br></pre></td></tr></table></figure><p>决策树适用于大数据场景，是非懒惰算法，内存开销和效率优于<a href="#knn">knn</a>，但准确率不如<a href="#knn">knn</a></p><h4 id="demo-3">demo</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> SelfTools <span class="keyword">import</span> Tool</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">DecisionTreeClassifier(criterion=&#x27;gini&#x27;, max_depth=None, random_rate=None)</span></span><br><span class="line"><span class="string">    决策树分类器</span></span><br><span class="line"><span class="string">    criterion: 默认&#x27;gini&#x27;系数，也可以选择信息增益的熵&#x27;entropy&#x27;</span></span><br><span class="line"><span class="string">    max_depth: 树的深度大小</span></span><br><span class="line"><span class="string">    random_rate: 随机数种子</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">决策树可视化</span></span><br><span class="line"><span class="string">export_graphviz(estimator, out_file=&#x27;tree.dot&#x27;, feature_names=[&quot;.&quot;])</span></span><br><span class="line"><span class="string">    导出DOT格式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decision_tree</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    决策树对鸢尾花分类</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取数据集</span></span><br><span class="line">    iris = load_iris()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (决策树不需要就算距离，因此不需要标准化)</span></span><br><span class="line">    transfer = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 决策树预估器进行分类</span></span><br><span class="line">    estimator = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>)</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型评估</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y_predict:\n&quot;</span>, y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y_predict == y_test:\n&quot;</span>, y_predict == y_test)</span><br><span class="line">    Tool.separate()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;score = &quot;</span>, estimator.score(x_test, y_test))</span><br><span class="line">    Tool.separate()</span><br><span class="line"></span><br><span class="line">    export_graphviz(estimator, out_file=<span class="string">&#x27;decision_tree.dot&#x27;</span>, feature_names=iris.feature_names)</span><br><span class="line">    <span class="comment"># 可视化决策树http://www.webgraphviz.com/，将dot文件传入</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    decision_tree()</span><br></pre></td></tr></table></figure><h3 id="随机森林">随机森林</h3><p>包含多个决策树的分类器</p><p>原理：一棵决策树可能有误差；建立一堆决策树，独立预测，结果取众数，少数服从多数</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">例如：训练了5个树，4个树True，1个树False，结果就是True</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">随机森林原理过程：</span><br><span class="line">    训练集：特征值、目标值</span><br><span class="line">    随机：训练集随机，特征随机（如果每次都用一样的数据，训练结果岂不是一样？）</span><br><span class="line">    假设训练集有 N 个样本，M 个特征</span><br><span class="line">    训练集随机：bootstrap 随机有放回抽样，N 个样本的训练集随机有放回抽样 N 次（一次一个），得到一个新的随机训练集</span><br><span class="line">    如： [1, 2, 3, 4, 5] -&gt; [2, 2, 3, 1, 5]</span><br><span class="line">    特征随机：M 个特征中抽取 m 个特征，要求: M &gt;&gt; m</span><br></pre></td></tr></table></figure><h4 id="demo-4">demo</h4><p>TBD~</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">RandomForestClassifier(n_estimators=10, criterion=&#x27;gini&#x27;, max_depth=None, max_features=&#x27;auto&#x27;,bootstrap=True, </span></span><br><span class="line"><span class="string">                       random_state=None, min_samples_split=2, min_samples_leaf)</span></span><br><span class="line"><span class="string">    随机森林分类器</span></span><br><span class="line"><span class="string">    n_estimators: optional, number of trees in forests, integer</span></span><br><span class="line"><span class="string">    criterion: optional, measures of splitting features, string</span></span><br><span class="line"><span class="string">    max_depth: optional, max depth of one tree, integer or None</span></span><br><span class="line"><span class="string">    max_features: max features of one tree</span></span><br><span class="line"><span class="string">        if &#x27;auto&#x27;, max_features = sqrt(n_features)</span></span><br><span class="line"><span class="string">        if &#x27;sqrt&#x27;, same as &#x27;auto&#x27;</span></span><br><span class="line"><span class="string">        if &#x27;log2&#x27;, max_features = log2 (n_features)</span></span><br><span class="line"><span class="string">        if None, max_features = n_features (Not recommended)</span></span><br><span class="line"><span class="string">        string</span></span><br><span class="line"><span class="string">    bootstrap: optional, whether using bootstrap, boolean</span></span><br><span class="line"><span class="string">    min_samples_split: min number of samples of samples&#x27; split</span></span><br><span class="line"><span class="string">    min_samples_leaf: min number of samples of leaf node</span></span><br><span class="line"><span class="string">    超参数：n_estimators, max_depth, min_samples_split, min_samples_leaf</span></span><br><span class="line"><span class="string">    -&gt; 网格搜索用</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rf_demo</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    随机森林</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    rf_demo()</span><br></pre></td></tr></table></figure><h2 id="实例">实例</h2><p>TBD~</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">titanic_predict</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Titanic号乘客生存预测</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    path = <span class="string">&quot;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt&quot;</span></span><br><span class="line">    titanic = pd.read_csv(path)</span><br><span class="line">    <span class="comment"># 数据处理：titanic.txt缺失age属性：缺失值处理；特征值-&gt;字典类型</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 准备好特征值、目标值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征工程：字典特征抽取</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 决策树预估器流程</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型评估</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    titanic_predict()</span><br></pre></td></tr></table></figure><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 分类算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-回归与聚类算法</title>
      <link href="/ymhui.github.io/2024/09/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%9B%9E%E5%BD%92%E4%B8%8E%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
      <url>/ymhui.github.io/2024/09/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%9B%9E%E5%BD%92%E4%B8%8E%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-特征工程</title>
      <link href="/ymhui.github.io/2024/09/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
      <url>/ymhui.github.io/2024/09/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1>特征工程</h1><h2 id="写在前面">写在前面</h2><p>以下算法实例均取自sklearn库，使用python编写</p><p>可使用以下指令快速安装sklearn</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pip install sklearn</span><br></pre></td></tr></table></figure><p>下文有时会使用<code>./SelfTools/Tool.py</code>中的内容</p><h3 id="Tool-py"><a href="http://Tool.py">Tool.py</a></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">separate</span>(<span class="params">self=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;---------------------------------------------------------------------------------------------------&quot;</span> * <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><h2 id="常用算法">常用算法</h2><h3 id="字典特征值抽取">字典特征值抽取</h3><p>从原始数据中提取有意义的特征，然后创建一个字典来存储提取的特征，再将数据转换为字典形式，以便可以方便地进行分析或输入到机器学习模型中（向量化？），最后从提取的特征中选择最有用的特征，以提高模型的性能。有时还需要对特征进行标准化处理，以确保它们在相同的尺度上，避免大数”吞噬“小数。</p><h4 id="demo">demo</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> SelfTools <span class="keyword">import</span> Tool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dict_demo</span>(<span class="params">flag=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    字典特征值抽取，抽取为one-hot编码</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data = [&#123;<span class="string">&#x27;city&#x27;</span>: <span class="string">&#x27;Beijing&#x27;</span>, <span class="string">&#x27;temperature&#x27;</span>: <span class="number">100</span>&#125;, &#123;<span class="string">&#x27;city&#x27;</span>: <span class="string">&#x27;Shanghai&#x27;</span>, <span class="string">&#x27;temperature&#x27;</span>: <span class="number">60</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;city&#x27;</span>: <span class="string">&#x27;Shenzhen&#x27;</span>, <span class="string">&#x27;temperature&#x27;</span>: <span class="number">30</span>&#125;]</span><br><span class="line">    <span class="comment"># 实例化一个转换器类</span></span><br><span class="line">    transfer = DictVectorizer(sparse=flag)</span><br><span class="line">    <span class="comment"># sparse默认为True，返回sparse矩阵的形式，想看到原来二维数组的形式需做还原: sparse=False</span></span><br><span class="line">    <span class="comment"># sparse: 稀疏矩阵，第一列的元组给出了所有非零值在二维数组中的位置，其后为对应的非零值</span></span><br><span class="line">    <span class="comment"># 使用稀疏矩阵的原因：数据规模足够大后，0很多，稀疏矩阵可以显著提高运行效率且节省内存</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用转换器的fit_transform方法</span></span><br><span class="line">    data_new = transfer.fit_transform(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_new: \n&quot;</span>, data_new)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;feature_names: \n&quot;</span>, transfer.get_feature_names())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    dict_demo()</span><br><span class="line">    Tool.separate()</span><br><span class="line">    dict_demo(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="特征降维">特征降维</h3><p>目的是去除冗余数据，这里的维数应当指的是特征的种类，有时我们并不需要一个数据的全部维度，比如西瓜瓜皮颜色，对于大部分数据都具有相同的值，属于冗余特征。</p><p>一般采用<strong>低方差特征过滤法</strong>，其原理为：某个特征的方差小，意味着大部分样本该特征的值比较相近，为冗余数据，无需考虑。</p><h4 id="demo-2">demo</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> SelfTools <span class="keyword">import</span> Tool</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">VarianceThreshold(threshold=0.0)</span></span><br><span class="line"><span class="string">    删除所有低方差特征</span></span><br><span class="line"><span class="string">    Variance.fit_transform(X)</span></span><br><span class="line"><span class="string">        X: numpy array格式的数据 [n_samples, n_features]</span></span><br><span class="line"><span class="string">        return: 训练集差异小于等于threshold的特征被删除。默认值是保存所有非零方差特征(即threshold = 0.0), 即删除所有具有相同值的特征</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">卡方检验(Pearsonr检验)：高中知识，反应相关性 r, r -&gt; [-1, 1]; |r|越接近 1，相关性越强，大于0正相关</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">从scipy中引进pearsonr卡方检验</span></span><br><span class="line"><span class="string">pearsonr(x, y)</span></span><br><span class="line"><span class="string">    x: (N,) array_like</span></span><br><span class="line"><span class="string">    y: (N,) array_like</span></span><br><span class="line"><span class="string">    return: Pearson&#x27;s correlation coefficient(statistic), p-value(pvalue)</span></span><br><span class="line"><span class="string">p-value衡量 r 的有效性，越小越可靠</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">variance_demo</span>(<span class="params">std=<span class="number">0.0</span></span>):</span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    data = pd.read_csv(<span class="string">&quot;factor_returns.csv&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data: \n&quot;</span>, data)</span><br><span class="line">    Tool.separate()</span><br><span class="line">    data = data.iloc[:, <span class="number">2</span>:]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data: \n&quot;</span>, data)</span><br><span class="line">    Tool.separate()</span><br><span class="line">    <span class="comment"># 实例化转换器类</span></span><br><span class="line">    transfer = VarianceThreshold(std)</span><br><span class="line">    <span class="comment"># 调用fit_transform()方法</span></span><br><span class="line">    data_new = transfer.fit_transform(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_new: \n&quot;</span>, data_new)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;shape of data_new: \n&quot;</span>, data_new.shape)</span><br><span class="line">    Tool.separate()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算某两个变量之间的相关系数</span></span><br><span class="line">    r = pearsonr(data[<span class="string">&quot;Open&quot;</span>], data[<span class="string">&quot;Close&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;相关系数: \n&quot;</span>, r)</span><br><span class="line">    Tool.separate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    variance_demo()</span><br><span class="line">    variance_demo(<span class="number">13</span>)</span><br><span class="line">    variance_demo(<span class="number">14</span>)</span><br></pre></td></tr></table></figure><h3 id="特征值归一化">特征值归一化</h3><p>为防止一些很大的特征值在进行机器学习时对整个结果起支配作用，从而使计算机无法学习到其他特征值较小的特征。</p><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">对特征值 x 作归一化:</span><br><span class="line">    step1: X&#x27; = (x - min) / (max - min)</span><br><span class="line">    step2: X&#x27;&#x27; = x&#x27; * (mx - mi) + mi</span><br><span class="line">    注：mx, mi分别为指定区间值（标准化后值的范围，例如归一化一般采用(0, 1)），默认mx = 1, mi = 0 feature<span class="built_in">_</span>range = (mi, mx)</span><br><span class="line">X&#x27;&#x27;为归一化后的 x</span><br></pre></td></tr></table></figure><p>*缺点：*依赖于最大值与最小值，一旦最大值或最小值中出现异常值，归一化就会失效，只适合传统精确小数据场景</p><h4 id="demo-3">demo</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> SelfTools <span class="keyword">import</span> Tool</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">MinMaxScaler(feature_range=(0,1)...)</span></span><br><span class="line"><span class="string">    MinMaxScaler.fit_transform(X)</span></span><br><span class="line"><span class="string">        X: numpy array格式的数据[n_samples, n_features] (二维数组，行数为样本数，列数为特征的个数)</span></span><br><span class="line"><span class="string">        return: 转换后的形状相同的array</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">minmax_scaler</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    归一化</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    data = pd.read_csv(<span class="string">&quot;dating.txt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data: \n&quot;</span>, data)</span><br><span class="line">    Tool.separate()</span><br><span class="line">    <span class="comment"># 只处理数据前三列</span></span><br><span class="line">    data = data.iloc[:, <span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data: \n&quot;</span>, data)</span><br><span class="line">    Tool.separate()</span><br><span class="line">    <span class="comment"># 实例化转换器类</span></span><br><span class="line">    transfer = MinMaxScaler()</span><br><span class="line">    <span class="comment"># 调用fit_transform()方法</span></span><br><span class="line">    data_new = transfer.fit_transform(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_new: \n&quot;</span>, data_new)</span><br><span class="line">    Tool.separate()</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    minmax_scaler()</span><br></pre></td></tr></table></figure><h3 id="特征值标准化">特征值标准化</h3><p><strong>解决归一化的不足</strong>，通过对原始数据进行变换，把数据变换到均值为 0，标准差为 1 的范围内</p><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">对 x 标准化: X&#x27; = (x - mean) / std</span><br><span class="line">    mean: 均值</span><br><span class="line">    std: 标准差</span><br></pre></td></tr></table></figure><p>关于异常值，标准化对于异常值的敏感度低于归一化；对于大量的大数据而言，少量异常点对于均值和标准差的影响都不会很大。</p><h4 id="demo-4">demo</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> SelfTools <span class="keyword">import</span> Tool</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">StandardScaler()</span></span><br><span class="line"><span class="string">    处理之后，对每列来说，所有数据都聚集在均值 0 附近，标准差为 1</span></span><br><span class="line"><span class="string">    StandardScaler.fit_transform(X)</span></span><br><span class="line"><span class="string">        X: numpy array格式的数据 [n_samples, n_features] (同样为二维数组，行数为样本数，列数为特征的个数)</span></span><br><span class="line"><span class="string">        return: 转换后形状相同的array</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">std_scaler</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    标准化</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data = pd.read_csv(<span class="string">&quot;dating.txt&quot;</span>)</span><br><span class="line">    data = data.iloc[:, :<span class="number">3</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;dara: \n&quot;</span>, data)</span><br><span class="line">    Tool.separate()</span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    data_new = transfer.fit_transform(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_new: \n&quot;</span>, data_new)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    std_scaler()</span><br></pre></td></tr></table></figure><h3 id="主成分分析-PCA">主成分分析 PCA</h3><p>又称<a href="https://cn.bing.com/search?q=PCA%E9%99%8D%E7%BB%B4&amp;cvid=a05cda1e7c3d44b9a4c22f16ad1e3a45&amp;gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQABhAMgYIAhAAGEAyBggDEAAYQDIGCAQQABhAMgYIBRAAGEAyBggGEEUYPDIGCAcQRRg8MgYICBBFGDzSAQgzOTM4ajBqMagCCLACAQ&amp;FORM=ANAB01&amp;ucpdpc=UCPD&amp;adppc=EdgeStart&amp;PC=LCTS">PCA降维</a>，是一种常用的数据降维技术，它通过将原始数据集中的特征线性组合，提取一组新的特征（称为主成分），这些新特征具有最大的方差，并且彼此正交。这样，我们就可以用尽可能少的主成分来保留原始数据集的大部分信息。</p><p>往往需要数据标准化，计算协方差矩阵，计算特征值和特征向量，选择主成分，降维…</p><p><strong><s>万幸，python什么都有</s></strong></p><h4 id="demo-5">demo</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> SelfTools <span class="keyword">import</span> Tool</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">PCA(n_components=None)</span></span><br><span class="line"><span class="string">    将数据分解为较低维数空间</span></span><br><span class="line"><span class="string">    n_components:</span></span><br><span class="line"><span class="string">        小数：表示保留百分之多少的信息</span></span><br><span class="line"><span class="string">        整数：减少到多少特征</span></span><br><span class="line"><span class="string">    PCA.fit_transform(X) </span></span><br><span class="line"><span class="string">    X: numpy array格式的数据 [n_samples, n_features]</span></span><br><span class="line"><span class="string">    return: 转换后指定维度的array</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pca_demo</span>():</span><br><span class="line">    data = [[<span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">8</span>], [<span class="number">5</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">1</span>]]</span><br><span class="line">    transfer = PCA(n_components=<span class="number">0.95</span>)</span><br><span class="line">    data_new = transfer.fit_transform(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_new:\n&quot;</span>, data_new)</span><br><span class="line">    Tool.separate()</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    pca_demo()</span><br></pre></td></tr></table></figure><p>下面介绍一些和文本内容相关的数据分析算法</p><h3 id="文本特征值提取">文本特征值提取</h3><p>以<code>单词</code>作为特征进行提取，sklearn提供了一个**<code>CountVectorizer</code>**统计特征值出现的个数</p><h4 id="demo-6">demo</h4><p>可能需要的指令，（如果<code>import jieba</code>报错</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pip install jieba</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="comment"># jieba用于分词</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">from</span> SelfTools <span class="keyword">import</span> Tool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_demo</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    文本特征抽取：CountVectorizer</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    text = [<span class="string">&quot;life is short, i like like python&quot;</span>,</span><br><span class="line">            <span class="string">&quot;life is too long, i dislike python&quot;</span>]</span><br><span class="line">    <span class="comment"># 实例化转换器类</span></span><br><span class="line">    transfer = CountVectorizer()</span><br><span class="line">    <span class="comment"># 调用fit_transform方法</span></span><br><span class="line">    txt_new = transfer.fit_transform(text)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;features_names: \n&quot;</span>, transfer.get_feature_names_out())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;txt_new: \n&quot;</span>, txt_new)</span><br><span class="line">    Tool.separate()</span><br><span class="line">    <span class="comment"># 输出·txt_new的类型</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;type of txt_new: \n&quot;</span>, <span class="built_in">type</span>(txt_new))</span><br><span class="line">    <span class="comment"># 发现fit_transform方法默认返回仍是sparse矩阵（对象）</span></span><br><span class="line">    <span class="comment"># 该对象自带一个toarray()方法，可返回原始二维数组(类似于之前的将sparse设为False，但是文本特征值抽取没有sparse参数)</span></span><br><span class="line">    Tool.separate()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;array of txt_new: \n&quot;</span>, txt_new.toarray())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chn_txt_demo</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    中文文本特征值抽取</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    text = [<span class="string">&quot;这些新特征具有最大的方差，并且彼此正交。这样，我们就可以用尽可能少的主成分来保留原始数据集的大部分信息。&quot;</span>, <span class="string">&quot;牛魔大酬宾&quot;</span>]</span><br><span class="line">    <span class="comment"># 中文不似英文，天生不带空格，所以抽取时会出现短语（按标点划分）</span></span><br><span class="line">    transfer = CountVectorizer()</span><br><span class="line">    txt_new = transfer.fit_transform(text)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;features_names: \n&quot;</span>, transfer.get_feature_names_out())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;array of txt_new: \n&quot;</span>, txt_new.toarray())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cut_words</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对字符串进行中文分词</span></span><br><span class="line"><span class="string">    :param: text</span></span><br><span class="line"><span class="string">    :return: &quot; &quot;.join(list(jieba.cut(text)))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tokenizer = jieba.cut(text)</span><br><span class="line">    <span class="comment"># print(tokenizer)</span></span><br><span class="line">    <span class="comment"># tokenizer: &lt;generator object Tokenizer.cut</span></span><br><span class="line">    l_token = <span class="built_in">list</span>(tokenizer)</span><br><span class="line">    <span class="comment"># print(l_token)</span></span><br><span class="line">    text = <span class="string">&quot; &quot;</span>.join(l_token)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chn_txt_sep</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    中文特征值抽取，自动分词</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    text = [<span class="string">&quot;我会在本学期认真自学课程内容，并按时上交课程作业&quot;</span>,</span><br><span class="line">            <span class="string">&quot;参加每一次课程考试。&quot;</span>,</span><br><span class="line">            <span class="string">&quot;如因未随堂上课迟交或错过提交作业、参加测试等影响本课程成绩，本人自行承担一切后果。&quot;</span>]</span><br><span class="line">    txt_new = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(text)):</span><br><span class="line">        txt_new.append(cut_words(text[i]))</span><br><span class="line">    <span class="comment"># print(txt_new)</span></span><br><span class="line">    transfer = CountVectorizer(stop_words=[<span class="string">&quot;本人&quot;</span>, <span class="string">&quot;我会&quot;</span>])</span><br><span class="line">    <span class="comment"># stop_words为忽略的单词（往往没有实质作用）</span></span><br><span class="line">    txt_new = transfer.fit_transform(txt_new)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;features_names: \n&quot;</span>, transfer.get_feature_names_out())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;array of txt_new: \n&quot;</span>, txt_new.toarray())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># text_demo()</span></span><br><span class="line">    <span class="comment"># chn_txt_demo()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># cut_words(&quot;我爱北京天安门&quot;) # [&#x27;我&#x27;, &#x27;爱&#x27;, &#x27;北京&#x27;, &#x27;天安门&#x27;]</span></span><br><span class="line">    <span class="comment"># print(cut_words(&quot;我爱北京天安门&quot;)) # 我 爱 北京 天安门</span></span><br><span class="line"></span><br><span class="line">    chn_txt_sep()</span><br></pre></td></tr></table></figure><h3 id="TF-IDF">TF-IDF</h3><figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">tf: text frequency 词频 = 某单词出现次数 / 总单词数目</span><br><span class="line">idf: inverse document frequency 逆向文档频率 = lg (总文档数目 / 包含某单词的文档数目)</span><br><span class="line">tf-idf = tf * idf</span><br><span class="line">tf-idf的值越高，代表该单词的重要性越高，作为关键词区分不同类型文章的价值越高</span><br><span class="line">关键词：在某一个类别的文章中出现次数很多，而在其它类别的文章中出现次数较少</span><br></pre></td></tr></table></figure><h4 id="demo-7">demo</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tf-idf十分重要，是分类机器算法进行文章分类中前期数据处理方式</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cut_words</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对字符串进行中文分词</span></span><br><span class="line"><span class="string">    :param: text</span></span><br><span class="line"><span class="string">    :return: &quot; &quot;.join(list(jieba.cut(text)))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tokenizer = jieba.cut(text)</span><br><span class="line">    <span class="comment"># print(tokenizer)</span></span><br><span class="line">    <span class="comment"># tokenizer: &lt;generator object Tokenizer.cut</span></span><br><span class="line">    l_token = <span class="built_in">list</span>(tokenizer)</span><br><span class="line">    <span class="comment"># print(l_token)</span></span><br><span class="line">    text = <span class="string">&quot; &quot;</span>.join(l_token)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tfidf_demo</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用tf-idf方法进行文本特征抽取</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    text = [<span class="string">&quot;我会在本学期认真自学课程内容，并按时上交课程作业&quot;</span>,</span><br><span class="line">            <span class="string">&quot;参加每一次课程考试。&quot;</span>,</span><br><span class="line">            <span class="string">&quot;如因未随堂上课迟交或错过提交作业、参加测试等影响本课程成绩，本人自行承担一切后果。&quot;</span>]</span><br><span class="line">    txt_new = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(text)):</span><br><span class="line">        txt_new.append(cut_words(text[i]))</span><br><span class="line">    transfer = TfidfVectorizer(stop_words=[<span class="string">&quot;本人&quot;</span>, <span class="string">&quot;我会&quot;</span>])</span><br><span class="line">    txt_new = transfer.fit_transform(txt_new)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;feature_names: \n&quot;</span>, transfer.get_feature_names())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;txt_new: \n&quot;</span>, txt_new.toarray())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    tfidf_demo()</span><br></pre></td></tr></table></figure><h2 id="综合运用实践">综合运用实践</h2><p>一个简单的数据集训练过程</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">separate</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;分隔符&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">datasets_demo</span>():</span><br><span class="line">    <span class="comment"># 获取数据集</span></span><br><span class="line">    iris = load_iris()</span><br><span class="line">    <span class="comment"># 查看数据集</span></span><br><span class="line">    <span class="built_in">print</span>(iris)</span><br><span class="line">    separate()</span><br><span class="line">    <span class="comment"># 查看键DESCR（数据集描述）的值</span></span><br><span class="line">    <span class="built_in">print</span>(iris[<span class="string">&quot;DESCR&quot;</span>])</span><br><span class="line">    separate()</span><br><span class="line">    <span class="comment"># 查看特征值名字</span></span><br><span class="line">    <span class="built_in">print</span>(iris.feature_names)</span><br><span class="line">    separate()</span><br><span class="line">    <span class="comment"># 查看特征值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;特征值: \n&quot;</span>, iris.data, <span class="string">&quot;\n特征值维数: &quot;</span>, iris.data.shape)</span><br><span class="line">    separate()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据集划分</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class="number">0.2</span>, random_state=<span class="number">22</span>)</span><br><span class="line">    <span class="comment"># 第一个参数特征值，第二个参数目标值，后面参数可选，为划分大小，默认测试集大小0.25; random_state随机数种子e</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;训练集的特征值: \n&quot;</span>, x_train, <span class="string">&quot;\n大小: &quot;</span>, x_train.shape)</span><br><span class="line">    separate()</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    datasets_demo()</span><br></pre></td></tr></table></figure><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
            <tag> 数据分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数理逻辑</title>
      <link href="/ymhui.github.io/2024/09/10/%E6%95%B0%E7%90%86%E9%80%BB%E8%BE%91/"/>
      <url>/ymhui.github.io/2024/09/10/%E6%95%B0%E7%90%86%E9%80%BB%E8%BE%91/</url>
      
        <content type="html"><![CDATA[<h1>期末题型</h1><h2 id="简答题9题-5-9">简答题9题 5 * 9</h2><p>三道纯文字，剩下需要构造</p><h2 id="6个大题-9-5-10-1">6个大题 9 * 5 + 10 * 1</h2><p>看哥德尔不完备定理证明的想法，P.200的思路</p><p>所有公式都是可编码的</p><h2 id="叙述题">叙述题</h2><h3 id="简述哥德尔定理的证明思路">简述哥德尔定理的证明思路</h3><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>G</mi><mi>P</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">ChatGPT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">ha</span><span class="mord mathnormal" style="margin-right:0.13889em;">tGPT</span></span></span></span> 如是说：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>↓</mo></mrow><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mrel">↓</span></span></span></span></p><p>哥德尔的证明是基于<strong>自指</strong>和<strong>递归函数</strong>的构造，具体思路如下：</p><p>第一不完备定理：</p><p>构造哥德尔编码 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> 构造递归公式 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> 不可证明性与自洽性</p><p>第二不完备定理：</p><p>如果一个系统是<strong>自洽的</strong>（即不包含矛盾），那么它不能证明自身的<strong>自洽性</strong></p><p>第二定理的结果基于第一定理，通过系统自洽性 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> 构造命题 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">G</span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> 证明不完备性</p><h3 id="定理30A的证明思路">定理30A的证明思路</h3><p>定理30A: 设 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊆</mo><mtext>Th</mtext><mi mathvariant="fraktur">N</mi></mrow><annotation encoding="application/x-tex">A \subseteq \text{Th}\mathfrak{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8193em;vertical-align:-0.136em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⊆</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">Th</span></span><span class="mord mathfrak">N</span></span></span></span> 是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="fraktur">N</mi></mrow><annotation encoding="application/x-tex">\mathfrak{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6914em;"></span><span class="mord mathfrak">N</span></span></span></span> 中取值为真的句子集，且 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span> 的哥德尔数集合 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi mathvariant="normal">#</mi><mi>α</mi><mo>∣</mo><mi>α</mi><mo>∈</mo><mi>A</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\#\alpha \mid \alpha \in A\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">#</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mclose">}</span></span></span></span> 是一个在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="fraktur">N</mi></mrow><annotation encoding="application/x-tex">\mathfrak{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6914em;"></span><span class="mord mathfrak">N</span></span></span></span> 中可定义的集合。则我们可以找到一个在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="fraktur">N</mi></mrow><annotation encoding="application/x-tex">\mathfrak{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6914em;"></span><span class="mord mathfrak">N</span></span></span></span> 中为真的句子 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>，但 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 不能由 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span> 推出。</p><p>证明思路：构造一个句子 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 来表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 本身不是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span> 的定理。概括地说，接下来的讨论是这样的：如果 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊢</mo><mi>σ</mi></mrow><annotation encoding="application/x-tex">A \vdash \sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⊢</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>，那么 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 是假的，这与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span> 是取值为真的句子集相矛盾。因此 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 是取值为真的句子但 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊬</mo><mi>σ</mi></mrow><annotation encoding="application/x-tex">A \not\vdash \sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mrel">⊢</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>。</p><h3 id="定理30A中三元关系R是怎么定义的？其中的符号a-b-c-alpha分别代表什么？其中的推理是如何编码的">定理30A中三元关系R是怎么定义的？其中的符号<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">a,b,c,\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>分别代表什么？其中的推理是如何编码的</h3><p>这样定义三元关系 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span>:</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">⟩</mo><mo>∈</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">\langle a,b,c\rangle \in R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">⟨</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span><span class="mclose">⟩</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span> <code>iff</code> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span> 是某个公式 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 的哥德尔数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span> 是从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false">(</mo><msup><mi>S</mi><mi>b</mi></msup><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a(S^b0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mord mathnormal">a</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span></span></span></span><span class="mord">0</span><span class="mclose">)</span></span></span></span> 的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span> 出发的某个推理 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="fraktur">G</mi></mrow><annotation encoding="application/x-tex">\mathfrak{G}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6914em;"></span><span class="mord mathfrak">G</span></span></span></span> 上的值</p><p>推理编码为：</p><p>将哥德尔数为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span> 的公式中的变元 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">v_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 换成数字 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span> 后所得到的公式的推理</p><h3 id="AE公理集有哪些？分别是什么？">AE公理集有哪些？分别是什么？</h3><p>如下：</p><p>0 不是任何数的后继  $$\forall x, Sx \neq 0$$  (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">S1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord">1</span></span></span></span>)。</p><p>后继运算的性质：两个数的后继相等，则它们相等   $$\forall x \forall y, (Sx = Sy \rightarrow x = y)$$  (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">S2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord">2</span></span></span></span>)。</p><p>？$$\forall x \forall y, (x &lt; Sy \leftrightarrow x \leq y)$$  (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">L1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mord">1</span></span></span></span>)。</p><p>没有小于 0 的数 $$\forall x, (x \not&lt; 0)$$  (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">L2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mord">2</span></span></span></span>)。</p><p>&lt; 是一个全序  $$\forall x \forall y, (x &lt; y \vee x = y \vee y &lt; x)$$  (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">L3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mord">3</span></span></span></span>)。</p><p>任何数加上 0 等于自身  $$\forall x, x + 0 = x$$  (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">A1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord">1</span></span></span></span>)。</p><p>加法运算的递归定义   $$\forall x \forall y, x + Sy = S(x + y)$$  (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">A2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord">2</span></span></span></span>)。</p><p>任何数乘以 0 等于 0   $$\forall x, x \cdot 0 = 0$$  (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">M1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord">1</span></span></span></span>)。</p><p>乘法运算的递归定义   $$\forall x \forall y, x \cdot Sy = x \cdot y + x$$  (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">M2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord">2</span></span></span></span>)。</p><p>任何数的 0 次方等于 1    $$\forall x, x^{E0} = S0$$  (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">E1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord">1</span></span></span></span>)。</p><p>幂乘运算的递归定义    $$\forall x \forall y, x^{E}Sy = x^{E}y \cdot x$$。(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">E2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord">2</span></span></span></span>)</p><h3 id="可表示、可定义是怎么定义的？有什么区别">可表示、可定义是怎么定义的？有什么区别</h3><p>可定义：设 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span> 是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">N</mi></mrow><annotation encoding="application/x-tex">\mathbb{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6889em;"></span><span class="mord mathbb">N</span></span></span></span> 上的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> 元关系，公式 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ρ</span></span></span></span> （其中只有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mo separator="true">,</mo><msub><mi>a</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">a_1,⋯,a_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是自由变元）在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">N</mi></mrow><annotation encoding="application/x-tex">\mathbb{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6889em;"></span><span class="mord mathbb">N</span></span></span></span> 中定义 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span> 当且仅当对于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">N</mi></mrow><annotation encoding="application/x-tex">\mathbb{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6889em;"></span><span class="mord mathbb">N</span></span></span></span> 中任意的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mo separator="true">,</mo><msub><mi>a</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">a_1,⋯,a_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>a</mi><mi>m</mi></msub><mo stretchy="false">⟩</mo><mo>∈</mo><mi>R</mi><mtext>  </mtext><mo>⟺</mo><mtext>  </mtext><msub><mo>⊨</mo><mi mathvariant="fraktur">N</mi></msub><mi>ρ</mi><mo stretchy="false">[</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>a</mi><mi>m</mi></msub><mo stretchy="false">]</mo><mtext>  </mtext><mo>⟺</mo><mtext>  </mtext><msub><mo>⊨</mo><mi mathvariant="fraktur">N</mi></msub><mi>ρ</mi><mo stretchy="false">(</mo><msup><mi mathvariant="bold">S</mi><msub><mi>a</mi><mn>1</mn></msub></msup><mn mathvariant="bold">0</mn><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msup><mi mathvariant="bold">S</mi><msub><mi>a</mi><mi>m</mi></msub></msup><mn mathvariant="bold">0</mn><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle a_1, \cdots, a_m \rangle \in R \iff \models_{\mathfrak{N}} \rho[a_1, \cdots, a_m] \iff \models_{\mathfrak{N}} \rho(\mathbf{S}^{a_1} \mathbf{0}, \cdots, \mathbf{S}^{a_m} \mathbf{0}).</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">⟨</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">⟩</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7073em;vertical-align:-0.024em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⟺</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.999em;vertical-align:-0.249em;"></span><span class="mrel"><span class="mrel">⊨</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.334em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathfrak mtight">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ρ</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⟺</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.999em;vertical-align:-0.249em;"></span><span class="mrel"><span class="mrel">⊨</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.334em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathfrak mtight">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ρ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord mathbf">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord mathbf">0</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></p><p>可表示：将上述结果写成两个蕴含式：</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>a</mi><mi>m</mi></msub><mo stretchy="false">⟩</mo><mo>∈</mo><mi>R</mi><mo>⇒</mo><msub><mo>⊨</mo><mi mathvariant="fraktur">N</mi></msub><mi>ρ</mi><mo stretchy="false">(</mo><msup><mi mathvariant="bold">S</mi><msub><mi>a</mi><mn>1</mn></msub></msup><mn mathvariant="bold">0</mn><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msup><mi mathvariant="bold">S</mi><msub><mi>a</mi><mi>m</mi></msub></msup><mn mathvariant="bold">0</mn><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mo stretchy="false">⟨</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>a</mi><mi>m</mi></msub><mo stretchy="false">⟩</mo><mo mathvariant="normal">∉</mo><mi>R</mi><mo>⇒</mo><msub><mo>⊨</mo><mi mathvariant="fraktur">N</mi></msub><mi mathvariant="normal">¬</mi><mi>ρ</mi><mo stretchy="false">(</mo><msup><mi mathvariant="bold">S</mi><msub><mi>a</mi><mn>1</mn></msub></msup><mn mathvariant="bold">0</mn><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msup><mi mathvariant="bold">S</mi><msub><mi>a</mi><mi>m</mi></msub></msup><mn mathvariant="bold">0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\langle a_1, \cdots, a_m \rangle \in R \Rightarrow \models_{\mathfrak{N}} \rho(\mathbf{S}^{a_1} \mathbf{0}, \cdots, \mathbf{S}^{a_m} \mathbf{0}) \\ \langle a_1, \cdots, a_m \rangle \notin R \Rightarrow \models_{\mathfrak{N}} \neg \rho(\mathbf{S}^{a_1} \mathbf{0}, \cdots, \mathbf{S}^{a_m} \mathbf{0})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">⟨</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">⟩</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⇒</span></span><span class="base"><span class="strut" style="height:0.999em;vertical-align:-0.249em;"></span><span class="mrel"><span class="mrel">⊨</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.334em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathfrak mtight">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ρ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord mathbf">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord mathbf">0</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">⟨</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">⟩</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mord"><span class="mrel">∈</span></span><span class="mord vbox"><span class="thinbox"><span class="llap"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="inner"><span class="mord"><span class="mord">/</span><span class="mspace" style="margin-right:0.0556em;"></span></span></span><span class="fix"></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⇒</span></span><span class="base"><span class="strut" style="height:0.999em;vertical-align:-0.249em;"></span><span class="mrel"><span class="mrel">⊨</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.334em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathfrak mtight">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">¬</span><span class="mord mathnormal">ρ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord mathbf">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord mathbf">0</span><span class="mclose">)</span></span></span></span></p><p>如果在这两个蕴含式中，“在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="fraktur">N</mi></mrow><annotation encoding="application/x-tex">\mathfrak{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6914em;"></span><span class="mord mathfrak">N</span></span></span></span> 中为真”的概念可以被更强的概念“从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>E</mi></msub></mrow><annotation encoding="application/x-tex">A_E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中推出”取代，我们称<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ρ</span></span></span></span>在理论<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>n</mi><mtext> </mtext><msub><mi>A</mi><mi>E</mi></msub></mrow><annotation encoding="application/x-tex">Cn~A_E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">n</span><span class="mspace nobreak"> </span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中可以表示<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span></p><p>区别：</p><p><strong>可表示</strong>强调的是某个对象是否可以在某个系统或模型中“存在”并“展示”。表示可以是具体的实例化过程。</p><p><strong>可定义</strong>则强调的是通过某种规则或逻辑结构来描述一个对象的方式，即能否通过精确的定义或条件来唯一确定一个对象。</p><h3 id="可表示函数、弱可表示的定义">可表示函数、弱可表示的定义</h3><p>可表示性：</p><p>设<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mi mathvariant="double-struck">N</mi><mi>m</mi></msup><mo>→</mo><mi mathvariant="double-struck">N</mi></mrow><annotation encoding="application/x-tex">f: \mathbb{N}^m \rightarrow \mathbb{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6889em;"></span><span class="mord"><span class="mord mathbb">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6889em;"></span><span class="mord mathbb">N</span></span></span></span>是自然数上的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> 元函数，公式中只有 $v_1, \cdots, v_{m+1} $是自由变元。我们称 $\varphi $（在 $\text{Cn } A_E $ 中）函数表示 $ f <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>，当且仅当对于</mtext></mrow><annotation encoding="application/x-tex">，当且仅当对于</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">，当且仅当对于</span></span></span></span> \mathbb{N}$ 中的每一 $ m$ 元组$a_1, \cdots, a_m $，</p><p>$ A_E \vdash \forall v_{m+1} [\varphi(\mathbf{S}^{a_1} \mathbf{0}, \cdots, \mathbf{S}^{a_m} \mathbf{0}, v_{m+1}) \leftrightarrow v_{m+1} = \mathbf{S}^{f(a_1, \cdots, a_m)} \mathbf{0}] $</p><p>弱可表示性（？）：</p><p>我们知道存在公式<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ρ</span></span></span></span>在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>n</mi><mtext> </mtext><msub><mi>A</mi><mi>E</mi></msub></mrow><annotation encoding="application/x-tex">Cn~A_E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">n</span><span class="mspace nobreak"> </span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 中表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span> 。因此，公式 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∃</mi><msub><mi>v</mi><mn>2</mn></msub><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\exists v_2ρ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">∃</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">ρ</span></span></span></span> 在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="fraktur">N</mi></mrow><annotation encoding="application/x-tex">\mathfrak{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6914em;"></span><span class="mord mathfrak">N</span></span></span></span> 中定义了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span>。这个公式在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>n</mi><msub><mi>A</mi><mi>E</mi></msub></mrow><annotation encoding="application/x-tex">Cn A_E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">n</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 中不能表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span>，除非 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span> 是递归的。但我们说这个公式几乎可以表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></p><h3 id="不动点定理的证明思路">不动点定理的证明思路</h3><p>不动点引理：对于只含有自由变元 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">v_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的公式 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> ，我们可以找到句子 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 使得</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>E</mi></msub><mo>⊢</mo><mo stretchy="false">[</mo><mi>σ</mi><mo>↔</mo><mi>β</mi><mo stretchy="false">(</mo><msup><mi mathvariant="bold">S</mi><mrow><mi mathvariant="normal">♯</mi><mi>σ</mi></mrow></msup><mn mathvariant="bold">0</mn><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">A_E \vdash [\sigma \leftrightarrow \beta(\mathbf{S}^{\sharp \sigma} \mathbf{0})].</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⊢</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">↔</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">♯</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">σ</span></span></span></span></span></span></span></span></span><span class="mord mathbf">0</span><span class="mclose">)]</span><span class="mord">.</span></span></span></span></p><p>我们可以认为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 间接地表达“<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> 是真的我”。当然，实际上 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 什么也没说，它只是一些符号串~~（这诗人话？）~~。甚至当我们在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="fraktur">N</mi></mrow><annotation encoding="application/x-tex">\mathfrak{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6914em;"></span><span class="mord mathfrak">N</span></span></span></span> 中把它翻译成自然语言时，它也仅仅是关于一些数字和它们的后继及运算结果的句子。</p><h3 id="解释范式定理中符号e-a-k-Tm-k-k-的意思">解释范式定理中符号<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><mi>T</mi><mi>m</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><msup><mi>k</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">e,a,k,Tm,k,k&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9463em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">e</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>的意思</h3><p>看不懂，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>G</mi><mi>P</mi><mi>T</mi></mrow><annotation encoding="application/x-tex">ChatGPT</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">ha</span><span class="mord mathnormal" style="margin-right:0.13889em;">tGPT</span></span></span></span>如是说：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>↓</mo></mrow><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mrel">↓</span></span></span></span></p><ol><li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi></mrow><annotation encoding="application/x-tex">e</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">e</span></span></span></span></strong>：是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> 元部分递归函数</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span>：数据</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>：长度为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span> 的数字序列，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>k</mi><msub><mo stretchy="false">)</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">(k)_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><msup><mi mathvariant="bold">S</mi><msub><mi>a</mi><mn>1</mn></msub></msup><mn>0</mn><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msup><mi mathvariant="bold">S</mi><msub><mi>a</mi><mi>m</mi></msub></msup><mn>0</mn><mo separator="true">,</mo><msup><mi mathvariant="bold">S</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><msub><mo stretchy="false">)</mo><mn>1</mn></msub></mrow></msup><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\phi(\mathbf{S}^{a_1}0,\cdots,\mathbf{S}^{a_m}0,\mathbf{S}^{(k)_1}0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord">0</span><span class="mclose">)</span></span></span></span> 的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>E</mi></msub></mrow><annotation encoding="application/x-tex">A_E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 推出的哥德尔数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="fraktur">G</mi></mrow><annotation encoding="application/x-tex">\mathfrak{G}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6914em;"></span><span class="mord mathfrak">G</span></span></span></span>。</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">Tm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal">m</span></span></span></span>：代表一个项(Term)</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo separator="true">,</mo><mtext> </mtext><msup><mi>k</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">k,~k&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9463em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace nobreak"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>：用来区分不同的元素或者公式。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人机交互系统</title>
      <link href="/ymhui.github.io/2024/09/10/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F/"/>
      <url>/ymhui.github.io/2024/09/10/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<h1>人机交互系统</h1><h2 id="References">References</h2><p>蔡之恒 ——《人机交互——要背的》，室友提炼的重点（？）</p><h2 id="人机交互的概念">人机交互的概念</h2><ul><li>HCI， Human Computer Interaction</li><li>HCI是一门与人类使用的交互式计算系统的设计、评估和实施有关的学科，并且研究了他们周围的主要现象</li><li>以用户为中心(UCD, User Centered Design)的设计方法</li></ul><h2 id="人机交互发展历史">人机交互发展历史</h2><p>有四个阶段，但不是后面出现了前面的就消失了，旧的交互形式会作为特例保存下来</p><h3 id="批处理卡片">批处理卡片</h3><ul><li>每次只能有一个用户对计算机进行操作</li><li>编写程序使用<code>01</code>串表示的机器语言</li><li>缺点：<ul><li>不符合人的习惯</li><li>耗时易出错</li><li>只有少数专业人士运用自如</li></ul></li></ul><h3 id="联机终端（命令行）">联机终端（命令行）</h3><p>*缺点：*大部分命令语言对用户的输入有严格要求</p><p>命令名称的缩写在一定程度上减轻了用户的使用负担</p><h3 id="图形用户界面（GUI）">图形用户界面（GUI）</h3><p><strong>Graphical User Interface</strong></p><ul><li>主要特征：直接操纵</li><li>GUI优于字符界面吗？<ul><li>不同的交互方式本身在可用性方面没有根本性区别，更重要的是认真对待界面设计的态度 [Whiteside 1985]</li></ul></li><li>优点：依赖识别而非记忆（10条启发式规则之一）</li></ul><h3 id="未来的人机交互">未来的人机交互</h3><ul><li>多媒体界面</li><li>多通道交互技术</li><li>虚拟现实、语音交互、脑机交互</li></ul><h2 id="EEC">EEC</h2><p><strong>Execution Evaluation Circle 执行-评估活动周期</strong></p><h3 id="活动的四个组成部分">活动的四个组成部分</h3><ul><li>目标（Goal）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo mathvariant="normal">≠</mo></mrow><annotation encoding="application/x-tex">\ne</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span></span></span></span> 意图（Intention），单个目标可对应多个意图</li><li>执行（Execution）</li><li>客观因素（World）</li><li>评估（Evaluation）</li></ul><h3 id="EEC模型">EEC模型</h3><img src="/ymhui.github.io/2024/09/10/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F/EEC.png" class="" title="image-20241226150151702"><ul><li>共7个阶段：1 - 4执行阶段，5 - 7评估阶段</li><li>可解释为什么有些界面的使用存在问题</li></ul><h3 id="执行隔阂">执行隔阂</h3><p>用户为达目标而制定的动作与系统允许的动作之间的差别</p><h3 id="评估隔阂">评估隔阂</h3><p>系统状态的实际表现与用户预期之间的差别</p><h2 id="界面类型">界面类型</h2><p>这是啥？</p><h2 id="人类处理机模型">人类处理机模型</h2><h3 id="三个交互式组件">三个交互式组件</h3><ul><li><strong>感知处理器</strong>：信息将输出到声音存储和视觉存储区域</li><li><strong>认知处理器</strong>：输入将输出到工作记忆</li><li><strong>动作处理器</strong>：执行动作</li></ul><h3 id="存在的问题">存在的问题</h3><ul><li>把认知过程描述为一系列处理步骤</li><li>仅关注单个人和单个任务的执行过程，忽视了复杂操作执行中人与人之间及任务与任务之间的互动，忽视了环境和其他人的影响</li></ul><h2 id="格式塔心理学">格式塔心理学</h2><h3 id="相近性原则">相近性原则</h3><p>空间上比较靠近的物体容易被视为整体</p><img src="/ymhui.github.io/2024/09/10/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F/relative.png" class="" title="image-20241226152036538"><h3 id="相似性原则">相似性原则</h3><p>人们习惯将看上去相似的物品看成一个整体</p><img src="/ymhui.github.io/2024/09/10/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F/similar.png" class="" title="image-20241226152103314"><h3 id="连续性原则">连续性原则</h3><p>共线或具有相同方向的物体会被组合到一起</p><img src="/ymhui.github.io/2024/09/10/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F/%E8%BF%9E%E7%BB%AD%E6%80%A7%E5%8E%9F%E5%88%99.png" class="" title="image-20241226152126736"><h3 id="完整和闭合性原则">完整和闭合性原则</h3><p>人们倾向于忽视轮廓的间隙而将其视作一个完整的整体</p><img src="/ymhui.github.io/2024/09/10/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F/%E5%AE%8C%E6%95%B4%E5%92%8C%E9%97%AD%E5%90%88%E6%80%A7%E5%8E%9F%E5%88%99.png" class="" title="image-20241226152327790"><h2 id="人脑记忆特性">人脑记忆特性</h2><h3 id="三个阶段">三个阶段</h3><ul><li>感觉记忆</li><li>短时记忆</li><li>长时记忆</li></ul><h3 id="感觉记忆">感觉记忆</h3><ul><li>又称瞬时记忆</li><li>在脑中持续约<code>1s</code></li><li>帮助人们把相继出现的一组图片组合成一个连续的图像序列</li></ul><h3 id="短时记忆">短时记忆</h3><ul><li>感觉记忆经编码后形成</li><li>又称工作记忆，约保持<code>30s</code></li><li>储存的是当前正在使用的信息，是信息系统加工的核心，可以理解为计算机的<strong>内存</strong></li><li>短时记忆的存储能力约为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><mo>±</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">7 \pm 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">±</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span>个单元</li></ul><h3 id="长时记忆">长时记忆</h3><ul><li>短时记忆中的信息经进一步加工后会变成长时记忆</li><li><strong>只有与长时记忆区的信息具有某种联系的新信息</strong>才能进入长时记忆</li><li>长时记忆的容量几乎无限，但会遗忘；遗忘指长时记忆中的信息无法提取，但不是信息丢失了</li></ul><h2 id="交互设计目标">交互设计目标</h2><h3 id="可用性目标">可用性目标</h3><ul><li>易学性</li><li>易记性</li><li>高效率</li><li>安全性</li><li>效用性<ul><li><code>utility</code>, 指<strong>一定程度上</strong>该产品<strong>提供了正确的功能</strong>，可以让用户做他们需要做或想做的事；这个<strong>程度</strong>就是衡量效用性的标准</li></ul></li></ul><h3 id="用户体验目标">用户体验目标</h3><p>通常交互式系统包含一个多样性的用户体验目标，涵盖了一系列情绪和感受体验</p><p>在特定时间和地点使用或者与一个产品交互时，选择传达用户的感受、目前状态、情绪、感觉等最佳词汇的过程，可以帮助设计者了解用户体验的多面性变化性的本质</p><h3 id="可用性和用户体验的关系">可用性和用户体验的关系</h3><ul><li>体验是主观的而可用性是客观的</li><li>矛盾性<ul><li>用软锤打屏幕上的老鼠较用鼠标点击更费劲且容易出错，但体验会更愉快、有趣</li></ul></li><li>有些可用性和用户体验目标是不兼容的<ul><li>设计既安全又有趣的过程控制系统是不可能的也不可取的</li></ul></li><li>认识和理解可用性和其他用户体验目标之间的关系是交互设计的核心</li></ul><h3 id="简易可用性工程">简易可用性工程</h3><p><strong>四种有效技术</strong></p><ul><li>用户和人物观察</li><li>场景</li><li><strong>边做边说</strong><ul><li>让真实用户在使用系统执行一组特定任务的时候，讲出他们的所思所想</li><li><strong>最有价值的单个可用性工程方法</strong></li><li>可了解用户为什么这样做，并确定其可能对系统产生的误解</li><li>实验人员需要不断提示用户，或请他们事先观摩</li></ul></li><li><strong>启发式评估</strong><ul><li>涉及定量的问题</li><li>能发现许多可用性问题</li><li>为避免偏见，应当让多个不同的人进行经验性评估</li><li>流程：<ul><li>检查界面</li><li>与启发式规则对比列举</li><li>列举可用性问题</li><li>用启发式规则检查并确认问题</li></ul></li></ul></li></ul><p>边做边说 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo></mrow><annotation encoding="application/x-tex">+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">+</span></span></span></span> 启发式评估可以发现软件设计中的大部分问题</p><h2 id="交互设计原则">交互设计原则</h2><h3 id="8条黄金规则">8条黄金规则</h3><ol><li>尽可能保持一致</li><li>符合普遍可用性</li><li>提供信息丰富的反馈</li><li>设计说明对话框以生成结束信息</li><li>预防并处理错误</li><li>让操作容易撤销</li><li>支持内部控制点</li><li>减轻短时记忆负担</li></ol><h3 id="10条启发式规则">10条启发式规则</h3><p>与黄金规则有一定重复，括号内注明</p><ol><li>系统状态可见度（3）</li><li>系统和现实世界吻合（2）</li><li>用户享有控制权和自主权（6， 7）</li><li>一致性和标准化（1，8）</li><li>避免出错（5）</li><li>依赖识别而非记忆（8）</li><li>使用的灵活性和高效性（6）</li><li>审美感和最小化设计（8）</li><li>帮助用户识别、诊断和恢复错误（5，6）</li><li>帮助和文档（8）</li></ol><h2 id="交互设计过程">交互设计过程</h2><p><em>课件上找的，不太确定</em></p><h3 id="定义外形因素和输入方法">定义外形因素和输入方法</h3><p>TBD</p><h3 id="定义功能和数据元素">定义功能和数据元素</h3><p>TBD</p><h3 id="决定功能组合层次">决定功能组合层次</h3><p>TBD</p><h3 id="勾画大致的设计框架">勾画大致的设计框架</h3><p>TBD</p><h3 id="构建关键情景场景剧本">构建关键情景场景剧本</h3><p>TBD</p><h3 id="通过验证性场景剧本来检查设计">通过验证性场景剧本来检查设计</h3><p>TBD</p><h2 id="交互式系统的需求">交互式系统的需求</h2><h3 id="需求获取">需求获取</h3><ul><li>观察（直接&amp;间接）</li><li>场景</li></ul><h3 id="用户建模——人物角色">用户建模——人物角色</h3><ul><li>不是真实的人</li><li>是基于观察到的那些真实人的行为和动机，并且在整个设计过程中代表真实的人</li><li>是在人口统计学调查收集到的实际用户的行为数据的基础上形成的综合原型，拼凑出几个用户档案是不行的</li><li>作用：解决产品开发过程中出现的三个设计问题：<ul><li>弹性用户</li><li>自参考设计（开发者以为用户会按自己预设的想法做事情？）</li><li>边缘情况设计</li></ul></li></ul><p>人物角色 + 情景 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> 需求（需求获取：观察 + 场景）</p><h3 id="需求验证——原型">需求验证——原型</h3><h4 id="低保真原型">低保真原型</h4><p>多数项目的首选</p><ul><li>与最终产品不太相似的原型</li><li>优点是简单、快速、便宜、易于制造和修改</li><li>三种方式：<ul><li><strong>草图</strong>：每张卡片代表一个屏幕或屏幕一部分，经常用于网站开发</li><li><strong>故事板</strong>：一系列草图，展示了用户如何使用该设备完成任务，在设计早期使用，通常与场景一起使用，具有更多细节，并有机会进行角色扮演</li><li><strong>绿野仙踪法</strong>：用户以为他们在与计算系统交互，但实际上是开发人员同他们进行相应</li></ul></li></ul><h4 id="高保真原型">高保真原型</h4><ul><li>与最终产品更为接近，使用相同材料</li><li>制作时间长，难以修改（这也算缺点之一）</li><li>风险：用户会认为原型就是系统，开发人员可能认为已经找到了一个用户满意的设计（但实际没有）</li></ul><h3 id="层次化任务分析-HTA">层次化任务分析 HTA</h3><p><strong>H</strong>ierarchical <strong>T</strong>ask <strong>A</strong>nalysis</p><p>把任务分解为若干子任务，再把子任务进一步分解为更细致的子任务；之后把它们组织成一个”执行次序“，说明实际情况下如何执行各项任务。</p><h2 id="评估">评估</h2><ul><li><p>评估是设计过程的组成部分</p></li><li><p>评估是系统化的数据搜集过程</p></li><li><p>评估侧重系统的可用性和用户体验</p></li></ul><h3 id="评估的原则">评估的原则</h3><ol><li>评估应该依赖产品的用户，与专业技术人员的水平和技术无关</li><li>评估与设计应结合进行，仅靠用户最后对产品的一两次评估无法反映出软件可用性（评估不是设计过程中的一个独立阶段）</li><li>评估应在用户的实际工作任务和操作环境下进行，根据用户完成任务得结果，进行客观的分析和评估</li><li>要选择有广泛代表性的用户，参加测试的人必须具有代表性</li></ol><h3 id="评估范型">评估范型</h3><h4 id="快速评估">快速评估</h4><ul><li>可以在任何阶段进行</li><li>强调“快速了解”，而非仔细记录研究发现，得到的数据通常是非正式的、叙述性的</li><li>是设计网站时常用的方法</li><li>基本特性：<strong>快速</strong></li></ul><h4 id="可用性测试">可用性测试</h4><p>用户测试，<strong>DECIDE评估框架</strong></p><ul><li>评估典型用户执行典型任务时的情况，包括出错次数、完成任务的时间等</li><li>基本特性：在评估人员的密切控制下实行</li><li>主要任务：量化表示用户的执行情况</li><li>缺点：<ul><li>测试用户的数量通常较少</li><li>不适合细致的数据分析</li></ul></li></ul><h4 id="实地研究">实地研究</h4><p>可以对<strong>小孩子</strong>用户群体进行</p><ul><li>基本特征：在自然工作环境中进行</li><li>目的：理解用户的实际工作情形以及技术对他们的影响</li><li>重难点：<ul><li>如何不对受试者造成影响</li><li>控制权在用户，很难预测即将发生和出现的情况</li></ul></li></ul><h4 id="预测性评估">预测性评估</h4><ul><li>研究人员通过想象或对界面的使用过程进行建模，逐步通过场景或基于问题回答的走查法</li><li>基本特征：用户可以不在场，整个过程快速、成本较低</li><li><strong>启发式评估属于预测性评估</strong></li></ul><h2 id="用户测试">用户测试</h2><h3 id="DECIDE评估框架">DECIDE评估框架</h3><p><em>考试的时候要写具体内容，比如”决定评估的总体目标是…“，不能单纯背步骤</em></p><h4 id="六个步骤，分别对应D-E-C-I-D-E">六个步骤，分别对应D, E, C, I, D, E</h4><ol><li>决定评估需要完成的总体目标 <strong>D</strong>etermine</li><li>发掘需要回答的具体问题 <strong>E</strong>xplore</li><li>选择评估范型和技术 <strong>C</strong>hoose</li><li>标识必须解决的实际问题，如测试用户的选择 <strong>I</strong>dentify</li><li>决定如何处理有关道德的问题 <strong>D</strong>ecide<ol><li>保护个人隐私，可签署协议书</li><li>指导原则：<ol><li>说明研究的目的及要求参与者做的工作</li><li>说明保密事项，对用户 &amp; 对项目</li><li>测试对象是软件，而非个人</li><li>对测试过程的特殊要求，是否边做边说等</li><li>说明是否对过程录像</li><li>用户有随时终止测试的权利</li></ol></li></ol></li><li>评估解释并表示数据 <strong>E</strong>valuate</li></ol><h3 id="测试设计">测试设计</h3><ol><li><p>定义目标和问题</p><p>目标描述了开展一个测试的原因，定义了测试在整个项目中的价值</p><p>目标是对关注点的说明和解答</p></li><li><p>选择参与者</p><p>参与者的选择对任何实验的成功至关重要</p><p>了解用户特性有助于选择典型用户，要尽可能接近实际用户</p><p>通常要平衡性别</p><p>至少4 ~ 5位，5 ~ 12位就足够了（视情况而定）</p><ol><li>参与者安排<ul><li>各种实验情形的参与者不同</li><li>各种情形的参与者相同</li><li>参与者配对</li></ul></li><li>参与者不同<ul><li>随即指派某个参与者组执行某个实验情形</li><li>缺点：参与者要足够多，结果可能受个别人影响（解决：随机分配/预测试）</li><li>优点：不存在”顺序效应“，即参与者在执行前一组任务时获得的经验将影响后面的测试任务</li></ul></li><li>参与者相同<ul><li>相同的参与者执行所有实验情形</li><li>与前一种方法相比，它只需要一半的参与者</li><li>优点：<ul><li>消除个别差异的影响</li><li>便于比较参与者执行不同实验情形的差异</li></ul></li><li>缺点：<ul><li>可能产生”顺序效应“</li><li>解决方法：<strong>均衡处理，即如果有两项任务A和B，应让一半参与者先A后B，另一半先B后A</strong></li></ul></li></ul></li><li>参与者配对<ul><li>按用户特性（技能/性别）把两位参与者组成组成一组，再随机安排他们执行某一种实验情形</li><li>适用于参与者无法执行两个实验的情形</li><li>缺点：结果可难受一些未考虑到的变量影响</li></ul></li></ol></li><li><p>设计测试任务</p><ul><li>测试任务应当与定义的目标有关</li><li>通常是简单任务</li><li>有时采用复杂任务</li><li>任务不能仅限于测试的功能，应使用户全面的使用设计的各个区域</li><li>每项任务时间介于5 ~ 20分钟</li><li>应当以合乎逻辑的方法安排任务</li></ul></li><li><p>明确测试步骤</p><ul><li>测试前准备测试进度和说明，设置好设备</li><li>正式测试前先进行小规模测试</li><li>评估人员必要时询问参与者遇到了什么问题</li><li>若用户确实无法完成某些任务，继续下一项</li><li>测试过程控制在1小时以内</li><li>必须分析所有搜集到的数据</li></ul></li><li><p>数据搜集</p><ul><li>确定如何度量观测的结果</li><li>使用的度量类型（定性/定量）依赖于所选择的任务</li><li>常用的定量度量：<ul><li>完成任务的时间</li><li>停止使用一段时间后完成任务的时间</li><li>执行某项任务出错次数和类型</li><li>单位时间出错次数</li><li>求助帮助的次数</li><li>犯某个特定错误的次数</li><li>成功完成任务的用户数</li></ul></li></ul></li></ol><h3 id="为什么进行显著性检验">为什么进行显著性检验</h3><p>个体的结果不能代表群体</p><p>当面对庞大的群体时我们不得不进行抽样检测，但是样本的结果又不能代表全体</p><p>因此显著性检验帮我们确定将样本的结果推广到全体的把握有多大</p><h3 id="评估实验的流程">评估实验的流程</h3><p>大致如下：</p><ol><li>确定测试用户群体</li><li>开展小规模预实验</li><li>使用DECIDE框架开展实验</li></ol><h3 id="一个实例——图标设计评估">一个实例——图标设计评估</h3><ul><li>自变量：<ul><li>图标的样式</li><li>自然的和抽象的</li></ul></li><li>因变量：<ul><li>关心用户记忆精确性方面的性能？还是记忆速度方面的性能？还是用户偏爱等主观度量？</li><li>假设选择一个图标的速度是记忆容易程度的一个指标<ul><li>在选择中错误的数目</li><li>选择一个图标所花费的时间</li></ul></li></ul></li></ul><p><em>注：去课件上看一遍该例子完整的实验设计，还有另一个例子——网站评估实例</em></p><h2 id="交互系统的设计策略">交互系统的设计策略</h2><h3 id="四个策略">四个策略</h3><ol><li>删除<ul><li>删除不必要的</li></ul></li><li>组织<ul><li>组织要提供的</li></ul></li><li>隐藏<ul><li>隐藏非核心的</li></ul></li><li>转移<ul><li>卡片分类</li></ul></li></ol><h2 id="交互设计的模型和理论">交互设计的模型和理论</h2><h3 id="模型">模型</h3><h4 id="预测模型">预测模型</h4><ul><li>能偶预测用户的执行情况，但不需要对用户做实际测试</li><li>特别适合用于无法执行用户测试的情形</li></ul><h4 id="GOMS">GOMS</h4><ul><li><p><strong>G</strong>oal 目标，<strong>O</strong>perator 操作，<strong>M</strong>ethod 方法，<strong>S</strong>election-Rule 选择规则</p><ul><li>GOMS认为方法的选择不是随机的</li></ul></li><li><p>最著名的<strong>预测模型</strong></p></li><li><p>基于<strong>人类处理机模型</strong></p></li><li><p>是关于人类如何执行认知—动作型任务以及如何与系统交互的理论模型</p></li><li><p>优点：能够容易地对不同界面或系统进行比较分析</p></li><li><p>局限性：</p><ul><li>假设用户完全按一种正确的方式进行人机交互，没有清楚地描述错误处理过程</li><li>只针对那些不犯任何错误的专家用户</li><li>任务之间的关系描述过于简单</li><li>忽略了用户间的个体差异</li></ul></li></ul><h4 id="KLM击键层次模型">KLM击键层次模型</h4><ul><li>对用户执行情况进行<strong>量化</strong>预测，仅涉及任务性能的一个方面：时间</li><li>用途：<ul><li>预测无错误情况下专家用户在一系列输入前提下完成任务的时间</li><li>便于比较不同系统</li><li>确定何种方案能最有效地支持特定任务</li></ul></li><li>但没有考虑一下内容：<ul><li>错误、学习性、功能性、回忆、专注程度、疲劳、可接受性等</li></ul></li></ul><h5 id="放置M的启发式规则">放置M的启发式规则</h5><ol><li>访问长时记忆区前的操作需要一个<code>M</code>，比如输入字符串、鼠标移动到某处（见下）</li><li>先在所有<code>K</code>和<code>P</code>操作前放置<code>M</code><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>→</mo><mi>M</mi><mi>K</mi><mo separator="true">;</mo><mtext> </mtext><mi>P</mi><mo>→</mo><mi>M</mi><mi>P</mi></mrow><annotation encoding="application/x-tex">K \rightarrow MK;~ P \rightarrow MP</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">;</span><span class="mspace nobreak"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">MP</span></span></span></span></li></ul></li><li>删除键入单词或字符串之间的<code>M</code><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>K</mi><mi>M</mi><mi>K</mi><mi>M</mi><mi>K</mi><mi>M</mi><mi>K</mi><mo>→</mo><mi>M</mi><mi>K</mi><mi>K</mi><mi>K</mi><mi>K</mi></mrow><annotation encoding="application/x-tex">MKMKMKMK \rightarrow MKKKK</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.07153em;">KKKK</span></span></span></span></li></ul></li><li>删除复合操作之间的<code>M</code>（如鼠标选中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo></mrow><annotation encoding="application/x-tex">+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">+</span></span></span></span> 点击）<ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>P</mi><mi>M</mi><msub><mi>P</mi><mn>1</mn></msub><mo>→</mo><mi>M</mi><mi>P</mi><msub><mi>P</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">MPMP_1 \rightarrow MPP_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MPM</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">MP</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li></ul></li></ol><p>总结：输入字符串之前需要一个<code>M</code>，<code>P</code>和<code>D</code>之前需要一个<code>M</code></p><h3 id="理论">理论</h3><h4 id="Fitts定律">Fitts定律</h4><ul><li>能够<strong>预测</strong>使用某种定位设备指向某个目标的时间</li><li>人机交互中，根据目标大小及到目标的距离，计算指向该目标的时间，可以指导设计人员设计按钮的位置、大小和密集程度</li><li>对GUI（图形化用户界面设计）有指导意义</li></ul><h4 id="Fitts定律使用建议">Fitts定律使用建议</h4><ul><li><strong>大目标、小距离具有优势</strong></li><li>大目标：<ul><li>屏幕元素应当<strong>尽可能多的占据屏幕空间</strong></li><li>屏幕元素要尽可能<strong>利用屏幕边缘的优势</strong></li></ul></li><li>小距离：<ul><li><strong>最好的像素是光标所处的像素</strong></li></ul></li><li><strong>大菜单比如饼型菜单</strong>，要比其他类型的菜单<strong>使用简单</strong></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 软件学院 </tag>
            
            <tag> 人机交互 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零启动个人博客</title>
      <link href="/ymhui.github.io/2024/09/09/%E4%BB%8E%E9%9B%B6%E5%90%AF%E5%8A%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/ymhui.github.io/2024/09/09/%E4%BB%8E%E9%9B%B6%E5%90%AF%E5%8A%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h1>如何从零开始搭建个人博客 —— Windows版本</h1><p><s>本文没有多余的废话，直接步入正题</s></p><h2 id="step-1-注册账号">step 1 注册账号</h2><p>一个GitHub账号，注册方式请自行搜索</p><h2 id="step-2-创建仓库">step 2 创建仓库</h2><p>登录GitHub账号，进入首页；在左侧仓库列表栏中点击<code>New</code></p><p><img src="%E4%BB%8E%E9%9B%B6%E5%90%AF%E5%8A%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/github_homepage.png" alt="github_homepage"></p><p>进入仓库创建页面</p><p><img src="%E4%BB%8E%E9%9B%B6%E5%90%AF%E5%8A%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/create_repo.png" alt="create_repo"></p><ul><li>在<code>Repository name</code>处填写自己的个人博客仓库名字，须遵循***.github.io**的格式，*****处可填写自己想要的用户名 e.g. <a href="http://zhangsan.github.io">zhangsan.github.io</a></li><li><code>Description</code>处填写对仓库的描述</li><li>勾选<code>public</code></li><li>最好勾选<code>Add a README file</code></li><li>其余保持默认即可</li></ul><p>点击<code>Create repository</code>创建仓库</p><h2 id="step-3-为个人博客生成一个页面">step 3 为个人博客生成一个页面</h2><p>进入刚创建的仓库</p><ol><li>点击导航栏处的<code>Setttings</code>选项卡，进入设置界面</li><li>随后点击左侧导航栏的<code>Pages</code>选项，进入页面选项</li><li>之后在<code>Branch</code>区域将分支切换为main (<a id="master">稍后要修改为master</a>)</li><li>点击<code>Save</code></li><li>之后便可以看到Github为你的博客自动生成了一个url</li></ol><p><img src="%E4%BB%8E%E9%9B%B6%E5%90%AF%E5%8A%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/create_page.png" alt="create_page"></p><p><em>注：如果没有看到生成的url，可以刷新一下页面或者重新 Settings =&gt; Pages进入该界面</em></p><p>访问这个url就可以看到你的个人博客了！内容一般为README的内容~</p><p><img src="%E4%BB%8E%E9%9B%B6%E5%90%AF%E5%8A%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/image-20240909105836327.png" alt="image-20240909105836327"></p><p>到这里你就完成了一个基本的个人博客搭建，如果希望自己的博客不只是单纯的文字而是具备一定的~~<strong>交互性</strong>和<strong>美观度</strong>~~，可以进行下面内容的配置</p><h1>丰富你的博客主题与内容</h1><p>下面介绍一个可以拓展博客主题的工具<strong>Hexo</strong>，指路官方文档 =&gt; <a href="https://hexo.io/docs/">Documentation | Hexo</a></p><p>Hexo工具容易配置，简单易上手，作为入门比较推荐</p><h2 id="step-1-安装git">step 1 安装git</h2><p>请上<a href="https://git-scm.com/">官网Git (git-scm.com)</a>选择适配自己系统的版本，跟随安装指引即可完成</p><h2 id="step-2-安装node-js">step 2 安装node.js</h2><p>请上<a href="https://nodejs.org/en">官网 Download | Node.js</a>选择合适的版本，跟随安装指引即可完成</p><h2 id="step-3-验证git与node-js安装成功">step 3 验证git与node.js安装成功</h2><p>安装完成后打开Power Shell或cmd，输入下列命令，若均输出版本号则git与node.js安装成功</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git --version</span><br><span class="line">$ node -v</span><br><span class="line">$ npm -v</span><br></pre></td></tr></table></figure><p><em>注：若<code>git --version</code>后出现报错，可能是因为 git 没有被加入环境变量，此时可以找到 git.exe所在父级目录(bin)，将 bin的绝对路径加入系统环境变量 path；也可以暂缓此步，直接打开 Git Bash终端，若在该终端中 git --version有版本号输出，可以继续下面步骤，反之请自行搜索解决 git安装问题</em></p><h2 id="step-4-连接本地与GitHub">step 4 连接本地与GitHub</h2><p>打开刚刚下载好的Git Bash终端</p><p><em>（如果找不到下到什么地方或者忘记创建快捷方式，可使用<code>win</code>唤起开始界面，在搜索栏搜索<strong>git</strong>，最佳匹配即是）</em></p><p>在终端中输入下面命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git config --global user.name <span class="string">&quot;GitHub用户名&quot;</span></span><br><span class="line">$ git config --global user.email <span class="string">&quot;GitHub邮箱&quot;</span></span><br></pre></td></tr></table></figure><p>继续创建ssh密钥</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C <span class="string">&quot;GitHub邮箱&quot;</span></span><br></pre></td></tr></table></figure><p>随后一直回车完成创建</p><p>看到类似下图结果即为创建成功</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">The key<span class="string">&#x27;s randomart image is:</span></span><br><span class="line"><span class="string">    +---[RSA 3072]----+</span></span><br><span class="line"><span class="string">    |                 |</span></span><br><span class="line"><span class="string">    |       .         |</span></span><br><span class="line"><span class="string">    |        + * .. + |</span></span><br><span class="line"><span class="string">    |       . + = .  o|</span></span><br><span class="line"><span class="string">    |      . S   . . .|</span></span><br><span class="line"><span class="string">    |     . .     . . |</span></span><br><span class="line"><span class="string">    |  .   = .  ...o  |</span></span><br><span class="line"><span class="string">    |   +.= o +.++o   |</span></span><br><span class="line"><span class="string">    |                 |</span></span><br><span class="line"><span class="string">    +----[SHA256]-----+</span></span><br></pre></td></tr></table></figure><p>找到密钥文件生成的路径<strong>Your public key has been saved in /yourpath/.ssh/id_rsa.pub</strong>，打开并复制文件全部内容</p><ul><li><p>登录GitHub，点击右上角自己的头像</p></li><li><p>再点击Settings</p></li><li><p>找到SSH and GPG keys，点击New SSH key</p></li><li><p>将刚才复制的内容粘贴到<code>Key</code>文本框，然后起一个你想要的title，其余保持默认，点击Add SSH key</p></li></ul><p><img src="%E4%BB%8E%E9%9B%B6%E5%90%AF%E5%8A%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/githubhome.png" alt="githubhome"></p><p><img src="%E4%BB%8E%E9%9B%B6%E5%90%AF%E5%8A%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/github_rightmenu.png" alt="github_rightmenu"></p><p><img src="%E4%BB%8E%E9%9B%B6%E5%90%AF%E5%8A%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/sshpage.png" alt="sshpage"></p><p><img src="%E4%BB%8E%E9%9B%B6%E5%90%AF%E5%8A%A8%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/add_ssh_page.png" alt="add_ssh_page"></p><h2 id="step-5-验证连接">step 5 验证连接</h2><p>使用ssh验证本地与GitHub是否正确连接</p><p>输入以下命令之一，出现**Hi xxx! You’ve successfully authenticated, but GitHub does not provide shell access.**则说明连接成功</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br><span class="line">$ ssh git@github.com</span><br></pre></td></tr></table></figure><p>如果上述两个命令均不可行，且出现<strong>ssh: connect to host <a href="http://github.com">github.com</a> port 22: Connection refused</strong>报错，说明你所在的局域网禁用了端口22。此时进入刚才生成的ssh密钥文件的父目录.ssh，创建或修改config文件为如下内容</p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Host github.com</span><br><span class="line">  Hostname ssh.github.com</span><br><span class="line">  Port 443</span><br></pre></td></tr></table></figure><p>此时再次使用上述两个命令之一，<s>应该</s>至少会成功一个</p><p>若仍不成功或出现其他报错，请自行<a href="https://chatgpt.com">搜索</a>解决~~，绝对不是我也不知道~~</p><h2 id="step-6-安装Hexo工具">step 6 安装Hexo工具</h2><p>使用npm安装Hexo</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure><p>输入<code>hexo -v</code>查看是否安装成功</p><p>注：若遇到安装后仍报错找不到hexo的问题请在所有涉及<code>hexo</code>的命令前加上<code>npx</code></p><p>即</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npx hexo -v</span><br></pre></td></tr></table></figure><h2 id="step-7-建立hexo根目录">step 7 建立hexo根目录</h2><p>新建一个目录作为hexo站点根目录，在该目录下</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo init</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure><p><em>注：如果遇到指令运行报错，请多在Power Shell和 Git Bash中切换尝试运行，<s>个中缘由我也不知</s>，如遇全都报错，还请<a href="https://chatgpt.com">自行搜索解决</a></em> ToT</p><p><strong>再插入一次官方文档 <a href="https://hexo.io/docs/">Documentation | Hexo</a>, 其中介绍了一部分hexo的解析机制</strong></p><p>根目录的文件树大致如下, 一般来说只多不少，<s>不过秉承着能跑就不要过问的原则，如果之后可以正常运行此时的文件树并不重要</s></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── _config.yml     ---&gt;  网站配置信息</span><br><span class="line">├── package.json    ---&gt;  应用程序的信息</span><br><span class="line">├── scaffolds       ---&gt;  模板文件夹</span><br><span class="line">├── source          ---&gt;  放置资源文件。</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts      ---&gt;  .md文件，其中存放的就是你的博客正文</span><br><span class="line">└── themes</span><br></pre></td></tr></table></figure><h2 id="step-8-写博客">step 8 写博客</h2><p>下文如不特殊说明，路径<code>/</code>均代表站点根目录</p><p><strong>想要编写的博客正文均放在<code>/source/_posts/</code>目录下，格式为markdown，即所有博客正文路径均为 <code>/source/_posts/*.md</code></strong></p><p>markdown的编写请自行学习</p><h2 id="step-9-生成网页">step 9 生成网页</h2><p>使用如下命令之一生成网页</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate <span class="comment"># 生成页面</span></span><br><span class="line">$ hexo g <span class="comment"># 简写</span></span><br></pre></td></tr></table></figure><p>~~一些废话：~~此处展现出Hexo的便捷之处，它可以自动的将你编写并放入指定位置的markdown文件转化为带样式(css)与交互能力(script)的html文件，并自动完成本地配置，非常适合入门使用，并且本身也有一定的可扩展性</p><h2 id="step-10-本地调试">step 10 本地调试</h2><p>使用如下命令之一进行本地部署</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo serve <span class="comment"># 部署</span></span><br><span class="line">$ hexo s <span class="comment"># 可加参数 -p [port number]指定端口号，默认4000</span></span><br></pre></td></tr></table></figure><p>之后就可以通过http://localhost:4000访问生成的hexo页面啦！</p><p>到这里你已经完成了hexo最基本的配置并跑通了最基础的一个页面，接下里我们将其与GitHub关联起来</p><h2 id="step-11-部署到GitHub">step 11 部署到GitHub</h2><p>请到站点根目录<code>/</code>下修改<code>_config.yml</code>文件中如下部分</p><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/one-command-deployment</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">      <span class="attr">repository:</span> <span class="string">git@github.com:GitHub用户名/你之前起的用户名.github.io.git</span></span><br><span class="line">      <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><p>使用如下命令之一部署到远程仓库</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><p>如果还有报错，请<a href="https://chatgpt.com">自行搜索解决</a></p><p>注意到生成了一个新分支<code>master</code>，<strong>因此<a href="#master">回到此处</a>，将<code>main</code>切换为<code>master</code>，然后点击<code>Save</code>保存</strong></p><p>这之后再访问之前GitHub为你生成的那个url，就能看到使用hexo工具生成后的页面了(即上一步访问localhost:4000看到的页面)</p><h1>结语</h1><p>走到这里你已经完成了所有必需内容的配置，接下来就可以上 <a href="https://hexo.io/themes/">Themes | Hexo</a> 选择一个你喜欢的主题了，关于主题的配置，请参考另一篇文章~</p><p>(•‿•)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 零基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/ymhui.github.io/2024/09/08/hello-world/"/>
      <url>/ymhui.github.io/2024/09/08/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
